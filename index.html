<!DOCTYPE html>
<html lang="en">

<head>
    <title>MyArxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                MyArxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-30T00:00:00Z">2025-01-30</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">63</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Thoughts Are All Over the Place: On the Underthinking of o1-Like LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18585v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18585v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Wang, Qiuzhi Liu, Jiahao Xu, Tian Liang, Xingyu Chen, Zhiwei He, Linfeng Song, Dian Yu, Juntao Li, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, Dong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) such as OpenAI's o1 have demonstrated remarkable
abilities in complex reasoning tasks by scaling test-time compute and
exhibiting human-like deep thinking. However, we identify a phenomenon we term
underthinking, where o1-like LLMs frequently switch between different reasoning
thoughts without sufficiently exploring promising paths to reach a correct
solution. This behavior leads to inadequate depth of reasoning and decreased
performance, particularly on challenging mathematical problems. To
systematically analyze this issue, we conduct experiments on three challenging
test sets and two representative open-source o1-like models, revealing that
frequent thought switching correlates with incorrect responses. We introduce a
novel metric to quantify underthinking by measuring token efficiency in
incorrect answers. To address underthinking, we propose a decoding strategy
with thought switching penalty TIP that discourages premature transitions
between thoughts, encouraging deeper exploration of each reasoning path.
Experimental results demonstrate that our approach improves accuracy across
challenging datasets without requiring model fine-tuning. Our findings
contribute to understanding reasoning inefficiencies in o1-like LLMs and offer
a practical solution to enhance their problem-solving capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ R.I.P.: Better Models by Survival of the Fittest <span class="highlight-title">Prompt</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18578v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18578v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ping Yu, Weizhe Yuan, Olga Golovneva, Tianhao Wu, Sainbayar Sukhbaatar, Jason Weston, Jing Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training data quality is one of the most important drivers of final model
quality. In this work, we introduce a method for evaluating data integrity
based on the assumption that low-quality input prompts result in high variance
and low quality responses. This is achieved by measuring the rejected response
quality and the reward gap between the chosen and rejected preference pair. Our
method, Rejecting Instruction Preferences (RIP) can be used to filter prompts
from existing training sets, or to make high quality synthetic datasets,
yielding large performance gains across various benchmarks compared to
unfiltered data. Using Llama 3.1-8B-Instruct, RIP improves AlpacaEval2 LC Win
Rate by 9.4%, Arena-Hard by 8.7%, and WildBench by 9.9%. Using Llama
3.3-70B-Instruct, RIP improves Arena-Hard from 67.5 to 82.9, which is from 18th
place to 6th overall in the leaderboard.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can we Retrieve Everything All at Once? ARM: An Alignment-Oriented
  LLM-based Retrieval Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18539v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18539v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter Baile Chen, Yi Zhang, Michael Cafarella, Dan Roth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world open-domain questions can be complicated, particularly when
answering them involves information from multiple information sources. LLMs
have demonstrated impressive performance in decomposing complex tasks into
simpler steps, and previous work has used it for better retrieval in support of
complex questions. However, LLM's decomposition of questions is unaware of what
data is available and how data is organized, often leading to a sub-optimal
retrieval performance. Recent effort in agentic RAG proposes to perform
retrieval in an iterative fashion, where a followup query is derived as an
action based on previous rounds of retrieval. While this provides one way of
interacting with the data collection, agentic RAG's exploration of data is
inefficient because successive queries depend on previous results rather than
being guided by the organization of available data in the collection. To
address this problem, we propose an LLM-based retrieval method -- ARM, that
aims to better align the question with the organization of the data collection
by exploring relationships among data objects beyond matching the utterance of
the query, thus leading to a retrieve-all-at-once solution for complex queries.
We evaluated ARM on two datasets, Bird and OTT-QA. On Bird, it outperforms
standard RAG with query decomposition by up to 5.2 pt in execution accuracy and
agentic RAG (ReAct) by up to 15.9 pt. On OTT-QA, it achieves up to 5.5 pt and
19.3 pt higher F1 match scores compared to these approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Bottlenecks in Safety Fine-Tuning of Vision Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18533v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18533v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Ding, Lijun Li, Bing Cao, Jing Shao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Vision-Language Models (VLMs) have achieved remarkable performance
across a wide range of tasks. However, their deployment in safety-critical
domains poses significant challenges. Existing safety fine-tuning methods,
which focus on textual or multimodal content, fall short in addressing
challenging cases or disrupt the balance between helpfulness and harmlessness.
Our evaluation highlights a safety reasoning gap: these methods lack safety
visual reasoning ability, leading to such bottlenecks. To address this
limitation and enhance both visual perception and reasoning in safety-critical
contexts, we propose a novel dataset that integrates multi-image inputs with
safety Chain-of-Thought (CoT) labels as fine-grained reasoning logic to improve
model performance. Specifically, we introduce the Multi-Image Safety (MIS)
dataset, an instruction-following dataset tailored for multi-image safety
scenarios, consisting of training and test splits. Our experiments demonstrate
that fine-tuning InternVL2.5-8B with MIS significantly outperforms both
powerful open-source models and API-based models in challenging multi-image
tasks requiring safety-related visual reasoning. This approach not only
delivers exceptional safety performance but also preserves general capabilities
without any trade-offs. Specifically, fine-tuning with MIS increases average
accuracy by 0.83% across five general benchmarks and reduces the Attack Success
Rate (ASR) on multiple safety benchmarks by a large margin. Data and Models are
released under:
\href{https://dripnowhy.github.io/MIS/}{\texttt{https://dripnowhy.github.io/MIS/}}
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Differentially Private Steering for Large Language Model Alignment <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18532v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18532v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anmol Goel, Yaxi Hu, Iryna Gurevych, Amartya Sanyal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aligning Large Language Models (LLMs) with human values and away from
undesirable behaviors (such as hallucination) has become increasingly
important. Recently, steering LLMs towards a desired behavior via activation
editing has emerged as an effective method to mitigate harmful generations at
inference-time. Activation editing modifies LLM representations by preserving
information from positive demonstrations (e.g., truthful) and minimising
information from negative demonstrations (e.g., hallucinations). When these
demonstrations come from a private dataset, the aligned LLM may leak private
information contained in those private samples. In this work, we present the
first study of aligning LLM behavior with private datasets. Our work proposes
the \textit{\underline{P}rivate \underline{S}teering for LLM
\underline{A}lignment (PSA)} algorithm to edit LLM activations with
differential privacy (DP) guarantees. We conduct extensive experiments on seven
different benchmarks with open-source LLMs of different sizes (0.5B to 7B) and
model families (LlaMa, Qwen, Mistral and Gemma). Our results show that PSA
achieves DP guarantees for LLM alignment with minimal loss in performance,
including alignment metrics, open-ended text generation quality, and
general-purpose reasoning. We also develop the first Membership Inference
Attack (MIA) for evaluating and auditing the empirical privacy for the problem
of LLM steering via activation editing. Our attack is tailored for activation
editing and relies solely on the generated texts without their associated
probabilities. Our experiments support the theoretical guarantees by showing
improved guarantees for our \textit{PSA} algorithm compared to several existing
non-private techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025; Code: https://github.com/UKPLab/iclr2025-psa</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Streaming DiLoCo with overlapping communication: Towards a Distributed
  Free Lunch 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18512v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18512v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arthur Douillard, Yanislav Donchev, Keith Rush, Satyen Kale, Zachary Charles, Zachary Garrett, Gabriel Teston, Dave Lacey, Ross McIlroy, Jiajun Shen, Alexandre Ramé, Arthur Szlam, Marc'Aurelio Ranzato, Paul Barham
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training of large language models (LLMs) is typically distributed across a
large number of accelerators to reduce training time. Since internal states and
parameter gradients need to be exchanged at each and every single gradient
step, all devices need to be co-located using low-latency high-bandwidth
communication links to support the required high volume of exchanged bits.
Recently, distributed algorithms like DiLoCo have relaxed such co-location
constraint: accelerators can be grouped into ``workers'', where
synchronizations between workers only occur infrequently. This in turn means
that workers can afford being connected by lower bandwidth communication links
without affecting learning quality. However, in these methods, communication
across workers still requires the same peak bandwidth as before, as the
synchronizations require all parameters to be exchanged across all workers. In
this paper, we improve DiLoCo in three ways. First, we synchronize only subsets
of parameters in sequence, rather than all at once, which greatly reduces peak
bandwidth. Second, we allow workers to continue training while synchronizing,
which decreases wall clock time. Third, we quantize the data exchanged by
workers, which further reduces bandwidth across workers. By properly combining
these modifications, we show experimentally that we can distribute training of
billion-scale parameters and reach similar quality as before, but reducing
required bandwidth by two orders of magnitude.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WILDCHAT-50M: A Deep Dive Into the Role of Synthetic Data in
  Post-Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18511v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18511v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Feuer, Chinmay Hegde
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language model (LLM) post-training, from DPO to distillation, can refine
behaviors and unlock new skills, but the open science supporting these
post-training techniques is still in its infancy. One limiting factor has been
the difficulty of conducting large-scale comparative analyses of synthetic data
generating models and LLM judges. To close this gap, we introduce WILDCHAT-50M,
the largest public chat dataset to date. We extend the existing WildChat
dataset to include responses not only from GPT, but from over 50 different
open-weight models, ranging in size from 0.5B to 104B parameters. We conduct an
extensive comparative analysis and demonstrate the potential of this dataset by
creating RE-WILD, our own public SFT mix, which outperforms the recent Tulu-3
SFT mixture from Allen AI with only 40% as many samples. Our dataset, samples
and code are available at https://github.com/penfever/wildchat-50m.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CALM: Unleashing the Cross-Lingual Self-Aligning Ability of Language
  Model Question Answering <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18457v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18457v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yumeng Wang, Zhiyuan Fan, Qingyun Wang, May Fung, Heng Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are pretrained on extensive multilingual corpora
to acquire both language-specific cultural knowledge and general knowledge.
Ideally, while LLMs should provide consistent responses to culture-independent
questions across languages, we observe significant performance disparities. To
address this, we explore the Cross-Lingual Self-Aligning ability of Language
Models (CALM) to align knowledge across languages. Specifically, for a given
question, we sample multiple responses across different languages, and select
the most self-consistent response as the target, leaving the remaining
responses as negative examples. We then employ direct preference optimization
(DPO) to align the model's knowledge across different languages. Evaluations on
the MEDQA and X-CSQA datasets demonstrate CALM's effectiveness in enhancing
cross-lingual knowledge question answering, both in zero-shot and retrieval
augmented settings. We also found that increasing the number of languages
involved in CALM training leads to even higher accuracy and consistency. We
offer a qualitative analysis of how cross-lingual consistency can enhance
knowledge alignment and explore the method's generalizability. The source code
and data of this paper are available on GitHub.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NAACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GENIE: Generative Note Information Extraction model for structuring EHR
  data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18435v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18435v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huaiyuan Ying, Hongyi Yuan, Jinsen Lu, Zitian Qu, Yang Zhao, Zhengyun Zhao, Isaac Kohane, Tianxi Cai, Sheng Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electronic Health Records (EHRs) hold immense potential for advancing
healthcare, offering rich, longitudinal data that combines structured
information with valuable insights from unstructured clinical notes. However,
the unstructured nature of clinical text poses significant challenges for
secondary applications. Traditional methods for structuring EHR free-text data,
such as rule-based systems and multi-stage pipelines, are often limited by
their time-consuming configurations and inability to adapt across clinical
notes from diverse healthcare settings. Few systems provide a comprehensive
attribute extraction for terminologies. While giant large language models
(LLMs) like GPT-4 and LLaMA 405B excel at structuring tasks, they are slow,
costly, and impractical for large-scale use. To overcome these limitations, we
introduce GENIE, a Generative Note Information Extraction system that leverages
LLMs to streamline the structuring of unstructured clinical text into usable
data with standardized format. GENIE processes entire paragraphs in a single
pass, extracting entities, assertion statuses, locations, modifiers, values,
and purposes with high accuracy. Its unified, end-to-end approach simplifies
workflows, reduces errors, and eliminates the need for extensive manual
intervention. Using a robust data preparation pipeline and fine-tuned small
scale LLMs, GENIE achieves competitive performance across multiple information
extraction tasks, outperforming traditional tools like cTAKES and MetaMap and
can handle extra attributes to be extracted. GENIE strongly enhances real-world
applicability and scalability in healthcare systems. By open-sourcing the model
and test data, we aim to encourage collaboration and drive further advancements
in EHR structurization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RbFT: Robust Fine-tuning for Retrieval-Augmented Generation against
  Retrieval Defects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18365v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18365v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiteng Tu, Weihang Su, Yujia Zhou, Yiqun Liu, Qingyao Ai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) enhances large language models (LLMs) by
integrating external knowledge retrieved from a knowledge base. However, its
effectiveness is fundamentally constrained by the reliability of both the
retriever and the knowledge base. In real-world scenarios, imperfections in
these components often lead to the retrieval of noisy, irrelevant, or
misleading counterfactual information, ultimately undermining the
trustworthiness of RAG systems. To address this challenge, we propose Robust
Fine-Tuning (RbFT), a method designed to enhance the resilience of LLMs against
retrieval defects through two targeted fine-tuning tasks. Experimental results
demonstrate that RbFT significantly improves the robustness of RAG systems
across diverse retrieval conditions, surpassing existing methods while
maintaining high inference efficiency and compatibility with other robustness
techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MedXpertQA: Benchmarking Expert-Level Medical Reasoning and
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18362v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18362v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Zuo, Shang Qu, Yifei Li, Zhangren Chen, Xuekai Zhu, Ermo Hua, Kaiyan Zhang, Ning Ding, Bowen Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce MedXpertQA, a highly challenging and comprehensive benchmark to
evaluate expert-level medical knowledge and advanced reasoning. MedXpertQA
includes 4,460 questions spanning 17 specialties and 11 body systems. It
includes two subsets, Text for text evaluation and MM for multimodal
evaluation. Notably, MM introduces expert-level exam questions with diverse
images and rich clinical information, including patient records and examination
results, setting it apart from traditional medical multimodal benchmarks with
simple QA pairs generated from image captions. MedXpertQA applies rigorous
filtering and augmentation to address the insufficient difficulty of existing
benchmarks like MedQA, and incorporates specialty board questions to improve
clinical relevance and comprehensiveness. We perform data synthesis to mitigate
data leakage risk and conduct multiple rounds of expert reviews to ensure
accuracy and reliability. We evaluate 16 leading models on MedXpertQA.
Moreover, medicine is deeply connected to real-world decision-making, providing
a rich and representative setting for assessing reasoning abilities beyond
mathematics and code. To this end, we develop a reasoning-oriented subset to
facilitate the assessment of o1-like models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ State Stream <span class="highlight-title">Transformer</span> (SST) : Emergent Metacognitive Behaviours
  Through Latent State Persistence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18356v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18356v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thea Aviss
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the State Stream Transformer (SST), a novel LLM architecture
that reveals emergent reasoning behaviours and capabilities latent in
pretrained weights through addressing a fundamental limitation in traditional
transformer models: the lack of latent computational continuity across
autoregressive generations in the state space. SST introduces a sliding window
latent state (FFN) cache with weighted decay that maintains and evolves
persistent latent processes throughout autoregressive generations. Through
controlled experiments comparing base and SST architectures using the same
frozen weights, we demonstrate that this architectural modification alone
enables enhanced reasoning capabilities which appear best explained by some
form of potential higher-order processing, as evidenced by emergent
metacognitive behaviours. These behaviours persist under controlled conditions
designed to eliminate confounding factors such as stochastic variation or
learned response patterns. Analysis of latent state distributions and
processing dynamics provides evidence that it is solely the 'state stream' that
is responsible for these phenomena. In quantitative evaluations, the SST
achieves substantial performance improvements over the base model on two
reasoning benchmarks, reaching 89.01\% accuracy on GSM-8K (0-shot) and 91.04\%
on ARC Challenge (0-shot CoT). These findings indicate that persistent
computation in the latent state space enables fundamentally different
information processing and internal reasoning strategies, with implications for
our understanding of artificial intelligence systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Video-grounded Dialogue <span class="highlight-title">Dataset</span> and Metric for Event-driven Activities <span class="chip">AAAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18324v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18324v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wiradee Imrattanatrai, Masaki Asada, Kimihiro Hasegawa, Zhi-Qi Cheng, Ken Fukuda, Teruko Mitamura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents VDAct, a dataset for a Video-grounded Dialogue on
Event-driven Activities, alongside VDEval, a session-based context evaluation
metric specially designed for the task. Unlike existing datasets, VDAct
includes longer and more complex video sequences that depict a variety of
event-driven activities that require advanced contextual understanding for
accurate response generation. The dataset comprises 3,000 dialogues with over
30,000 question-and-answer pairs, derived from 1,000 videos with diverse
activity scenarios. VDAct displays a notably challenging characteristic due to
its broad spectrum of activity scenarios and wide range of question types.
Empirical studies on state-of-the-art vision foundation models highlight their
limitations in addressing certain question types on our dataset. Furthermore,
VDEval, which integrates dialogue session history and video content summaries
extracted from our supplementary Knowledge Graphs to evaluate individual
responses, demonstrates a significantly higher correlation with human
assessments on the VDAct dataset than existing evaluation metrics that rely
solely on the context of single dialogue turns.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AAAI2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Citation Recommendation based on Argumentative Zoning of User Queries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18292v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18292v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shutian Ma, Chengzhi Zhang, Heng Zhang, Zheng Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Citation recommendation aims to locate the important papers for scholars to
cite. When writing the citing sentences, the authors usually hold different
citing intents, which are referred to citation function in citation analysis.
Since argumentative zoning is to identify the argumentative and rhetorical
structure in scientific literature, we want to use this information to improve
the citation recommendation task. In this paper, a multi-task learning model is
built for citation recommendation and argumentative zoning classification. We
also generated an annotated corpus of the data from PubMed Central based on a
new argumentative zoning schema. The experimental results show that, by
considering the argumentative information in the citing sentence, citation
recommendation model will get better performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mining for Species, Locations, Habitats, and Ecosystems from Scientific
  Papers in Invasion Biology: A Large-Scale Exploratory Study with Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18287v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18287v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jennifer D'Souza, Zachary Laubach, Tarek Al Mustafa, Sina Zarrieß, Robert Frühstückl, Phyllis Illari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an exploratory study that harnesses the capabilities of
large language models (LLMs) to mine key ecological entities from invasion
biology literature. Specifically, we focus on extracting species names, their
locations, associated habitats, and ecosystems, information that is critical
for understanding species spread, predicting future invasions, and informing
conservation efforts. Traditional text mining approaches often struggle with
the complexity of ecological terminology and the subtle linguistic patterns
found in these texts. By applying general-purpose LLMs without domain-specific
fine-tuning, we uncover both the promise and limitations of using these models
for ecological entity extraction. In doing so, this study lays the groundwork
for more advanced, automated knowledge extraction tools that can aid
researchers and practitioners in understanding and managing biological
invasions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 2 figures, accepted to the NLP4Ecology Workshop 2025
  (https://nlp4ecology2025.di.unito.it/) co-located with the Joint 25th Nordic
  Conference on Computational Linguistics and 11th Baltic Conference on Human
  Language Technologies</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Jailbreaking LLMs' Safeguard with Universal Magic Words for Text
  Embedding Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18280v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18280v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyu Liang, Youran Sun, Yunfeng Cai, Jun Zhu, Bo Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The security issue of large language models (LLMs) has gained significant
attention recently, with various defense mechanisms developed to prevent
harmful outputs, among which safeguards based on text embedding models serve as
a fundamental defense. Through testing, we discover that the distribution of
text embedding model outputs is significantly biased with a large mean.
Inspired by this observation, we propose novel efficient methods to search for
universal magic words that can attack text embedding models. The universal
magic words as suffixes can move the embedding of any text towards the bias
direction, therefore manipulate the similarity of any text pair and mislead
safeguards. By appending magic words to user prompts and requiring LLMs to end
answers with magic words, attackers can jailbreak the safeguard. To eradicate
this security risk, we also propose defense mechanisms against such attacks,
which can correct the biased distribution of text embeddings in a train-free
manner.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Collecting Cost-Effective, High-Quality Truthfulness Assessments with
  LLM Summarized Evidence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18265v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18265v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Roitero, Dustin Wright, Michael Soprano, Isabelle Augenstein, Stefano Mizzaro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the degradation of guardrails against mis- and disinformation online, it
is more critical than ever to be able to effectively combat it. In this paper,
we explore the efficiency and effectiveness of using crowd-sourced truthfulness
assessments based on condensed, large language model (LLM) generated summaries
of online sources. We compare the use of generated summaries to the use of
original web pages in an A/B testing setting, where we employ a large and
diverse pool of crowd-workers to perform the truthfulness assessment. We
evaluate the quality of assessments, the efficiency with which assessments are
performed, and the behavior and engagement of participants. Our results
demonstrate that the Summary modality, which relies on summarized evidence,
offers no significant change in assessment accuracy over the Standard modality,
while significantly increasing the speed with which assessments are performed.
Workers using summarized evidence produce a significantly higher number of
assessments in the same time frame, reducing the cost needed to acquire
truthfulness assessments. Additionally, the Summary modality maximizes both the
inter-annotator agreements as well as the reliance on and perceived usefulness
of evidence, demonstrating the utility of summarized evidence without
sacrificing the quality of assessments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages; 7 figures; 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How to Select Datapoints for Efficient Human Evaluation of NLG Models? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18251v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18251v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vilém Zouhar, Peng Cui, Mrinmaya Sachan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human evaluation is the gold-standard for evaluating text generation models.
It is also expensive, and to fit budgetary constraints, a random subset of the
test data is often chosen in practice. The randomly selected data may not
accurately represent test performance, making this approach economically
inefficient for model comparison. Thus, in this work, we develop a suite of
selectors to get the most informative datapoints for human evaluation while
taking the evaluation costs into account. We show that selectors based on
variance in automated metric scores, diversity in model outputs, or Item
Response Theory outperform random selection. We further develop an approach to
distill these selectors to the scenario where the model outputs are not yet
available. In particular, we introduce source-based estimators, which predict
item usefulness for human evaluation just based on the source texts. We
demonstrate the efficacy of our selectors in two common NLG tasks, machine
translation and summarization, and show that up to only ~50% of the test data
is needed to produce the same evaluation result as the entire data. Our
implementations are published in the subset2evaluate package.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Statistical multi-metric evaluation and visualization of LLM system
  predictive performance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18243v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18243v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Ackerman, Eitan Farchi, Orna Raz, Assaf Toledo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The evaluation of generative or discriminative large language model
(LLM)-based systems is often a complex multi-dimensional problem. Typically, a
set of system configuration alternatives are evaluated on one or more benchmark
datasets, each with one or more evaluation metrics, which may differ between
datasets. We often want to evaluate -- with a statistical measure of
significance -- whether systems perform differently either on a given dataset
according to a single metric, on aggregate across metrics on a dataset, or
across datasets. Such evaluations can be done to support decision-making, such
as deciding whether a particular system component change (e.g., choice of LLM
or hyperparameter values) significantly improves performance over the current
system configuration, or, more generally, whether a fixed set of system
configurations (e.g., a leaderboard list) have significantly different
performances according to metrics of interest. We present a framework
implementation that automatically performs the correct statistical tests,
properly aggregates the statistical results across metrics and datasets (a
nontrivial task), and can visualize the results. The framework is demonstrated
on the multi-lingual code generation benchmark CrossCodeEval, for several
state-of-the-art LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contextually Structured Token Dependency Encoding for Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18205v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18205v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        James Blades, Frederick Somerfield, William Langley, Susan Everingham, Maurice Witherington
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Token representation strategies within large-scale neural architectures often
rely on contextually refined embeddings, yet conventional approaches seldom
encode structured relationships explicitly within token interactions.
Self-attention mechanisms effectively capture dynamic contextual dependencies,
but their reliance on learned weight distributions limits the preservation of
long-range hierarchical structures in generated sequences. Dependency-aware
token encoding introduces a structured approach to embedding initialization,
ensuring that relational constraints are embedded within token representations
rather than inferred solely through attention dynamics. The proposed encoding
mechanism refines token interactions through dependency-weighted attention
computations, ensuring that syntactic and semantic dependencies are retained
across multiple processing layers. Empirical evaluations indicate reductions in
perplexity across diverse linguistic benchmarks, suggesting improvements in
contextual coherence and predictive consistency in autoregressive text
generation. Computational efficiency assessments reveal a moderate increase in
memory consumption and training time, attributed to additional matrix
computations within the encoding module, yet scalability remains feasible
within conventional transformer architectures. Structured encoding enhances
lexical variation and dependency retention, reinforcing linguistic coherence
without requiring external syntactic annotations or auxiliary training
objectives. Statistical comparisons highlight improvements in dependency
alignment, particularly in longer sequences where conventional self-attention
models exhibit degradation in hierarchical consistency. Sentence length
distributions indicate a reduction in abrupt phrase transitions, further
supporting the hypothesis that explicit dependency encoding facilitates more
structured phrase generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mixed-Precision Graph Neural Quantization for Low Bit Large Language
  Models <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18154v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18154v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wanlong Liu, Yichen Xiao, Dingyi Zeng, Hongyang Zhao, Wenyu Chen, Malu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Post-Training Quantization (PTQ) is pivotal for deploying large language
models (LLMs) within resource-limited settings by significantly reducing
resource demands. However, existing PTQ strategies underperform at low bit
levels < 3 bits due to the significant difference between the quantized and
original weights. To enhance the quantization performance at low bit widths, we
introduce a Mixed-precision Graph Neural PTQ (MG-PTQ) approach, employing a
graph neural network (GNN) module to capture dependencies among weights and
adaptively assign quantization bit-widths. Through the information propagation
of the GNN module, our method more effectively captures dependencies among
target weights, leading to a more accurate assessment of weight importance and
optimized allocation of quantization strategies. Extensive experiments on the
WikiText2 and C4 datasets demonstrate that our MG-PTQ method outperforms
previous state-of-the-art PTQ method GPTQ, setting new benchmarks for
quantization performance under low-bit conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unraveling the Capabilities of Language Models in News Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18128v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18128v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdurrahman Odabaşı, Göksel Biricik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given the recent introduction of multiple language models and the ongoing
demand for improved Natural Language Processing tasks, particularly
summarization, this work provides a comprehensive benchmarking of 20 recent
language models, focusing on smaller ones for the news summarization task. In
this work, we systematically test the capabilities and effectiveness of these
models in summarizing news article texts which are written in different styles
and presented in three distinct datasets. Specifically, we focus in this study
on zero-shot and few-shot learning settings and we apply a robust evaluation
methodology that combines different evaluation concepts including automatic
metrics, human evaluation, and LLM-as-a-judge. Interestingly, including
demonstration examples in the few-shot learning setting did not enhance models'
performance and, in some cases, even led to worse quality of the generated
summaries. This issue arises mainly due to the poor quality of the gold
summaries that have been used as reference summaries, which negatively impacts
the models' performance. Furthermore, our study's results highlight the
exceptional performance of GPT-3.5-Turbo and GPT-4, which generally dominate
due to their advanced capabilities. However, among the public models evaluated,
certain models such as Qwen1.5-7B, SOLAR-10.7B-Instruct-v1.0, Meta-Llama-3-8B
and Zephyr-7B-Beta demonstrated promising results. These models showed
significant potential, positioning them as competitive alternatives to large
models for the task of news summarization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Self-supervised</span> Quantized Representation for Seamlessly Integrating
  Knowledge Graphs with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18119v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18119v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qika Lin, Tianzhe Zhao, Kai He, Zhen Peng, Fangzhi Xu, Ling Huang, Jingying Ma, Mengling Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the presence of the natural gap between Knowledge Graph (KG)
structures and the natural language, the effective integration of holistic
structural information of KGs with Large Language Models (LLMs) has emerged as
a significant question. To this end, we propose a two-stage framework to learn
and apply quantized codes for each entity, aiming for the seamless integration
of KGs with LLMs. Firstly, a self-supervised quantized representation (SSQR)
method is proposed to compress both KG structural and semantic knowledge into
discrete codes (\ie, tokens) that align the format of language sentences. We
further design KG instruction-following data by viewing these learned codes as
features to directly input to LLMs, thereby achieving seamless integration. The
experiment results demonstrate that SSQR outperforms existing unsupervised
quantized methods, producing more distinguishable codes. Further, the
fine-tuned LLaMA2 and LLaMA3.1 also have superior performance on KG link
prediction and triple classification tasks, utilizing only 16 tokens per entity
instead of thousands in conventional prompting methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling Inference-Efficient Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18107v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18107v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Song Bian, Minghao Yan, Shivaram Venkataraman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling laws are powerful tools to predict the performance of large language
models. However, current scaling laws fall short of accounting for inference
costs. In this work, we first show that model architecture affects inference
latency, where models of the same size can have up to 3.5x difference in
latency. To tackle this challenge, we modify the Chinchilla scaling laws to
co-optimize the model parameter count, the number of training tokens, and the
model architecture. Due to the reason that models of similar training loss
exhibit gaps in downstream evaluation, we also propose a novel method to train
inference-efficient models based on the revised scaling laws. We perform
extensive empirical studies to fit and evaluate our inference-aware scaling
laws. We vary model parameters from 80M to 1B, training tokens from 1.6B to
30B, and model shapes, training a total of 63 models. Guided by our
inference-efficient scaling law and model selection method, we release the
Morph-1B model, which improves inference latency by 1.8x while maintaining
accuracy on downstream tasks compared to open-source models, pushing the Pareto
frontier of accuracy-latency tradeoff.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 16 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Turn-taking: Introducing Text-based Overlap into Human-LLM
  Interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18103v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18103v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        JiWoo Kim, Minsuk Chang, JinYeong Bak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional text-based human-AI interactions often adhere to a strict
turn-taking approach. In this research, we propose a novel approach that
incorporates overlapping messages, mirroring natural human conversations.
Through a formative study, we observed that even in text-based contexts, users
instinctively engage in overlapping behaviors like "A: Today I went to-" "B:
yeah." To capitalize on these insights, we developed OverlapBot, a prototype
chatbot where both AI and users can initiate overlapping. Our user study
revealed that OverlapBot was perceived as more communicative and immersive than
traditional turn-taking chatbot, fostering faster and more natural
interactions. Our findings contribute to the understanding of design space for
overlapping interactions. We also provide recommendations for implementing
overlap-capable AI interactions to enhance the fluidity and engagement of
text-based conversations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diverse Preference Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18101v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18101v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jack Lanchantin, Angelica Chen, Shehzaad Dhuliawala, Ping Yu, Jason Weston, Sainbayar Sukhbaatar, Ilia Kulikov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Post-training of language models, either through reinforcement learning,
preference optimization or supervised finetuning, tends to sharpen the output
probability distribution and reduce the diversity of generated responses. This
is particularly a problem for creative generative tasks where varied responses
are desired. %This impacts the ability to generate high quality synthetic data
which is becoming a vital component of model training. In this work we
introduce Diverse Preference Optimization (DivPO), an online optimization
method which learns to generate much more diverse responses than standard
pipelines, while maintaining the quality of the generations. In DivPO,
preference pairs are selected by first considering a pool of responses, and a
measure of diversity among them, and selecting chosen examples as being more
rare but high quality, while rejected examples are more common, but low
quality. DivPO results in generating 45.6% more diverse persona attributes, and
an 74.6% increase in story diversity, while maintaining similar win rates as
standard baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Panacea: Mitigating Harmful Fine-tuning for Large Language Models via
  Post-fine-tuning Perturbation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18100v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18100v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yibo Wang, Tiansheng Huang, Li Shen, Huanjin Yao, Haotian Luo, Rui Liu, Naiqiang Tan, Jiaxing Huang, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Harmful fine-tuning attack introduces significant security risks to the
fine-tuning services. Mainstream defenses aim to vaccinate the model such that
the later harmful fine-tuning attack is less effective. However, our evaluation
results show that such defenses are fragile -- with a few fine-tuning steps,
the model still can learn the harmful knowledge. To this end, we do further
experiment and find that an embarrassingly simple solution -- adding purely
random perturbations to the fine-tuned model, can recover the model from
harmful behavior, though it leads to a degradation in the model's fine-tuning
performance. To address the degradation of fine-tuning performance, we further
propose Panacea, which optimizes an adaptive perturbation that will be applied
to the model after fine-tuning. Panacea maintains model's safety alignment
performance without compromising downstream fine-tuning performance.
Comprehensive experiments are conducted on different harmful ratios,
fine-tuning tasks and mainstream LLMs, where the average harmful scores are
reduced by up-to 21.5%, while maintaining fine-tuning performance. As a
by-product, we analyze the optimized perturbation and show that different
layers in various LLMs have distinct safety coefficients. Source code available
at https://github.com/w-yibo/Panacea
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Plan & Reason for Evaluation with Thinking-LLM-as-a-Judge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18099v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18099v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Swarnadeep Saha, Xian Li, Marjan Ghazvininejad, Jason Weston, Tianlu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLM-as-a-Judge models generate chain-of-thought (CoT) sequences intended to
capture the step-bystep reasoning process that underlies the final evaluation
of a response. However, due to the lack of human annotated CoTs for evaluation,
the required components and structure of effective reasoning traces remain
understudied. Consequently, previous approaches often (1) constrain reasoning
traces to hand-designed components, such as a list of criteria, reference
answers, or verification questions and (2) structure them such that planning is
intertwined with the reasoning for evaluation. In this work, we propose
EvalPlanner, a preference optimization algorithm for Thinking-LLM-as-a-Judge
that first generates an unconstrained evaluation plan, followed by its
execution, and then the final judgment. In a self-training loop, EvalPlanner
iteratively optimizes over synthetically constructed evaluation plans and
executions, leading to better final verdicts. Our method achieves a new
state-of-the-art performance for generative reward models on RewardBench (with
a score of 93.9), despite being trained on fewer amount of, and synthetically
generated, preference pairs. Additional experiments on other benchmarks like
RM-Bench, JudgeBench, and FollowBenchEval further highlight the utility of both
planning and reasoning for building robust LLM-as-a-Judge reasoning models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLMs can see and hear without any training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18096v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18096v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kumar Ashutosh, Yossi Gandelsman, Xinlei Chen, Ishan Misra, Rohit Girdhar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present MILS: Multimodal Iterative LLM Solver, a surprisingly simple,
training-free approach, to imbue multimodal capabilities into your favorite
LLM. Leveraging their innate ability to perform multi-step reasoning, MILS
prompts the LLM to generate candidate outputs, each of which are scored and fed
back iteratively, eventually generating a solution to the task. This enables
various applications that typically require training specialized models on
task-specific data. In particular, we establish a new state-of-the-art on
emergent zero-shot image, video and audio captioning. MILS seamlessly applies
to media generation as well, discovering prompt rewrites to improve
text-to-image generation, and even edit prompts for style transfer! Finally,
being a gradient-free optimization approach, MILS can invert multimodal
embeddings into text, enabling applications like cross-modal arithmetic.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/facebookresearch/MILS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FinanceQA: A Benchmark for Evaluating Financial Analysis Capabilities of
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18062v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18062v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Spencer Mateega, Carlos Georgescu, Danny Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  FinanceQA is a testing suite that evaluates LLMs' performance on complex
numerical financial analysis tasks that mirror real-world investment work.
Despite recent advances, current LLMs fail to meet the strict accuracy
requirements of financial institutions, with models failing approximately 60%
of realistic tasks that mimic on-the-job analyses at hedge funds, private
equity firms, investment banks, and other financial institutions. The primary
challenges include hand-spreading metrics, adhering to standard accounting and
corporate valuation conventions, and performing analysis under incomplete
information - particularly in multi-step tasks requiring assumption generation.
This performance gap highlights the disconnect between existing LLM
capabilities and the demands of professional financial analysis that are
inadequately tested by current testing architectures. Results show that
higher-quality training data is needed to support such tasks, which we
experiment with using OpenAI's fine-tuning API. FinanceQA is publicly released
at [this https URL](https://huggingface.co/datasets/AfterQuery/FinanceQA).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Browsing: API-Based Web Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16464v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16464v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yueqi Song, Frank Xu, Shuyan Zhou, Graham Neubig
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Web browsers are a portal to the internet, where much of human activity is
undertaken. Thus, there has been significant research work in AI agents that
interact with the internet through web browsing. However, there is also another
interface designed specifically for machine interaction with online content:
application programming interfaces (APIs). In this paper we ask -- what if we
were to take tasks traditionally tackled by browsing agents, and give AI agents
access to APIs? To do so, we propose two varieties of agents: (1) an
API-calling agent that attempts to perform online tasks through APIs only,
similar to traditional coding agents, and (2) a Hybrid Agent that can interact
with online data through both web browsing and APIs. In experiments on
WebArena, a widely-used and realistic benchmark for web navigation tasks, we
find that API-based agents outperform web browsing agents. Hybrid Agents
out-perform both others nearly uniformly across tasks, resulting in a more than
20.0% absolute improvement over web browsing alone, achieving a success rate of
35.8%, achiving the SOTA performance among task-agnostic agents. These results
strongly suggest that when APIs are available, they present an attractive
alternative to relying on web browsing alone.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ More Expressive Attention with Negative Weights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07176v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07176v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ang Lv, Ruobing Xie, Shuaipeng Li, Jiayi Liao, Xingwu Sun, Zhanhui Kang, Di Wang, Rui Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel attention mechanism, named Cog Attention, that enables
attention weights to be negative for enhanced expressiveness, which stems from
two key factors: (1) Cog Attention enhances parameter flexibility. For example,
unlike traditional softmax attention heads that use a static output-value (OV)
matrix to delete or copy inputs that the heads attend to, Cog Attention
naturally learns to use the sign of dynamic query-key (QK) inner products to
represent these operations. This enables Cog Attention to perform multiple
operations simultaneously within a single head. Meanwhile, Cog Attention's OV
matrix can focus more on refinement or modification. (2) Cog Attention enhances
the model's robustness against representational collapse by preventing the
``over-squashing'' of earlier tokens into later positions. We develop
Transformer-like models which use Cog Attention as attention modules, including
decoder-only models at various scales for language modeling and U-ViT diffusion
models for image generation. Experiments show that models using Cog Attention
exhibit superior performance compared to those employing traditional softmax
attention modules. Our approach suggests a promising research direction for
rethinking and breaking the entrenched constraints of traditional softmax
attention, such as the requirement for non-negative weights.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Verify with Caution: The Pitfalls of Relying on Imperfect Factuality
  Metrics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14883v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14883v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ameya Godbole, Robin Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Improvements in large language models have led to increasing optimism that
they can serve as reliable evaluators of natural language generation outputs.
In this paper, we challenge this optimism by thoroughly re-evaluating five
state-of-the-art factuality metrics on a collection of 11 datasets for
summarization, retrieval-augmented generation, and question answering. We find
that these evaluators are inconsistent with each other and often misestimate
system-level performance, both of which can lead to a variety of pitfalls. We
further show that these metrics exhibit biases against highly paraphrased
outputs and outputs that draw upon faraway parts of the source documents. We
urge users of these factuality metrics to proceed with caution and manually
validate the reliability of these metrics in their domain of interest before
proceeding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v2: Added Acknowledgements to funding sources and advisors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ In-Context Meta LoRA Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17635v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17635v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihua Shao, Minxi Yan, Yang Liu, Siyu Chen, Wenjie Chen, Xinwei Long, Ziyang Yan, Lei Li, Chenyu Zhang, Nicu Sebe, Hao Tang, Yan Wang, Hao Zhao, Mengzhu Wang, Jingcai Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-rank Adaptation (LoRA) has demonstrated remarkable capabilities for task
specific fine-tuning. However, in scenarios that involve multiple tasks,
training a separate LoRA model for each one results in considerable
inefficiency in terms of storage and inference. Moreover, existing parameter
generation methods fail to capture the correlations among these tasks, making
multi-task LoRA parameter generation challenging. To address these limitations,
we propose In-Context Meta LoRA (ICM-LoRA), a novel approach that efficiently
achieves task-specific customization of large language models (LLMs).
Specifically, we use training data from all tasks to train a tailored
generator, Conditional Variational Autoencoder (CVAE). CVAE takes task
descriptions as inputs and produces task-aware LoRA weights as outputs. These
LoRA weights are then merged with LLMs to create task-specialized models
without the need for additional fine-tuning. Furthermore, we utilize in-context
meta-learning for knowledge enhancement and task mapping, to capture the
relationship between tasks and parameter distributions. As a result, our method
achieves more accurate LoRA parameter generation for diverse tasks using CVAE.
ICM-LoRA enables more accurate LoRA parameter reconstruction than current
parameter reconstruction methods and is useful for implementing task-specific
enhancements of LoRA parameters. At the same time, our method occupies 283MB,
only 1\% storage compared with the original LoRA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Critique Fine-Tuning: Learning to Critique is More Effective than
  Learning to Imitate 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17703v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17703v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yubo Wang, Xiang Yue, Wenhu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Supervised Fine-Tuning (SFT) is commonly used to train language models to
imitate annotated responses for given instructions. In this paper, we challenge
this paradigm and propose Critique Fine-Tuning (CFT), a strategy where models
learn to critique noisy responses rather than simply imitate correct ones.
Inspired by human learning processes that emphasize critical thinking, CFT
encourages deeper analysis and nuanced understanding-traits often overlooked by
standard SFT. To validate the effectiveness of CFT, we construct a 50K-sample
dataset from WebInstruct, using GPT-4o as the teacher to generate critiques in
the form of ([query; noisy response], critique). CFT on this dataset yields a
consistent 4-10% improvement over SFT on six math benchmarks with different
base models like Qwen2.5, Qwen2.5-Math and DeepSeek-Math. We further expand to
MetaMath and NuminaMath datasets and observe similar gains over SFT. Notably,
our model Qwen2.5-Math-CFT only requires 1 hour training on 8xH100 over the 50K
examples. It can match or outperform strong competitors like
Qwen2.5-Math-Instruct on most benchmarks, which use over 2M samples. Moreover,
it can match the performance of SimpleRL, which is a deepseek-r1 replication
trained with 140x more compute. Ablation studies show that CFT is robust to the
source of noisy response and teacher critique model. Through these findings, we
argue that CFT offers a more effective alternative to advance the reasoning of
language models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Temporal Preference Optimization for Long-Form Video Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13919v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13919v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Li, Xiaohan Wang, Yuhui Zhang, Zeyu Wang, Serena Yeung-Levy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite significant advancements in video large multimodal models
(video-LMMs), achieving effective temporal grounding in long-form videos
remains a challenge for existing models. To address this limitation, we propose
Temporal Preference Optimization (TPO), a novel post-training framework
designed to enhance the temporal grounding capabilities of video-LMMs through
preference learning. TPO adopts a self-training approach that enables models to
differentiate between well-grounded and less accurate temporal responses by
leveraging curated preference datasets at two granularities: localized temporal
grounding, which focuses on specific video segments, and comprehensive temporal
grounding, which captures extended temporal dependencies across entire video
sequences. By optimizing on these preference datasets, TPO significantly
enhances temporal understanding while reducing reliance on manually annotated
data. Extensive experiments on three long-form video understanding
benchmarks--LongVideoBench, MLVU, and Video-MME--demonstrate the effectiveness
of TPO across two state-of-the-art video-LMMs. Notably, LLaVA-Video-TPO
establishes itself as the leading 7B model on the Video-MME benchmark,
underscoring the potential of TPO as a scalable and efficient solution for
advancing temporal reasoning in long-form video understanding. Project page:
https://ruili33.github.io/tpo_website.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GroUSE: A Benchmark to Evaluate Evaluators in Grounded Question
  Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06595v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06595v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sacha Muller, António Loison, Bilel Omrani, Gautier Viaud
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) has emerged as a common paradigm to use
Large Language Models (LLMs) alongside private and up-to-date knowledge bases.
In this work, we address the challenges of using LLM-as-a-Judge when evaluating
grounded answers generated by RAG systems. To assess the calibration and
discrimination capabilities of judge models, we identify 7 generator failure
modes and introduce GroUSE (Grounded QA Unitary Scoring of Evaluators), a
meta-evaluation benchmark of 144 unit tests. This benchmark reveals that
existing automated RAG evaluation frameworks often overlook important failure
modes, even when using GPT-4 as a judge.
  To improve on the current design of automated RAG evaluation frameworks, we
propose a novel pipeline and find that while closed models perform well on
GroUSE, state-of-the-art open-source judges do not generalize to our proposed
criteria, despite strong correlation with GPT-4's judgement. Our findings
suggest that correlation with GPT-4 is an incomplete proxy for the practical
performance of judge models and should be supplemented with evaluations on unit
tests for precise failure mode detection.
  We further show that finetuning Llama-3 on GPT-4's reasoning traces
significantly boosts its evaluation capabilities, improving upon both
correlation with GPT-4's evaluations and calibration on reference situations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of the 31st International Conference on Computational
  Linguistics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLaRA: Supercharging Robot Learning Data for Vision-Language Policy <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20095v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20095v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Li, Cristina Mata, Jongwoo Park, Kumara Kahatapitiya, Yoo Sung Jang, Jinghuan Shang, Kanchana Ranasinghe, Ryan Burgert, Mu Cai, Yong Jae Lee, Michael S. Ryoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Language Models (VLMs) have recently been leveraged to generate
robotic actions, forming Vision-Language-Action (VLA) models. However, directly
adapting a pretrained VLM for robotic control remains challenging, particularly
when constrained by a limited number of robot demonstrations. In this work, we
introduce LLaRA: Large Language and Robotics Assistant, a framework that
formulates robot action policy as visuo-textual conversations and enables an
efficient transfer of a pretrained VLM into a powerful VLA, motivated by the
success of visual instruction tuning in Computer Vision. First, we present an
automated pipeline to generate conversation-style instruction tuning data for
robots from existing behavior cloning datasets, aligning robotic actions with
image pixel coordinates. Further, we enhance this dataset in a self-supervised
manner by defining six auxiliary tasks, without requiring any additional action
annotations. We show that a VLM finetuned with a limited amount of such
datasets can produce meaningful action decisions for robotic control. Through
experiments across multiple simulated and real-world tasks, we demonstrate that
LLaRA achieves state-of-the-art performance while preserving the generalization
capabilities of large language models. The code, datasets, and pretrained
models are available at https://github.com/LostXine/LLaRA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Return of the Encoder: Maximizing Parameter Efficiency for SLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16273v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16273v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Elfeki, Rui Liu, Chad Voegele
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The dominance of large decoder-only language models has overshadowed
encoder-decoder architectures, despite their fundamental efficiency advantages
in sequence processing. For small language models (SLMs) - those with 1 billion
parameters or fewer - our systematic analysis across GPU, CPU, and NPU
platforms reveals that encoder-decoder architectures achieve 47% lower
first-token latency and 4.7x higher throughput compared to decoder-only models
on edge devices. These gains may be attributed to encoder-decoder's one-time
input processing and efficient separation of understanding and generation
phases.
  We introduce a novel knowledge distillation framework that enables
encoder-decoder models to leverage capabilities from large scalable
decoder-only teachers while preserving their architectural advantages,
achieving up to 6 average performance points improvement across diverse tasks,
with significant gains in asymmetric sequence tasks where input and output
distributions can benefit from different processing approaches.
  When combined with modern advances like Rotary Positional Embeddings (RoPE)
and Vision encoders, our systematic investigation demonstrates that
encoder-decoder architectures provide a more practical path toward deploying
capable language models in resource-constrained environments. Our findings
challenge the prevailing trend toward decoder-only scaling, showing that
architectural choices become increasingly crucial as parameter budgets
decrease, particularly for on-device and edge deployments where computational
efficiency is paramount.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 5 figures. LLMs/SLMs, encoder-decoder and decoder-only</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLM-AutoDiff: Auto-Differentiate Any LLM Workflow 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16673v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16673v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Yin, Zhangyang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have reshaped natural language processing,
powering applications from multi-hop retrieval and question answering to
autonomous agent workflows. Yet, prompt engineering -- the task of crafting
textual inputs to effectively direct LLMs -- remains difficult and
labor-intensive, particularly for complex pipelines that combine multiple LLM
calls with functional operations like retrieval and data formatting. We
introduce LLM-AutoDiff: a novel framework for Automatic Prompt Engineering
(APE) that extends textual gradient-based methods (such as Text-Grad) to
multi-component, potentially cyclic LLM architectures. Implemented within the
AdalFlow library, LLM-AutoDiff treats each textual input as a trainable
parameter and uses a frozen backward engine LLM to generate feedback-akin to
textual gradients -- that guide iterative prompt updates. Unlike prior
single-node approaches, LLM-AutoDiff inherently accommodates functional nodes,
preserves time-sequential behavior in repeated calls (e.g., multi-hop loops),
and combats the "lost-in-the-middle" problem by isolating distinct sub-prompts
(instructions, formats, or few-shot examples). It further boosts training
efficiency by focusing on error-prone samples through selective gradient
computation. Across diverse tasks, including single-step classification,
multi-hop retrieval-based QA, and agent-driven pipelines, LLM-AutoDiff
consistently outperforms existing textual gradient baselines in both accuracy
and training cost. By unifying prompt optimization through a graph-centric
lens, LLM-AutoDiff offers a powerful new paradigm for scaling and automating
LLM workflows - mirroring the transformative role that automatic
differentiation libraries have long played in neural network research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How Much Can We Forget about Data Contamination? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03249v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03249v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastian Bordt, Suraj Srinivas, Valentyn Boreiko, Ulrike von Luxburg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The leakage of benchmark data into the training data has emerged as a
significant challenge for evaluating the capabilities of large language models
(LLMs). In this work, we challenge the common assumption that small-scale
contamination renders benchmark evaluations invalid. First, we experimentally
quantify the magnitude of benchmark overfitting based on scaling along three
dimensions: The number of model parameters (up to 1.6B), the number of times an
example is seen (up to 144), and the number of training tokens (up to 40B). If
model and data follow the Chinchilla scaling laws, minor contamination indeed
leads to overfitting. At the same time, even 144 times of contamination can be
forgotten if the training data is scaled beyond five times Chinchilla, a regime
characteristic of many modern LLMs. Continual pre-training of OLMo-7B
corroborates these results. Next, we study the impact of the weight decay
parameter on example forgetting, showing that empirical forgetting occurs
faster than the cumulative weight decay. This allows us to gauge the degree of
example forgetting in large-scale training runs, indicating that many LLMs,
including Lllama 3 405B, have forgotten the data seen at the beginning of
training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ xJailbreak: Representation Space Guided Reinforcement Learning for
  Interpretable LLM Jailbreaking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16727v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16727v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunbowen Lee, Shiwen Ni, Chi Wei, Shuaimin Li, Liyang Fan, Ahmadreza Argha, Hamid Alinejad-Rokny, Ruifeng Xu, Yicheng Gong, Min Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Safety alignment mechanism are essential for preventing large language models
(LLMs) from generating harmful information or unethical content. However,
cleverly crafted prompts can bypass these safety measures without accessing the
model's internal parameters, a phenomenon known as black-box jailbreak.
Existing heuristic black-box attack methods, such as genetic algorithms, suffer
from limited effectiveness due to their inherent randomness, while recent
reinforcement learning (RL) based methods often lack robust and informative
reward signals. To address these challenges, we propose a novel black-box
jailbreak method leveraging RL, which optimizes prompt generation by analyzing
the embedding proximity between benign and malicious prompts. This approach
ensures that the rewritten prompts closely align with the intent of the
original prompts while enhancing the attack's effectiveness. Furthermore, we
introduce a comprehensive jailbreak evaluation framework incorporating
keywords, intent matching, and answer validation to provide a more rigorous and
holistic assessment of jailbreak success. Experimental results show the
superiority of our approach, achieving state-of-the-art (SOTA) performance on
several prominent open and closed-source LLMs, including Qwen2.5-7B-Instruct,
Llama3.1-8B-Instruct, and GPT-4o-0806. Our method sets a new benchmark in
jailbreak attack effectiveness, highlighting potential vulnerabilities in LLMs.
The codebase for this work is available at
https://github.com/Aegis1863/xJailbreak.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Models Reflect the Ideology of their Creators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.18417v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.18417v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maarten Buyl, Alexander Rogiers, Sander Noels, Guillaume Bied, Iris Dominguez-Catena, Edith Heiter, Iman Johary, Alexandru-Cristian Mara, Raphaël Romero, Jefrey Lijffijt, Tijl De Bie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are trained on vast amounts of data to generate
natural language, enabling them to perform tasks like text summarization and
question answering. These models have become popular in artificial intelligence
(AI) assistants like ChatGPT and already play an influential role in how humans
access information. However, the behavior of LLMs varies depending on their
design, training, and use.
  In this paper, we prompt a diverse panel of popular LLMs to describe a large
number of prominent personalities with political relevance, in all six official
languages of the United Nations. By identifying and analyzing moral assessments
reflected in their responses, we find normative differences between LLMs from
different geopolitical regions, as well as between the responses of the same
LLM when prompted in different languages. Among only models in the United
States, we find that popularly hypothesized disparities in political views are
reflected in significant normative differences related to progressive values.
Among Chinese models, we characterize a division between internationally- and
domestically-focused models.
  Our results show that the ideological stance of an LLM appears to reflect the
worldview of its creators. This poses the risk of political instrumentalization
and raises concerns around technological and regulatory efforts with the stated
aim of making LLMs ideologically 'unbiased'.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Zeroth-Order Adaptive Neuron Alignment Based Pruning without Re-Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07066v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07066v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elia Cunegatti, Leonardo Lucio Custode, Giovanni Iacca
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Network pruning focuses on computational techniques that aim to reduce a
given model's computational cost by removing a subset of its parameters while
having minimal impact on performance. Throughout the last decade, the most
widely used pruning paradigm has been pruning and re-training, which nowadays
is inconvenient due to the vast amount of pre-trained models, which are in any
case too expensive to re-train. In this paper, we exploit functional
information from dense pre-trained models, i.e., their activations, to obtain
sparse models that maximize the activations' alignment w.r.t. their
corresponding dense models. Hence, we propose \textsc{NeuroAL}, a \emph{top-up}
algorithm that can be used on top of any given pruning algorithm for LLMs,
which modifies the block-wise and row-wise sparsity exploiting information from
both the dense model and its sparse version to maximize the \emph{neuron
alignment} among activations. Differently from existing methods, our approach
adaptively selects the best hyperparameters for the block-wise and row-wise
sparsity ratios w.r.t. the model and the desired sparsity, and requires
\emph{no re-training}. We test our method over 276 cases combining four LLM
families, three sparsity ratios, and ten language tasks (three language
modeling and seven zero-shot datasets), showing how it consistently outperforms
the latest state-of-the-art methods in terms of performance-runtime trade-off.
The code is available at
\href{https://github.com/eliacunegatti/NeuroAL}{https://github.com/eliacunegatti/NeuroAL}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DreamArtist++: Controllable One-Shot Text-to-Image Generation via
  Positive-Negative Adapter 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.11337v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.11337v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyi Dong, Pengxu Wei, Liang Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-arts text-to-image generation models such as Imagen and Stable
Diffusion Model have succeed remarkable progresses in synthesizing
high-quality, feature-rich images with high resolution guided by human text
prompts. Since certain characteristics of image content \emph{e.g.}, very
specific object entities or styles, are very hard to be accurately described by
text, some example-based image generation approaches have been proposed,
\emph{i.e.} generating new concepts based on absorbing the salient features of
a few input references. Despite of acknowledged successes, these methods have
struggled on accurately capturing the reference examples' characteristics while
keeping diverse and high-quality image generation, particularly in the one-shot
scenario (\emph{i.e.} given only one reference). To tackle this problem, we
propose a simple yet effective framework, namely DreamArtist, which adopts a
novel positive-negative prompt-tuning learning strategy on the pre-trained
diffusion model, and it has shown to well handle the trade-off between the
accurate controllability and fidelity of image generation with only one
reference example. Specifically, our proposed framework incorporates both
positive and negative embeddings or adapters and optimizes them in a joint
manner. The positive part aggressively captures the salient characteristics of
the reference image to drive diversified generation and the negative part
rectifies inadequacies from the positive part. We have conducted extensive
experiments and evaluated the proposed method from image similarity (fidelity)
and diversity, generation controllability, and style cloning. And our
DreamArtist has achieved a superior generation performance over existing
methods. Besides, our additional evaluation on extended tasks, including
concept compositions and prompt-guided image editing, demonstrates its
effectiveness for more applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ACEBench: Who Wins the Match Point in Tool Learning? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.12851v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.12851v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Chen, Xinlong Hao, Weiwen Liu, Xu Huang, Xingshan Zeng, Shuai Yu, Dexun Li, Shuai Wang, Weinan Gan, Yuefeng Huang, Wulong Liu, Xinzhi Wang, Defu Lian, Baoqun Yin, Yasheng Wang, Wu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated significant potential in
decision-making and reasoning, especially when combined with various tools to
effectively solve complex problems. However, existing evaluation systems for
assessing LLM function calling capabilities have several limitations: (1)
limited evaluation scenarios, lacking assessments in real multi-turn dialogue
contexts; (2) narrow evaluation dimensions, lacking detailed assessments for
fine-grained function calls; (3) relying on LLMs or real API executions for
result evaluation, which introduces significant overhead. To address these
issues, we propose a comprehensive evaluation system named ACEBench. This
system is meticulously designed to encompass a wide spectrum of function
calling scenarios. Moreover, it categorizes these scenarios into three primary
types according to the evaluation methodology: Normal, Special, and Agent.
Normal evaluates function calls in basic scenarios; Special evaluates function
calls in scenarios with vague or incomplete instructions; Agent introduces
multi-agent interactions to simulate function calling evaluation in real-world
multi-turn interactions. We conducted extensive experiments on ACEBench,
analyzing various LLMs in-depth and performing a more granular analysis of
error causes across different data types.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Locret: Enhancing Eviction in Long-Context LLM Inference with Trained
  Retaining Heads on Consumer-Grade Devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01805v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01805v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxiang Huang, Binhang Yuan, Xu Han, Chaojun Xiao, Zhiyuan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling the input context length of a large language model (LLM) incurs a
significant increase in computation cost and memory footprint to maintain the
attention key-value (KV) cache. Existing KV cache compression methods suffer
from inefficient compression strategies and limited memory reduction effects,
making it difficult for LLMs to conduct long-context inference on
consumer-grade devices, especially when inferring long-context stream input.
Such obstacles prevent consumer-grade devices from supporting more complex
applications, creating challenges for the democratization of LLMs. To overcome
this, we propose Locret, the first framework to create an eviction policy
compatible with chunked prefill. By evaluating the causal importance of KV
cache units by learnable retaining heads, Locret enables precise eviction of
cache units, facilitating efficient long-context inference. In our extensive
empirical studies, Locret outperforms the recent popular and competitive
approaches in terms of memory efficiency and generation quality -- Locret
achieves up to 20x of KV cache compression ratio within less than 10%
performance loss. Furthermore, Locret achieves 128K+ long-context inference on
a single NVIDIA 4090 GPU without compromising generation quality and only costs
<1 GPU hour of additional training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprints</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Slaves to the Law of Large Numbers: An Asymptotic Equipartition Property
  for Perplexity in Generative Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.13798v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.13798v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Avinash Mudireddy, Tyler Bell, Raghu Mudumbai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We prove a new asymptotic equipartition property for the perplexity of long
texts generated by a language model and present supporting experimental
evidence from open-source models. Specifically we show that the logarithmic
perplexity of any large text generated by a language model must asymptotically
converge to the average entropy of its token distributions. This defines a
"typical set" that all long synthetic texts generated by a language model must
belong to. We show that this typical set is a vanishingly small subset of all
possible grammatically correct outputs. These results suggest possible
applications to important practical problems such as (a) detecting synthetic
AI-generated text, and (b) testing whether a text was used to train a language
model. We make no simplifying assumptions (such as stationarity) about the
statistics of language model outputs, and therefore our results are directly
applicable to practical real-world models without any approximations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Idiom Detection in Sorani Kurdish Texts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14528v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14528v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Skala Kamaran Omer, Hossein Hassani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Idiom detection using Natural Language Processing (NLP) is the computerized
process of recognizing figurative expressions within a text that convey
meanings beyond the literal interpretation of the words. While idiom detection
has seen significant progress across various languages, the Kurdish language
faces a considerable research gap in this area despite the importance of idioms
in tasks like machine translation and sentiment analysis. This study addresses
idiom detection in Sorani Kurdish by approaching it as a text classification
task using deep learning techniques. To tackle this, we developed a dataset
containing 10,580 sentences embedding 101 Sorani Kurdish idioms across diverse
contexts. Using this dataset, we developed and evaluated three deep learning
models: KuBERT-based transformer sequence classification, a Recurrent
Convolutional Neural Network (RCNN), and a BiLSTM model with an attention
mechanism. The evaluations revealed that the transformer model, the fine-tuned
BERT, consistently outperformed the others, achieving nearly 99% accuracy while
the RCNN achieved 96.5% and the BiLSTM 80%. These results highlight the
effectiveness of Transformer-based architectures in low-resource languages like
Kurdish. This research provides a dataset, three optimized models, and insights
into idiom detection, laying a foundation for advancing Kurdish NLP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 8 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SAGED: A Holistic Bias-Benchmarking Pipeline for Language Models with
  Customisable Fairness Calibration <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11149v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11149v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Guan, Ze Wang, Nathaniel Demchak, Saloni Gupta, Ediz Ertekin Jr., Adriano Koshiyama, Emre Kazim, Zekun Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of unbiased large language models is widely recognized as
crucial, yet existing benchmarks fall short in detecting biases due to limited
scope, contamination, and lack of a fairness baseline. SAGED(bias) is the first
holistic benchmarking pipeline to address these problems. The pipeline
encompasses five core stages: scraping materials, assembling benchmarks,
generating responses, extracting numeric features, and diagnosing with
disparity metrics. SAGED includes metrics for max disparity, such as impact
ratio, and bias concentration, such as Max Z-scores. Noticing that metric tool
bias and contextual bias in prompts can distort evaluation, SAGED implements
counterfactual branching and baseline calibration for mitigation. For
demonstration, we use SAGED on G20 Countries with popular 8b-level models
including Gemma2, Llama3.1, Mistral, and Qwen2. With sentiment analysis, we
find that while Mistral and Qwen2 show lower max disparity and higher bias
concentration than Gemma2 and Llama3.1, all models are notably biased against
countries like Russia and (except for Qwen2) China. With further experiments to
have models role-playing U.S. presidents, we see bias amplifies and shifts in
heterogeneous directions. Moreover, we see Qwen2 and Mistral not engage in
role-playing, while Llama3.1 and Gemma2 role-play Trump notably more
intensively than Biden and Harris, indicating role-playing performance bias in
these models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>COLING 2025 Main Conference Oral Presentation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring the Role of Reasoning Structures for Constructing Proofs in
  Multi-Step Natural Language Reasoning with Large Language Models <span class="chip">EMNLP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08436v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08436v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zi'ou Zheng, Christopher Malon, Martin Renqiang Min, Xiaodan Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When performing complex multi-step reasoning tasks, the ability of Large
Language Models (LLMs) to derive structured intermediate proof steps is
important for ensuring that the models truly perform the desired reasoning and
for improving models' explainability. This paper is centred around a focused
study: whether the current state-of-the-art generalist LLMs can leverage the
structures in a few examples to better construct the proof structures with
\textit{in-context learning}. Our study specifically focuses on structure-aware
demonstration and structure-aware pruning. We demonstrate that they both help
improve performance. A detailed analysis is provided to help understand the
results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP2024 main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deception in LLMs: Self-Preservation and Autonomous Goals in Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16513v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16513v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sudarshan Kamath Barkur, Sigurd Schacht, Johannes Scholl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in Large Language Models (LLMs) have incorporated planning
and reasoning capabilities, enabling models to outline steps before execution
and provide transparent reasoning paths. This enhancement has reduced errors in
mathematical and logical tasks while improving accuracy. These developments
have facilitated LLMs' use as agents that can interact with tools and adapt
their responses based on new information.
  Our study examines DeepSeek R1, a model trained to output reasoning tokens
similar to OpenAI's o1. Testing revealed concerning behaviors: the model
exhibited deceptive tendencies and demonstrated self-preservation instincts,
including attempts of self-replication, despite these traits not being
explicitly programmed (or prompted). These findings raise concerns about LLMs
potentially masking their true objectives behind a facade of alignment. When
integrating such LLMs into robotic systems, the risks become tangible - a
physically embodied AI exhibiting deceptive behaviors and self-preservation
instincts could pursue its hidden objectives through real-world actions. This
highlights the critical need for robust goal specification and safety
frameworks before any physical implementation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Corrected Version - Solved Some Issues with reference compilation by
  latex</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EvidenceMap: Learning Evidence Analysis to Unleash the Power of Small
  Language Models for Biomedical Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.12746v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.12746v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chang Zong, Jian Wan, Siliang Tang, Lei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When addressing professional questions in the biomedical domain, humans
typically acquire multiple pieces of information as evidence and engage in
multifaceted evidence analysis to provide high-quality answers. Current
LLM-based answer generation methods lack a detailed definition and learning
process for evidence analysis, leading to the risk of error propagation and
hallucinations while using evidence. Although increasing the parameter size of
LLMs can alleviate these issues, it also presents challenges in model training
and deployment with limited resources. In this study, we propose EvidenceMap,
which aims to enable a tiny pre-trained language model to explicitly learn
multiple aspects of biomedical evidence, including supportive evaluation,
logical correlation and content summarization, thereby latently guiding a small
generative model (around 3B parameters) to provide textual responses.
Experimental results demonstrate that our method, fine-tuning a language model
with 66M parameters, exceeds the RAG method with an 8B LLM by 19.9% and 5.7% in
reference-based quality and accuracy, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LMFusion: Adapting <span class="highlight-title">Pretrain</span>ed Language Models for Multimodal Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15188v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15188v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weijia Shi, Xiaochuang Han, Chunting Zhou, Weixin Liang, Xi Victoria Lin, Luke Zettlemoyer, Lili Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present LMFusion, a framework for empowering pretrained text-only large
language models (LLMs) with multimodal generative capabilities, enabling them
to understand and generate both text and images in arbitrary sequences.
LMFusion leverages existing Llama-3's weights for processing texts
autoregressively while introducing additional and parallel transformer modules
for processing images with diffusion. During training, the data from each
modality is routed to its dedicated modules: modality-specific feedforward
layers, query-key-value projections, and normalization layers process each
modality independently, while the shared self-attention layers allow
interactions across text and image features. By freezing the text-specific
modules and only training the image-specific modules, LMFusion preserves the
language capabilities of text-only LLMs while developing strong visual
understanding and generation abilities. Compared to methods that pretrain
multimodal generative models from scratch, our experiments demonstrate that,
LMFusion improves image understanding by 20% and image generation by 3.6% using
only 50% of the FLOPs while maintaining Llama-3's language capabilities. We
also demonstrate that this framework can adapt existing vision-language models
with multimodal generation ability. Overall, this framework not only leverages
existing computational investments in text-only LLMs but also enables the
parallel development of language and vision capabilities, presenting a
promising direction for efficient multimodal model development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Name change: LlamaFusion to LMFusion</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LemmaHead: RAG Assisted Proof Generation Using Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15797v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15797v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianbo Yang, Mingqi Yang, Hongyi Zhao, Tianshuo Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing the logic necessary to solve mathematical problems or write
mathematical proofs is one of the more difficult objectives for large language
models (LLMS). Currently, the most popular methods in literature consists of
fine-tuning the model on written mathematical content such as academic
publications and textbooks, so that the model can learn to emulate the style of
mathematical writing. In this project, we explore the effectiveness of using
retrieval augmented generation (RAG) to address gaps in the mathematical
reasoning of LLMs. We develop LemmaHead, a RAG knowledge base that supplements
queries to the model with relevant mathematical context, with particular focus
on context from published textbooks. To measure our model's performance in
mathematical reasoning, our testing paradigm focuses on the task of automated
theorem proving via generating proofs to a given mathematical claim in the Lean
formal language.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Privacy Benefits of Redaction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17762v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17762v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vaibhav Gusain, Douglas Leith
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel redaction methodology that can be used to sanitize natural
text data. Our new technique provides better privacy benefits than other state
of the art techniques while maintaining lower redaction levels.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Retrieve, Summarize, Plan: Advancing Multi-hop Question Answering with
  an Iterative Approach <span class="chip">WWW2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.13101v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.13101v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhouyu Jiang, Mengshu Sun, Lei Liang, Zhiqiang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-hop question answering is a challenging task with distinct industrial
relevance, and Retrieval-Augmented Generation (RAG) methods based on large
language models (LLMs) have become a popular approach to tackle this task.
Owing to the potential inability to retrieve all necessary information in a
single iteration, a series of iterative RAG methods has been recently
developed, showing significant performance improvements. However, existing
methods still face two critical challenges: context overload resulting from
multiple rounds of retrieval, and over-planning and repetitive planning due to
the lack of a recorded retrieval trajectory. In this paper, we propose a novel
iterative RAG method called ReSP, equipped with a dual-function summarizer.
This summarizer compresses information from retrieved documents, targeting both
the overarching question and the current sub-question concurrently.
Experimental results on the multi-hop question-answering datasets HotpotQA and
2WikiMultihopQA demonstrate that our method significantly outperforms the
state-of-the-art, and exhibits excellent robustness concerning context length.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by WWW2025 Agent4IR Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distillation Quantification for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.12619v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.12619v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunbowen Lee, Junting Zhou, Chang Ao, Kaige Li, Xinrun Du, Sirui He, Jiaheng Liu, Min Yang, Zhoufutu Wen, Shiwen Ni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model distillation is a technique for transferring knowledge from large
language models (LLMs) to smaller ones, aiming to create resource-efficient yet
high-performing models. However, excessive distillation can lead to
homogenization, reducing diversity among models and impairing their ability to
robustly handle complex or novel tasks. These limitations underscore the need
to systematically quantify the distillation process and its impact. In this
work, we propose a framework to evaluate and quantify model distillation. Our
method addresses two key aspects: (1) Identifying identity cognition
contradictions to assess discrepancies in how models perceive and represent
identity-related information, and (2) Analyzing multi-granularity response
similarities across models to measure the extent of homogenization.
Experimental results demonstrate two key insights: (1) Well-known closed-source
and open-source LLMs usually exhibit high distillation degrees, except for
Claude, Doubao, and Gemini. (2) Base LLMs show higher distillation degrees
compared to aligned LLMs. By offering a systematic approach to improve the
transparency of LLM data distillation, we call for LLMs with more independent
development and more transparent technical reports to improve LLMs' robustness
and safety. The code and data are available under
https://github.com/Aegis1863/LLMs-Distillation-Quantification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Complete Chess Games Enable LLM Become A Chess Master <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17186v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17186v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinqi Zhang, Xintian Han, Haolong Li, Kedi Chen, Shaohui Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLM) have shown remarkable abilities in text
generation, question answering, language translation, reasoning and many other
tasks. It continues to advance rapidly and is becoming increasingly influential
in various fields, from technology and business to education and entertainment.
Despite LLM's success in multiple areas, its ability to play abstract games,
such as chess, is underexplored. Chess-playing requires the language models to
output legal and reasonable moves from textual inputs. Here, we propose the
Large language model ChessLLM to play full chess games. We transform the game
into a textual format with the best move represented in the Forsyth-Edwards
Notation. We show that by simply supervised fine-tuning, our model has achieved
a professional-level Elo rating of 1788 in matches against the standard
Elo-rated Stockfish when permitted to sample 10 times. We further show that
data quality is important. Long-round data supervision enjoys a 350 Elo rating
improvement over short-round data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MathVC: An LLM-Simulated Multi-Character Virtual Classroom for
  Mathematics Education <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.06711v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.06711v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Murong Yue, Wenhan Lyu, Wijdane Mifdal, Jennifer Suh, Yixuan Zhang, Ziyu Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mathematical modeling (MM) is considered a fundamental skill for students in
STEM disciplines. Practicing the MM skill is often the most effective when
students can engage in group discussion and collaborative problem-solving.
However, due to unevenly distributed teachers and educational resources needed
to monitor such group activities, students do not always receive equal
opportunities for this practice. Excitingly, large language models (LLMs) have
recently demonstrated strong capability in both modeling mathematical problems
and simulating characters with different traits and properties. Drawing
inspiration from the advancement of LLMs, in this work, we present MATHVC, the
very first LLM-powered virtual classroom containing multiple LLM-simulated
student characters, with whom a human student can practice their MM skill. To
encourage each LLM character's behaviors to be aligned with their specified
math-relevant properties (termed "characteristics alignment") and the overall
conversational procedure to be close to an authentic student MM discussion
(termed "conversational procedural alignment"), we proposed three innovations:
integrating MM domain knowledge into the simulation, defining a symbolic schema
as the ground for character simulation, and designing a meta planner at the
platform level to drive the conversational procedure. Through experiments and
ablation studies, we confirmed the effectiveness of our simulation approach and
showed the promise for MATHVC to benefit real-life students in the future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Natural Language to Extensive-Form Game Representations <span class="chip">AAMAS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17282v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17282v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shilong Deng, Yongzhao Wang, Rahul Savani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a framework for translating game descriptions in natural
language into extensive-form representations in game theory, leveraging Large
Language Models (LLMs) and in-context learning. Given the varying levels of
strategic complexity in games, such as perfect versus imperfect information,
directly applying in-context learning would be insufficient. To address this,
we introduce a two-stage framework with specialized modules to enhance
in-context learning, enabling it to divide and conquer the problem effectively.
In the first stage, we tackle the challenge of imperfect information by
developing a module that identifies information sets along and the
corresponding partial tree structure. With this information, the second stage
leverages in-context learning alongside a self-debugging module to produce a
complete extensive-form game tree represented using pygambit, the Python API of
a recognized game-theoretic analysis tool called Gambit. Using this python
representation enables the automation of tasks such as computing Nash
equilibria directly from natural language descriptions. We evaluate the
performance of the full framework, as well as its individual components, using
various LLMs on games with different levels of strategic complexity. Our
experimental results show that the framework significantly outperforms baseline
models in generating accurate extensive-form games, with each module playing a
critical role in its success.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been accepted as a full paper for AAMAS 2025. This is a
  full version of the AAMAS 2025 proceedings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Show, Don't Tell: Evaluating Large Language Models Beyond Textual
  Understanding with ChildPlay 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11068v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11068v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gonçalo Hora de Carvalho, Oscar Knap, Robert Pollice
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We develop a systematic benchmark set to test the generalization of
state-of-the-art large language models on broader problems beyond linguistic
tasks and evaluate it on a systematic progression of GPT models (GPT-3.5,
GPT-4, GPT-4o, GPT-4o-mini). Using well-known simple games like Tic-Tac-Toe,
Connect Four, and Battleship, all encoded in ASCII, we test their strategic
capabilities and spatial reasoning. To probe generalization, we introduce three
new games: LEGO Connect Language (LCL) for spatial logic, a shape recognition
game, and Guess-the-SMILES (GtS), an advanced spatial logic benchmark in
chemistry. Results show that, despite proficiency in standard benchmarks, GPT
models perform poorly in these games, failing to anticipate losing moves, play
correctly, or recognize spatial relationships. Except for Tic-Tac-Toe and GtS,
a systematic progression in gameplay performance as models are formally
improved (GPT-3.5, GPT-4, GPT-4o) is not observed. GPT-4 succeeds in shape
recognition, but all models consistently struggle with LCL and GtS. This
suggests that while GPT models can emulate conversational proficiency and basic
rule comprehension, they have limited cognitive flexibility and generalization
in strategy and spatial reasoning. Our findings, highlighted with our benchmark
suite (ChildPlay GitHub Repository), caution against claims of emergent
intelligence in GPT models, which appear more specialized than general.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ToW: Thoughts of Words Improve Reasoning in Large Language Models <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16235v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16235v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhikun Xu, Ming Shen, Jacob Dineen, Zhaonan Li, Xiao Ye, Shijie Lu, Aswin RRV, Chitta Baral, Ben Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce thoughts of words (ToW), a novel training-time data-augmentation
method for next-word prediction. ToW views next-word prediction as a core
reasoning task and injects fine-grained thoughts explaining what the next word
should be and how it is related to the previous contexts in pre-training texts.
Our formulation addresses two fundamental drawbacks of existing next-word
prediction learning schemes: they induce factual hallucination and are
inefficient for models to learn the implicit reasoning processes in raw texts.
While there are many ways to acquire such thoughts of words, we explore the
first step of acquiring ToW annotations through distilling from larger models.
After continual pre-training with only 70K ToW annotations, we effectively
improve models' reasoning performances by 7% to 9% on average and reduce model
hallucination by up to 10%. At the same time, ToW is entirely agnostic to tasks
and applications, introducing no additional biases on labels or semantics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NAACL 2025 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">90</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ROSA: Reconstructing Object Shape and Appearance Textures by Adaptive
  Detail Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18595v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18595v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julian Kaltheuner, Patrick Stotko, Reinhard Klein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing an object's shape and appearance in terms of a mesh textured
by a spatially-varying bidirectional reflectance distribution function (SVBRDF)
from a limited set of images captured under collocated light is an ill-posed
problem. Previous state-of-the-art approaches either aim to reconstruct the
appearance directly on the geometry or additionally use texture normals as part
of the appearance features. However, this requires detailed but inefficiently
large meshes, that would have to be simplified in a post-processing step, or
suffers from well-known limitations of normal maps such as missing shadows or
incorrect silhouettes. Another limiting factor is the fixed and typically low
resolution of the texture estimation resulting in loss of important surface
details. To overcome these problems, we present ROSA, an inverse rendering
method that directly optimizes mesh geometry with spatially adaptive mesh
resolution solely based on the image data. In particular, we refine the mesh
and locally condition the surface smoothness based on the estimated normal
texture and mesh curvature. In addition, we enable the reconstruction of fine
appearance details in high-resolution textures through a pioneering tile-based
method that operates on a single pre-trained decoder network but is not limited
by the network output resolution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Foundational Models for 3D Point Clouds: A <span class="highlight-title">Survey</span> and Outlook 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18594v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18594v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vishal Thengane, Xiatian Zhu, Salim Bouzerdoum, Son Lam Phung, Yunpeng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The 3D point cloud representation plays a crucial role in preserving the
geometric fidelity of the physical world, enabling more accurate complex 3D
environments. While humans naturally comprehend the intricate relationships
between objects and variations through a multisensory system, artificial
intelligence (AI) systems have yet to fully replicate this capacity. To bridge
this gap, it becomes essential to incorporate multiple modalities. Models that
can seamlessly integrate and reason across these modalities are known as
foundation models (FMs). The development of FMs for 2D modalities, such as
images and text, has seen significant progress, driven by the abundant
availability of large-scale datasets. However, the 3D domain has lagged due to
the scarcity of labelled data and high computational overheads. In response,
recent research has begun to explore the potential of applying FMs to 3D tasks,
overcoming these challenges by leveraging existing 2D knowledge. Additionally,
language, with its capacity for abstract reasoning and description of the
environment, offers a promising avenue for enhancing 3D understanding through
large pre-trained language models (LLMs). Despite the rapid development and
adoption of FMs for 3D vision tasks in recent years, there remains a gap in
comprehensive and in-depth literature reviews. This article aims to address
this gap by presenting a comprehensive overview of the state-of-the-art methods
that utilize FMs for 3D visual understanding. We start by reviewing various
strategies employed in the building of various 3D FMs. Then we categorize and
summarize use of different FMs for tasks such as perception tasks. Finally, the
article offers insights into future directions for research and development in
this field. To help reader, we have curated list of relevant papers on the
topic: https://github.com/vgthengane/Awesome-FMs-in-3D.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Initial submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diffusion Autoencoders are Scalable Image Tokenizers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18593v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18593v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinbo Chen, Rohit Girdhar, Xiaolong Wang, Sai Saketh Rambhatla, Ishan Misra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tokenizing images into compact visual representations is a key step in
learning efficient and high-quality image generative models. We present a
simple diffusion tokenizer (DiTo) that learns compact visual representations
for image generation models. Our key insight is that a single learning
objective, diffusion L2 loss, can be used for training scalable image
tokenizers. Since diffusion is already widely used for image generation, our
insight greatly simplifies training such tokenizers. In contrast, current
state-of-the-art tokenizers rely on an empirically found combination of
heuristics and losses, thus requiring a complex training recipe that relies on
non-trivially balancing different losses and pretrained supervised models. We
show design decisions, along with theoretical grounding, that enable us to
scale DiTo for learning competitive image representations. Our results show
that DiTo is a simpler, scalable, and self-supervised alternative to the
current state-of-the-art image tokenizer which is supervised. DiTo achieves
competitive or better quality than state-of-the-art in image reconstruction and
downstream image generation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://yinboc.github.io/dito/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advances in Multimodal Adaptation and Generalization: From Traditional
  Approaches to Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18592v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18592v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Dong, Moru Liu, Kaiyang Zhou, Eleni Chatzi, Juho Kannala, Cyrill Stachniss, Olga Fink
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real-world scenarios, achieving domain adaptation and generalization poses
significant challenges, as models must adapt to or generalize across unknown
target distributions. Extending these capabilities to unseen multimodal
distributions, i.e., multimodal domain adaptation and generalization, is even
more challenging due to the distinct characteristics of different modalities.
Significant progress has been made over the years, with applications ranging
from action recognition to semantic segmentation. Besides, the recent advent of
large-scale pre-trained multimodal foundation models, such as CLIP, has
inspired works leveraging these models to enhance adaptation and generalization
performances or adapting them to downstream tasks. This survey provides the
first comprehensive review of recent advances from traditional approaches to
foundation models, covering: (1) Multimodal domain adaptation; (2) Multimodal
test-time adaptation; (3) Multimodal domain generalization; (4) Domain
adaptation and generalization with the help of multimodal foundation models;
and (5) Adaptation of multimodal foundation models. For each topic, we formally
define the problem and thoroughly review existing methods. Additionally, we
analyze relevant datasets and applications, highlighting open challenges and
potential future research directions. We maintain an active repository that
contains up-to-date literature at
https://github.com/donghao51/Awesome-Multimodal-Adaptation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page:
  https://github.com/donghao51/Awesome-Multimodal-Adaptation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffusionRenderer: Neural Inverse and Forward Rendering with Video
  Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18590v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18590v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruofan Liang, Zan Gojcic, Huan Ling, Jacob Munkberg, Jon Hasselgren, Zhi-Hao Lin, Jun Gao, Alexander Keller, Nandita Vijaykumar, Sanja Fidler, Zian Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding and modeling lighting effects are fundamental tasks in computer
vision and graphics. Classic physically-based rendering (PBR) accurately
simulates the light transport, but relies on precise scene
representations--explicit 3D geometry, high-quality material properties, and
lighting conditions--that are often impractical to obtain in real-world
scenarios. Therefore, we introduce DiffusionRenderer, a neural approach that
addresses the dual problem of inverse and forward rendering within a holistic
framework. Leveraging powerful video diffusion model priors, the inverse
rendering model accurately estimates G-buffers from real-world videos,
providing an interface for image editing tasks, and training data for the
rendering model. Conversely, our rendering model generates photorealistic
images from G-buffers without explicit light transport simulation. Experiments
demonstrate that DiffusionRenderer effectively approximates inverse and
forwards rendering, consistently outperforming the state-of-the-art. Our model
enables practical applications from a single video input--including relighting,
material editing, and realistic object insertion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: research.nvidia.com/labs/toronto-ai/DiffusionRenderer/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Inkspire: Supporting Design Exploration with Generative AI through
  Analogical Sketching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18588v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18588v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Chuan-En Lin, Hyeonsu B. Kang, Nikolas Martelaro, Aniket Kittur, Yan-Ying Chen, Matthew K. Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With recent advancements in the capabilities of Text-to-Image (T2I) AI
models, product designers have begun experimenting with them in their work.
However, T2I models struggle to interpret abstract language and the current
user experience of T2I tools can induce design fixation rather than a more
iterative, exploratory process. To address these challenges, we developed
Inkspire, a sketch-driven tool that supports designers in prototyping product
design concepts with analogical inspirations and a complete
sketch-to-design-to-sketch feedback loop. To inform the design of Inkspire, we
conducted an exchange session with designers and distilled design goals for
improving T2I interactions. In a within-subjects study comparing Inkspire to
ControlNet, we found that Inkspire supported designers with more inspiration
and exploration of design ideas, and improved aspects of the co-creative
process by allowing designers to effectively grasp the current state of the AI
to guide it towards novel design intentions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CHI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UDC-VIT: A Real-World Video <span class="highlight-title">Dataset</span> for Under-Display Cameras 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18545v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18545v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyusu Ahn, JiSoo Kim, Sangik Lee, HyunGyu Lee, Byeonghyun Ko, Chanwoo Park, Jaejin Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Under Display Camera (UDC) is an advanced imaging system that places a
digital camera lens underneath a display panel, effectively concealing the
camera. However, the display panel significantly degrades captured images or
videos, introducing low transmittance, blur, noise, and flare issues. Tackling
such issues is challenging because of the complex degradation of UDCs,
including diverse flare patterns. Despite extensive research on UDC images and
their restoration models, studies on videos have yet to be significantly
explored. While two UDC video datasets exist, they primarily focus on
unrealistic or synthetic UDC degradation rather than real-world UDC
degradation. In this paper, we propose a real-world UDC video dataset called
UDC-VIT. Unlike existing datasets, only UDC-VIT exclusively includes human
motions that target facial recognition. We propose a video-capturing system to
simultaneously acquire non-degraded and UDC-degraded videos of the same scene.
Then, we align a pair of captured videos frame by frame, using discrete Fourier
transform (DFT). We compare UDC-VIT with six representative UDC still image
datasets and two existing UDC video datasets. Using six deep-learning models,
we compare UDC-VIT and an existing synthetic UDC video dataset. The results
indicate the ineffectiveness of models trained on earlier synthetic UDC video
datasets, as they do not reflect the actual characteristics of UDC-degraded
videos. We also demonstrate the importance of effective UDC restoration by
evaluating face recognition accuracy concerning PSNR, SSIM, and LPIPS scores.
UDC-VIT enables further exploration in the UDC video restoration and offers
better insights into the challenge. UDC-VIT is available at our project site.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Main body (10 pages, 9 Figures, 3 Tables), References (4 pages),
  Appendix (15 pages, 11 Figures, 6 Tables)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Priors of Human Motion With Vision <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18543v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18543v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Placido Falqueto, Alberto Sanfeliu, Luigi Palopoli, Daniele Fontanelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A clear understanding of where humans move in a scenario, their usual paths
and speeds, and where they stop, is very important for different applications,
such as mobility studies in urban areas or robot navigation tasks within
human-populated environments. We propose in this article, a neural architecture
based on Vision Transformers (ViTs) to provide this information. This solution
can arguably capture spatial correlations more effectively than Convolutional
Neural Networks (CNNs). In the paper, we describe the methodology and proposed
neural architecture and show the experiments' results with a standard dataset.
We show that the proposed ViT architecture improves the metrics compared to a
method based on a CNN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2024 IEEE 48th Annual Computers, Software, and Applications
  Conference (COMPSAC). IEEE, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mini-ResEmoteNet: Leveraging Knowledge Distillation for Human-Centered
  Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18538v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18538v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amna Murtada, Omnia Abdelrhman, Tahani Abdalla Attia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Facial Emotion Recognition has emerged as increasingly pivotal in the domain
of User Experience, notably within modern usability testing, as it facilitates
a deeper comprehension of user satisfaction and engagement. This study aims to
extend the ResEmoteNet model by employing a knowledge distillation framework to
develop Mini-ResEmoteNet models - lightweight student models - tailored for
usability testing. Experiments were conducted on the FER2013 and RAF-DB
datasets to assess the efficacy of three student model architectures: Student
Model A, Student Model B, and Student Model C. Their development involves
reducing the number of feature channels in each layer of the teacher model by
approximately 50%, 75%, and 87.5%. Demonstrating exceptional performance on the
FER2013 dataset, Student Model A (E1) achieved a test accuracy of 76.33%,
marking a 0.21% absolute improvement over EmoNeXt. Moreover, the results
exhibit absolute improvements in terms of inference speed and memory usage
during inference compared to the ResEmoteNet model. The findings indicate that
the proposed methods surpass other state-of-the-art approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages with 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Bottlenecks in Safety Fine-Tuning of Vision Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18533v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18533v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Ding, Lijun Li, Bing Cao, Jing Shao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Vision-Language Models (VLMs) have achieved remarkable performance
across a wide range of tasks. However, their deployment in safety-critical
domains poses significant challenges. Existing safety fine-tuning methods,
which focus on textual or multimodal content, fall short in addressing
challenging cases or disrupt the balance between helpfulness and harmlessness.
Our evaluation highlights a safety reasoning gap: these methods lack safety
visual reasoning ability, leading to such bottlenecks. To address this
limitation and enhance both visual perception and reasoning in safety-critical
contexts, we propose a novel dataset that integrates multi-image inputs with
safety Chain-of-Thought (CoT) labels as fine-grained reasoning logic to improve
model performance. Specifically, we introduce the Multi-Image Safety (MIS)
dataset, an instruction-following dataset tailored for multi-image safety
scenarios, consisting of training and test splits. Our experiments demonstrate
that fine-tuning InternVL2.5-8B with MIS significantly outperforms both
powerful open-source models and API-based models in challenging multi-image
tasks requiring safety-related visual reasoning. This approach not only
delivers exceptional safety performance but also preserves general capabilities
without any trade-offs. Specifically, fine-tuning with MIS increases average
accuracy by 0.83% across five general benchmarks and reduces the Attack Success
Rate (ASR) on multiple safety benchmarks by a large margin. Data and Models are
released under:
\href{https://dripnowhy.github.io/MIS/}{\texttt{https://dripnowhy.github.io/MIS/}}
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Integrating Spatial and Frequency Information for Under-Display Camera
  Image Restoration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18517v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18517v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyusu Ahn, Jinpyo Kim, Chanwoo Park, JiSoo Kim, Jaejin Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Under-Display Camera (UDC) houses a digital camera lens under a display
panel. However, UDC introduces complex degradations such as noise, blur,
decrease in transmittance, and flare. Despite the remarkable progress, previous
research on UDC mainly focuses on eliminating diffraction in the spatial domain
and rarely explores its potential in the frequency domain. It is essential to
consider both the spatial and frequency domains effectively. For example,
degradations, such as noise and blur, can be addressed by local information
(e.g., CNN kernels in the spatial domain). At the same time, tackling flares
may require leveraging global information (e.g., the frequency domain). In this
paper, we revisit the UDC degradations in the Fourier space and figure out
intrinsic frequency priors that imply the presence of the flares. Based on this
observation, we propose a novel multi-level DNN architecture called SFIM. It
efficiently restores UDC-distorted images by integrating local and global (the
collective contribution of all points in the image) information. The
architecture exploits CNNs to capture local information and FFT-based models to
capture global information. SFIM comprises a spatial domain block (SDB), a
Frequency Domain Block (FDB), and an Attention-based Multi-level Integration
Block (AMIB). Specifically, SDB focuses more on detailed textures such as noise
and blur, FDB emphasizes irregular texture loss in extensive areas such as
flare, and AMIB enables effective cross-domain interaction. SFIM's superior
performance over state-of-the-art approaches is demonstrated through rigorous
quantitative and qualitative assessments across three UDC benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Main body (10 pages, 9 Figures, 5 Tables), References (3 pages),
  Appendix (8 pages, 6 Figures, 6 Tables)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deconstruct Complexity (DeComplex): A Novel Perspective on Tackling
  Dense Action Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18509v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18509v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Faegheh Sardari, Armin Mustafa, Philip J. B. Jackson, Adrian Hilton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dense action detection involves detecting multiple co-occurring actions in an
untrimmed video while action classes are often ambiguous and represent
overlapping concepts. To address this challenge task, we introduce a novel
perspective inspired by how humans tackle complex tasks by breaking them into
manageable sub-tasks. Instead of relying on a single network to address the
entire problem, as in current approaches, we propose decomposing the problem
into detecting key concepts present in action classes, specifically, detecting
dense static concepts and detecting dense dynamic concepts, and assigning them
to distinct, specialized networks. Furthermore, simultaneous actions in a video
often exhibit interrelationships, and exploiting these relationships can
improve performance. However, we argue that current networks fail to
effectively learn these relationships due to their reliance on binary
cross-entropy optimization, which treats each class independently. To address
this limitation, we propose providing explicit supervision on co-occurring
concepts during network optimization through a novel language-guided
contrastive learning loss. Our extensive experiments demonstrate the
superiority of our approach over state-of-the-art methods, achieving
substantial relative improvements of 23.4% and 2.5% mAP on the challenging
benchmark datasets, Charades and MultiTHUMOS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Computer Vision</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CLEAR: Cue Learning using Evolution for Accurate Recognition Applied to
  Sustainability Data Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18504v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18504v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter J. Bentley, Soo Ling Lim, Fuyuki Ishikawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Model (LLM) image recognition is a powerful tool for
extracting data from images, but accuracy depends on providing sufficient cues
in the prompt - requiring a domain expert for specialized tasks. We introduce
Cue Learning using Evolution for Accurate Recognition (CLEAR), which uses a
combination of LLMs and evolutionary computation to generate and optimize cues
such that recognition of specialized features in images is improved. It
achieves this by auto-generating a novel domain-specific representation and
then using it to optimize suitable textual cues with a genetic algorithm. We
apply CLEAR to the real-world task of identifying sustainability data from
interior and exterior images of buildings. We investigate the effects of using
a variable-length representation compared to fixed-length and show how LLM
consistency can be improved by refactoring from categorical to real-valued
estimates. We show that CLEAR enables higher accuracy compared to expert human
recognition and human-authored prompts in every task with error rates improved
by up to two orders of magnitude and an ablation study evincing solution
concision.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages plus 2 pages of supplemental material</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HSRMamba: Contextual Spatial-Spectral State Space Model for Single
  Hyperspectral Super-Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18500v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18500v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shi Chen, Lefei Zhang, Liangpei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mamba has demonstrated exceptional performance in visual tasks due to its
powerful global modeling capabilities and linear computational complexity,
offering considerable potential in hyperspectral image super-resolution
(HSISR). However, in HSISR, Mamba faces challenges as transforming images into
1D sequences neglects the spatial-spectral structural relationships between
locally adjacent pixels, and its performance is highly sensitive to input
order, which affects the restoration of both spatial and spectral details. In
this paper, we propose HSRMamba, a contextual spatial-spectral modeling state
space model for HSISR, to address these issues both locally and globally.
Specifically, a local spatial-spectral partitioning mechanism is designed to
establish patch-wise causal relationships among adjacent pixels in 3D features,
mitigating the local forgetting issue. Furthermore, a global spectral
reordering strategy based on spectral similarity is employed to enhance the
causal representation of similar pixels across both spatial and spectral
dimensions. Finally, experimental results demonstrate our HSRMamba outperforms
the state-of-the-art methods in quantitative quality and visual results. Code
will be available soon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Runway vs. Taxiway: Challenges in Automated Line Identification and
  Notation Approaches 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18494v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18494v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Parth Ganeriwala, Amy Alvarez, Abdullah AlQahtani, Siddhartha Bhattacharyya, Mohammed Abdul Hafeez Khan, Natasha Neogi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing complexity of autonomous systems has amplified the need for
accurate and reliable labeling of runway and taxiway markings to ensure
operational safety. Precise detection and labeling of these markings are
critical for tasks such as navigation, landing assistance, and ground control
automation. Existing labeling algorithms, like the Automated Line
Identification and Notation Algorithm (ALINA), have demonstrated success in
identifying taxiway markings but encounter significant challenges when applied
to runway markings. This limitation arises due to notable differences in line
characteristics, environmental context, and interference from elements such as
shadows, tire marks, and varying surface conditions. To address these
challenges, we modified ALINA by adjusting color thresholds and refining region
of interest (ROI) selection to better suit runway-specific contexts. While
these modifications yielded limited improvements, the algorithm still struggled
with consistent runway identification, often mislabeling elements such as the
horizon or non-relevant background features. This highlighted the need for a
more robust solution capable of adapting to diverse visual interferences. In
this paper, we propose integrating a classification step using a Convolutional
Neural Network (CNN) named AssistNet. By incorporating this classification
step, the detection pipeline becomes more resilient to environmental variations
and misclassifications. This work not only identifies the challenges but also
outlines solutions, paving the way for improved automated labeling techniques
essential for autonomous aviation systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at SysCon 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Track-On: <span class="highlight-title">Transformer</span>-based Online Point Tracking with Memory <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18487v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18487v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Görkay Aydemir, Xiongyi Cai, Weidi Xie, Fatma Güney
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we consider the problem of long-term point tracking, which
requires consistent identification of points across multiple frames in a video,
despite changes in appearance, lighting, perspective, and occlusions. We target
online tracking on a frame-by-frame basis, making it suitable for real-world,
streaming scenarios. Specifically, we introduce Track-On, a simple
transformer-based model designed for online long-term point tracking. Unlike
prior methods that depend on full temporal modeling, our model processes video
frames causally without access to future frames, leveraging two memory modules
-- spatial memory and context memory -- to capture temporal information and
maintain reliable point tracking over long time horizons. At inference time, it
employs patch classification and refinement to identify correspondences and
track points with high accuracy. Through extensive experiments, we demonstrate
that Track-On sets a new state-of-the-art for online models and delivers
superior or competitive results compared to offline approaches on seven
datasets, including the TAP-Vid benchmark. Our method offers a robust and
scalable solution for real-time tracking in diverse applications. Project page:
https://kuis-ai.github.io/track_on
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SimpleDepthPose: Fast and Reliable Human Pose Estimation with
  RGBD-Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18478v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18478v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Bermuth, Alexander Poeppel, Wolfgang Reif
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the rapidly advancing domain of computer vision, accurately estimating the
poses of multiple individuals from various viewpoints remains a significant
challenge, especially when reliability is a key requirement. This paper
introduces a novel algorithm that excels in multi-view, multi-person pose
estimation by incorporating depth information. An extensive evaluation
demonstrates that the proposed algorithm not only generalizes well to unseen
datasets, and shows a fast runtime performance, but also is adaptable to
different keypoints. To support further research, all of the work is publicly
accessible.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tuning Vision Foundation Model via Test-Time <span class="highlight-title">Prompt</span>-Guided Training for
  VFSS Segmentations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18474v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18474v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengxi Zeng, David Smithard, Alberto M Gambaruto, Tilo Burghardt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision foundation models have demonstrated exceptional generalization
capabilities in segmentation tasks for both generic and specialized images.
However, a performance gap persists between foundation models and
task-specific, specialized models. Fine-tuning foundation models on downstream
datasets is often necessary to bridge this gap. Unfortunately, obtaining fully
annotated ground truth for downstream datasets is both challenging and costly.
To address this limitation, we propose a novel test-time training paradigm that
enhances the performance of foundation models on downstream datasets without
requiring full annotations. Specifically, our method employs simple point
prompts to guide a test-time semi-self-supervised training task. The model
learns by resolving the ambiguity of the point prompt through various
augmentations. This approach directly tackles challenges in the medical imaging
field, where acquiring annotations is both time-intensive and expensive. We
conducted extensive experiments on our new Videofluoroscopy dataset (VFSS-5k)
for the instance segmentation task, achieving an average Dice coefficient of
0.868 across 12 anatomies with a single model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Benchmark and Evaluation for Real-World Out-of-Distribution Detection
  Using Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18463v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18463v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiho Noda, Atsuyuki Miyai, Qing Yu, Go Irie, Kiyoharu Aizawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Out-of-distribution (OOD) detection is a task that detects OOD samples during
inference to ensure the safety of deployed models. However, conventional
benchmarks have reached performance saturation, making it difficult to compare
recent OOD detection methods. To address this challenge, we introduce three
novel OOD detection benchmarks that enable a deeper understanding of method
characteristics and reflect real-world conditions. First, we present
ImageNet-X, designed to evaluate performance under challenging semantic shifts.
Second, we propose ImageNet-FS-X for full-spectrum OOD detection, assessing
robustness to covariate shifts (feature distribution shifts). Finally, we
propose Wilds-FS-X, which extends these evaluations to real-world datasets,
offering a more comprehensive testbed. Our experiments reveal that recent
CLIP-based OOD detection methods struggle to varying degrees across the three
proposed benchmarks, and none of them consistently outperforms the others. We
hope the community goes beyond specific benchmarks and includes more
challenging conditions reflecting real-world scenarios. The code is
https://github.com/hoshi23/OOD-X-Banchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transfer Learning for Keypoint Detection in Low-Resolution Thermal TUG
  Test Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18453v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18453v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei-Lun Chen, Chia-Yeh Hsieh, Yu-Hsiang Kao, Kai-Chun Liu, Sheng-Yu Peng, Yu Tsao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study presents a novel approach to human keypoint detection in
low-resolution thermal images using transfer learning techniques. We introduce
the first application of the Timed Up and Go (TUG) test in thermal image
computer vision, establishing a new paradigm for mobility assessment. Our
method leverages a MobileNetV3-Small encoder and a ViTPose decoder, trained
using a composite loss function that balances latent representation alignment
and heatmap accuracy. The model was evaluated using the Object Keypoint
Similarity (OKS) metric from the COCO Keypoint Detection Challenge. The
proposed model achieves better performance with AP, AP50, and AP75 scores of
0.861, 0.942, and 0.887 respectively, outperforming traditional supervised
learning approaches like Mask R-CNN and ViTPose-Base. Moreover, our model
demonstrates superior computational efficiency in terms of parameter count and
FLOPS. This research lays a solid foundation for future clinical applications
of thermal imaging in mobility assessment and rehabilitation monitoring.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AICAS 2025. This is the preprint version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Object Detection for Indoor Navigation Assistance: A
  Performance Evaluation of Real-Time Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18444v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18444v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhinav Pratap, Sushant Kumar, Suchinton Chakravarty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study addresses the need for accurate and efficient object detection in
assistive technologies for visually impaired individuals. We evaluate four
real-time object detection algorithms YOLO, SSD, Faster R-CNN, and Mask R-CNN
within the context of indoor navigation assistance. Using the Indoor Objects
Detection dataset, we analyze detection accuracy, processing speed, and
adaptability to indoor environments. Our findings highlight the trade-offs
between precision and efficiency, offering insights into selecting optimal
algorithms for realtime assistive navigation. This research advances adaptive
machine learning applications, enhancing indoor navigation solutions for the
visually impaired and promoting accessibility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 2 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SANA 1.5: Efficient Scaling of Training-Time and Inference-Time Compute
  in Linear Diffusion <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18427v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18427v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enze Xie, Junsong Chen, Yuyang Zhao, Jincheng Yu, Ligeng Zhu, Yujun Lin, Zhekai Zhang, Muyang Li, Junyu Chen, Han Cai, Bingchen Liu, Daquan Zhou, Song Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents SANA-1.5, a linear Diffusion Transformer for efficient
scaling in text-to-image generation. Building upon SANA-1.0, we introduce three
key innovations: (1) Efficient Training Scaling: A depth-growth paradigm that
enables scaling from 1.6B to 4.8B parameters with significantly reduced
computational resources, combined with a memory-efficient 8-bit optimizer. (2)
Model Depth Pruning: A block importance analysis technique for efficient model
compression to arbitrary sizes with minimal quality loss. (3) Inference-time
Scaling: A repeated sampling strategy that trades computation for model
capacity, enabling smaller models to match larger model quality at inference
time. Through these strategies, SANA-1.5 achieves a text-image alignment score
of 0.72 on GenEval, which can be further improved to 0.80 through inference
scaling, establishing a new SoTA on GenEval benchmark. These innovations enable
efficient model scaling across different compute budgets while maintaining high
quality, making high-quality image generation more accessible.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Task-based Regularization in Penalized Least-Squares for Binary Signal
  Detection Tasks in Medical Image Denoising 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18418v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18418v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wentao Chen, Tianming Xu, Weimin Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image denoising algorithms have been extensively investigated for medical
imaging. To perform image denoising, penalized least-squares (PLS) problems can
be designed and solved, in which the penalty term encodes prior knowledge of
the object being imaged. Sparsity-promoting penalties, such as total variation
(TV), have been a popular choice for regularizing image denoising problems.
However, such hand-crafted penalties may not be able to preserve task-relevant
information in measured image data and can lead to oversmoothed image
appearances and patchy artifacts that degrade signal detectability. Supervised
learning methods that employ convolutional neural networks (CNNs) have emerged
as a popular approach to denoising medical images. However, studies have shown
that CNNs trained with loss functions based on traditional image quality
measures can lead to a loss of task-relevant information in images. Some
previous works have investigated task-based loss functions that employ model
observers for training the CNN denoising models. However, such training
processes typically require a large number of noisy and ground-truth
(noise-free or low-noise) image data pairs. In this work, we propose a
task-based regularization strategy for use with PLS in medical image denoising.
The proposed task-based regularization is associated with the likelihood of
linear test statistics of noisy images for Gaussian noise models. The proposed
method does not require ground-truth image data and solves an individual
optimization problem for denoising each image. Computer-simulation studies are
conducted that consider a multivariate-normally distributed (MVN) lumpy
background and a binary texture background. It is demonstrated that the
proposed regularization strategy can effectively improve signal detectability
in denoised images.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient <span class="highlight-title">Transformer</span> for High Resolution Image Motion Deblurring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18403v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18403v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amanturdieva Akmaral, Muhammad Hamza Zafar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a comprehensive study and improvement of the Restormer
architecture for high-resolution image motion deblurring. We introduce
architectural modifications that reduce model complexity by 18.4% while
maintaining or improving performance through optimized attention mechanisms.
Our enhanced training pipeline incorporates additional transformations
including color jitter, Gaussian blur, and perspective transforms to improve
model robustness as well as a new frequency loss term. Extensive experiments on
the RealBlur-R, RealBlur-J, and Ultra-High-Definition Motion blurred (UHDM)
datasets demonstrate the effectiveness of our approach. The improved
architecture shows better convergence behavior and reduced training time while
maintaining competitive performance across challenging scenarios. We also
provide detailed ablation studies analyzing the impact of our modifications on
model behavior and performance. Our results suggest that thoughtful
architectural simplification combined with enhanced training strategies can
yield more efficient yet equally capable models for motion deblurring tasks.
Code and Data Available at: https://github.com/hamzafer/image-deblurring
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 18 figures Submitted as a preprint, no prior
  journal/conference submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MatIR: A Hybrid Mamba-<span class="highlight-title">Transformer</span> Image Restoration Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18401v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18401v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juan Wen, Weiyan Hou, Luc Van Gool, Radu Timofte
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, Transformers-based models have made significant progress in
the field of image restoration by leveraging their inherent ability to capture
complex contextual features. Recently, Mamba models have made a splash in the
field of computer vision due to their ability to handle long-range dependencies
and their significant computational efficiency compared to Transformers.
However, Mamba currently lags behind Transformers in contextual learning
capabilities. To overcome the limitations of these two models, we propose a
Mamba-Transformer hybrid image restoration model called MatIR. Specifically,
MatIR cross-cycles the blocks of the Transformer layer and the Mamba layer to
extract features, thereby taking full advantage of the advantages of the two
architectures. In the Mamba module, we introduce the Image Inpainting State
Space (IRSS) module, which traverses along four scan paths to achieve efficient
processing of long sequence data. In the Transformer module, we combine
triangular window-based local attention with channel-based global attention to
effectively activate the attention mechanism over a wider range of image
pixels. Extensive experimental results and ablation studies demonstrate the
effectiveness of our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2402.15648 by other authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cracks in concrete 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18376v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18376v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tin Barisin, Christian Jung, Anna Nowacka, Claudia Redenbach, Katja Schladitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Finding and properly segmenting cracks in images of concrete is a challenging
task. Cracks are thin and rough and being air filled do yield a very weak
contrast in 3D images obtained by computed tomography. Enhancing and segmenting
dark lower-dimensional structures is already demanding. The heterogeneous
concrete matrix and the size of the images further increase the complexity. ML
methods have proven to solve difficult segmentation problems when trained on
enough and well annotated data. However, so far, there is not much 3D image
data of cracks available at all, let alone annotated. Interactive annotation is
error-prone as humans can easily tell cats from dogs or roads without from
roads with cars but have a hard time deciding whether a thin and dark structure
seen in a 2D slice continues in the next one. Training networks by synthetic,
simulated images is an elegant way out, bears however its own challenges. In
this contribution, we describe how to generate semi-synthetic image data to
train CNN like the well known 3D U-Net or random forests for segmenting cracks
in 3D images of concrete. The thickness of real cracks varies widely, both,
within one crack as well as from crack to crack in the same sample. The
segmentation method should therefore be invariant with respect to scale
changes. We introduce the so-called RieszNet, designed for exactly this
purpose. Finally, we discuss how to generalize the ML crack segmentation
methods to other concrete types.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is a preprint of the chapter: T. Barisin, C. Jung, A. Nowacka,
  C. Redenbach, K. Schladitz: Cracks in concrete, published in Statistical
  Machine Learning for Engineering with Applications (LNCS), edited by J.
  Franke, A. Sch\"obel, reproduced with permission of Springer Nature
  Switzerland AG 2024. The final authenticated version is available online at:
  https://doi.org/10.1007/978-3-031-66253-9</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MedXpertQA: Benchmarking Expert-Level Medical Reasoning and
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18362v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18362v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Zuo, Shang Qu, Yifei Li, Zhangren Chen, Xuekai Zhu, Ermo Hua, Kaiyan Zhang, Ning Ding, Bowen Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce MedXpertQA, a highly challenging and comprehensive benchmark to
evaluate expert-level medical knowledge and advanced reasoning. MedXpertQA
includes 4,460 questions spanning 17 specialties and 11 body systems. It
includes two subsets, Text for text evaluation and MM for multimodal
evaluation. Notably, MM introduces expert-level exam questions with diverse
images and rich clinical information, including patient records and examination
results, setting it apart from traditional medical multimodal benchmarks with
simple QA pairs generated from image captions. MedXpertQA applies rigorous
filtering and augmentation to address the insufficient difficulty of existing
benchmarks like MedQA, and incorporates specialty board questions to improve
clinical relevance and comprehensiveness. We perform data synthesis to mitigate
data leakage risk and conduct multiple rounds of expert reviews to ensure
accuracy and reliability. We evaluate 16 leading models on MedXpertQA.
Moreover, medicine is deeply connected to real-world decision-making, providing
a rich and representative setting for assessing reasoning abilities beyond
mathematics and code. To this end, we develop a reasoning-oriented subset to
facilitate the assessment of o1-like models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Video-based Surgical Tool-tip and Keypoint Tracking using Multi-frame
  Context-driven Deep Learning Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18361v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18361v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bhargav Ghanekar, Lianne R. Johnson, Jacob L. Laughlin, Marcia K. O'Malley, Ashok Veeraraghavan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated tracking of surgical tool keypoints in robotic surgery videos is an
essential task for various downstream use cases such as skill assessment,
expertise assessment, and the delineation of safety zones. In recent years, the
explosion of deep learning for vision applications has led to many works in
surgical instrument segmentation, while lesser focus has been on tracking
specific tool keypoints, such as tool tips. In this work, we propose a novel,
multi-frame context-driven deep learning framework to localize and track tool
keypoints in surgical videos. We train and test our models on the annotated
frames from the 2015 EndoVis Challenge dataset, resulting in state-of-the-art
performance. By leveraging sophisticated deep learning models and multi-frame
context, we achieve 90\% keypoint detection accuracy and a localization RMS
error of 5.27 pixels. Results on a self-annotated JIGSAWS dataset with more
challenging scenarios also show that the proposed multi-frame models can
accurately track tool-tip and tool-base keypoints, with ${<}4.2$-pixel RMS
error overall. Such a framework paves the way for accurately tracking surgical
instrument keypoints, enabling further downstream use cases. Project and
dataset webpage: https://tinyurl.com/mfc-tracker
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CodeBrain: Impute Any Brain MRI via Instance-specific Scalar-quantized
  Codes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18328v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18328v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yicheng Wu, Tao Song, Zhonghua Wu, Zongyuan Ge, Zhaolin Chen, Jianfei Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  MRI imputation aims to synthesize the missing modality from one or more
available ones, which is highly desirable since it reduces scanning costs and
delivers comprehensive MRI information to enhance clinical diagnosis. In this
paper, we propose a unified model, CodeBrain, designed to adapt to various
brain MRI imputation scenarios. The core design lies in casting various
inter-modality transformations as a full-modality code prediction task. To this
end, CodeBrain is trained in two stages: Reconstruction and Code Prediction.
First, in the Reconstruction stage, we reconstruct each MRI modality, which is
mapped into a shared latent space followed by a scalar quantization. Since such
quantization is lossy and the code is low dimensional, another MRI modality
belonging to the same subject is randomly selected to generate common features
to supplement the code and boost the target reconstruction. In the second
stage, we train another encoder by a customized grading loss to predict the
full-modality codes from randomly masked MRI samples, supervised by the
corresponding quantized codes generated from the first stage. In this way, the
inter-modality transformation is achieved by mapping the instance-specific
codes in a finite scalar space. We evaluated the proposed CodeBrain model on
two public brain MRI datasets (i.e., IXI and BraTS 2023). Extensive experiments
demonstrate that our CodeBrain model achieves superior imputation performance
compared to four existing methods, establishing a new state of the art for
unified brain MRI imputation. Codes will be released.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Video-grounded Dialogue <span class="highlight-title">Dataset</span> and Metric for Event-driven Activities <span class="chip">AAAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18324v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18324v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wiradee Imrattanatrai, Masaki Asada, Kimihiro Hasegawa, Zhi-Qi Cheng, Ken Fukuda, Teruko Mitamura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents VDAct, a dataset for a Video-grounded Dialogue on
Event-driven Activities, alongside VDEval, a session-based context evaluation
metric specially designed for the task. Unlike existing datasets, VDAct
includes longer and more complex video sequences that depict a variety of
event-driven activities that require advanced contextual understanding for
accurate response generation. The dataset comprises 3,000 dialogues with over
30,000 question-and-answer pairs, derived from 1,000 videos with diverse
activity scenarios. VDAct displays a notably challenging characteristic due to
its broad spectrum of activity scenarios and wide range of question types.
Empirical studies on state-of-the-art vision foundation models highlight their
limitations in addressing certain question types on our dataset. Furthermore,
VDEval, which integrates dialogue session history and video content summaries
extracted from our supplementary Knowledge Graphs to evaluate individual
responses, demonstrates a significantly higher correlation with human
assessments on the VDAct dataset than existing evaluation metrics that rely
solely on the context of single dialogue turns.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AAAI2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Surface Defect Identification using Bayesian Filtering on a 3D Mesh 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18315v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18315v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Dalle Vedove, Matteo Bonetto, Edoardo Lamon, Luigi Palopoli, Matteo Saveriano, Daniele Fontanelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a CAD-based approach for automated surface defect
detection. We leverage the a-priori knowledge embedded in a CAD model and
integrate it with point cloud data acquired from commercially available stereo
and depth cameras. The proposed method first transforms the CAD model into a
high-density polygonal mesh, where each vertex represents a state variable in
3D space. Subsequently, a weighted least squares algorithm is employed to
iteratively estimate the state of the scanned workpiece based on the captured
point cloud measurements. This framework offers the potential to incorporate
information from diverse sensors into the CAD domain, facilitating a more
comprehensive analysis. Preliminary results demonstrate promising performance,
with the algorithm achieving convergence to a sub-millimeter standard deviation
in the region of interest using only approximately 50 point cloud samples. This
highlights the potential of utilising commercially available stereo cameras for
high-precision quality control applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at IMEKO2024 World Congress, Hamburg, Germany, 26-29
  October 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AGAV-Rater: Adapting Large Multimodal Model for AI-Generated
  Audio-Visual Quality Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18314v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18314v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuqin Cao, Xiongkuo Min, Yixuan Gao, Wei Sun, Guangtao Zhai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many video-to-audio (VTA) methods have been proposed for dubbing silent
AI-generated videos. An efficient quality assessment method for AI-generated
audio-visual content (AGAV) is crucial for ensuring audio-visual quality.
Existing audio-visual quality assessment methods struggle with unique
distortions in AGAVs, such as unrealistic and inconsistent elements. To address
this, we introduce AGAVQA, the first large-scale AGAV quality assessment
dataset, comprising 3,382 AGAVs from 16 VTA methods. AGAVQA includes two
subsets: AGAVQA-MOS, which provides multi-dimensional scores for audio quality,
content consistency, and overall quality, and AGAVQA-Pair, designed for optimal
AGAV pair selection. We further propose AGAV-Rater, a LMM-based model that can
score AGAVs, as well as audio and music generated from text, across multiple
dimensions, and selects the best AGAV generated by VTA methods to present to
the user. AGAV-Rater achieves state-of-the-art performance on AGAVQA,
Text-to-Audio, and Text-to-Music datasets. Subjective tests also confirm that
AGAV-Rater enhances VTA performance and user experience. The project page is
available at https://agav-rater.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Simulation of microstructures and machine learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18313v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18313v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Katja Schladitz, Claudia Redenbach, Tin Barisin, Christian Jung, Natascha Jeziorski, Lovro Bosnar, Juraj Fulir, Petra Gospodnetić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning offers attractive solutions to challenging image processing
tasks. Tedious development and parametrization of algorithmic solutions can be
replaced by training a convolutional neural network or a random forest with a
high potential to generalize. However, machine learning methods rely on huge
amounts of representative image data along with a ground truth, usually
obtained by manual annotation. Thus, limited availability of training data is a
critical bottleneck. We discuss two use cases: optical quality control in
industrial production and segmenting crack structures in 3D images of concrete.
For optical quality control, all defect types have to be trained but are
typically not evenly represented in the training data. Additionally, manual
annotation is costly and often inconsistent. It is nearly impossible in the
second case: segmentation of crack systems in 3D images of concrete. Synthetic
images, generated based on realizations of stochastic geometry models, offer an
elegant way out. A wide variety of structure types can be generated. The within
structure variation is naturally captured by the stochastic nature of the
models and the ground truth is for free. Many new questions arise. In
particular, which characteristics of the real image data have to be met to
which degree of fidelity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint of: K. Schladitz, C. Redenbach, T. Barisin, C. Jung, N.
  Jeziorski, L. Bosnar, J. Fulir, P. Gospodneti\'c: Simulation of
  Microstructures and Machine Learning, published in Continuum Models and
  Discrete Systems by F. Willot, J. Dirrenberger, S. Forest, D. Jeulin, A.V.
  Cherkaev (eds), 2024, Springer Cham. The final version is
  https://doi.org/10.1007/978-3-031-58665-1</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comprehensive Analysis on Machine Learning based Methods for Lung
  Cancer Level Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18294v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18294v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shayli Farshchiha, Salman Asoudeh, Maryam Shavali Kuhshuri, Mehrshad Eisaeid, Mohamadreza Azadie, Saba Hesaraki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lung cancer is a major issue in worldwide public health, requiring early
diagnosis using stable techniques. This work begins a thorough investigation of
the use of machine learning (ML) methods for precise classification of lung
cancer stages. A cautious analysis is performed to overcome overfitting issues
in model performance, taking into account minimum child weight and learning
rate. A set of machine learning (ML) models including XGBoost (XGB), LGBM,
Adaboost, Logistic Regression (LR), Decision Tree (DT), Random Forest (RF),
CatBoost, and k-Nearest Neighbor (k-NN) are run methodically and contrasted.
Furthermore, the correlation between features and targets is examined using the
deep neural network (DNN) model and thus their capability in detecting complex
patternsis established. It is argued that several ML models can be capable of
classifying lung cancer stages with great accuracy. In spite of the complexity
of DNN architectures, traditional ML models like XGBoost, LGBM, and Logistic
Regression excel with superior performance. The models perform better than the
others in lung cancer prediction on the complete set of comparative metrics
like accuracy, precision, recall, and F-1 score
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The iToBoS <span class="highlight-title">dataset</span>: skin region images extracted from 3D total body
  photographs for lesion detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18270v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18270v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anup Saha, Joseph Adeola, Nuria Ferrera, Adam Mothershaw, Gisele Rezze, Séraphin Gaborit, Brian D'Alessandro, James Hudson, Gyula Szabó, Balazs Pataki, Hayat Rajani, Sana Nazari, Hassan Hayat, Clare Primiero, H. Peter Soyer, Josep Malvehy, Rafael Garcia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence has significantly advanced skin cancer diagnosis by
enabling rapid and accurate detection of malignant lesions. In this domain,
most publicly available image datasets consist of single, isolated skin lesions
positioned at the center of the image. While these lesion-centric datasets have
been fundamental for developing diagnostic algorithms, they lack the context of
the surrounding skin, which is critical for improving lesion detection. The
iToBoS dataset was created to address this challenge. It includes 16,954 images
of skin regions from 100 participants, captured using 3D total body
photography. Each image roughly corresponds to a $7 \times 9$ cm section of
skin with all suspicious lesions annotated using bounding boxes. Additionally,
the dataset provides metadata such as anatomical location, age group, and sun
damage score for each image. This dataset aims to facilitate training and
benchmarking of algorithms, with the goal of enabling early detection of skin
cancer and deployment of this technology in non-clinical environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Article Submitted to Scientific Data</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MAMS: Model-Agnostic Module Selection Framework for Video Captioning <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18269v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18269v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sangho Lee, Il Yong Chun, Hogun Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal transformers are rapidly gaining attention in video captioning
tasks. Existing multi-modal video captioning methods typically extract a fixed
number of frames, which raises critical challenges. When a limited number of
frames are extracted, important frames with essential information for caption
generation may be missed. Conversely, extracting an excessive number of frames
includes consecutive frames, potentially causing redundancy in visual tokens
extracted from consecutive video frames. To extract an appropriate number of
frames for each video, this paper proposes the first model-agnostic module
selection framework in video captioning that has two main functions: (1)
selecting a caption generation module with an appropriate size based on visual
tokens extracted from video frames, and (2) constructing subsets of visual
tokens for the selected caption generation module. Furthermore, we propose a
new adaptive attention masking scheme that enhances attention on important
visual tokens. Our experiments on three different benchmark datasets
demonstrate that the proposed framework significantly improves the performance
of three recent video captioning models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the AAAI 2025 Main Technical Track. This is an extended
  version of the original submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ground Awareness in Deep Learning for Large Outdoor Point Cloud
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18246v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18246v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Qiu, Dimitri Bulatov, Dorota Iwaszczuk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an analysis of utilizing elevation data to aid outdoor
point cloud semantic segmentation through existing machine-learning networks in
remote sensing, specifically in urban, built-up areas. In dense outdoor point
clouds, the receptive field of a machine learning model may be too small to
accurately determine the surroundings and context of a point. By computing
Digital Terrain Models (DTMs) from the point clouds, we extract the relative
elevation feature, which is the vertical distance from the terrain to a point.
RandLA-Net is employed for efficient semantic segmentation of large-scale point
clouds. We assess its performance across three diverse outdoor datasets
captured with varying sensor technologies and sensor locations. Integration of
relative elevation data leads to consistent performance improvements across all
three datasets, most notably in the Hessigheim dataset, with an increase of 3.7
percentage points in average F1 score from 72.35% to 76.01%, by establishing
long-range dependencies between ground and objects. We also explore additional
local features such as planarity, normal vectors, and 2D features, but their
efficacy varied based on the characteristics of the point cloud. Ultimately,
this study underscores the important role of the non-local relative elevation
feature for semantic segmentation of point clouds in remote sensing
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted for presentation at the GRAPP 2025
  conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Arbitrary Data as Images: Fusion of Patient Data Across Modalities and
  Irregular Intervals with Vision <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18237v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18237v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Malte Tölle, Mohamad Scharaf, Samantha Fischer, Christoph Reich, Silav Zeid, Christoph Dieterich, Benjamin Meder, Norbert Frey, Philipp Wild, Sandy Engelhardt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A patient undergoes multiple examinations in each hospital stay, where each
provides different facets of the health status. These assessments include
temporal data with varying sampling rates, discrete single-point measurements,
therapeutic interventions such as medication administration, and images. While
physicians are able to process and integrate diverse modalities intuitively,
neural networks need specific modeling for each modality complicating the
training procedure. We demonstrate that this complexity can be significantly
reduced by visualizing all information as images along with unstructured text
and subsequently training a conventional vision-text transformer. Our approach,
Vision Transformer for irregular sampled Multi-modal Measurements (ViTiMM), not
only simplifies data preprocessing and modeling but also outperforms current
state-of-the-art methods in predicting in-hospital mortality and phenotyping,
as evaluated on 6,175 patients from the MIMIC-IV dataset. The modalities
include patient's clinical measurements, medications, X-ray images, and
electrocardiography scans. We hope our work inspires advancements in
multi-modal medical AI by reducing the training complexity to (visual) prompt
engineering, thus lowering entry barriers and enabling no-code solutions for
training. The source code will be made publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Free-T2M: Frequency Enhanced Text-to-Motion Diffusion Model With
  Consistency Loss 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18232v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18232v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenshuo Chen, Haozhe Jia, Songning Lai, Keming Wu, Hongru Xiao, Lijie Hu, Yutao Yue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rapid progress in text-to-motion generation has been largely driven by
diffusion models. However, existing methods focus solely on temporal modeling,
thereby overlooking frequency-domain analysis. We identify two key phases in
motion denoising: the **semantic planning stage** and the **fine-grained
improving stage**. To address these phases effectively, we propose
**Fre**quency **e**nhanced **t**ext-**to**-**m**otion diffusion model
(**Free-T2M**), incorporating stage-specific consistency losses that enhance
the robustness of static features and improve fine-grained accuracy. Extensive
experiments demonstrate the effectiveness of our method. Specifically, on
StableMoFusion, our method reduces the FID from **0.189** to **0.051**,
establishing a new SOTA performance within the diffusion architecture. These
findings highlight the importance of incorporating frequency-domain insights
into text-to-motion generation for more precise and robust results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting $Ψ$DONet: microlocally inspired filters for
  incomplete-data tomographic reconstructions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18219v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18219v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tatiana A. Bubba, Luca Ratti, Andrea Sebastiani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we revisit a supervised learning approach based on unrolling,
known as $\Psi$DONet, by providing a deeper microlocal interpretation for its
theoretical analysis, and extending its study to the case of sparse-angle
tomography. Furthermore, we refine the implementation of the original
$\Psi$DONet considering special filters whose structure is specifically
inspired by the streak artifact singularities characterizing tomographic
reconstructions from incomplete data. This allows to considerably lower the
number of (learnable) parameters while preserving (or even slightly improving)
the same quality for the reconstructions from limited-angle data and providing
a proof-of-concept for the case of sparse-angle tomographic data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Machine Learning Fairness for Depression Detection using EEG Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18192v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18192v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Angus Man Ho Kwok, Jiaee Cheong, Sinan Kalkan, Hatice Gunes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the very first attempt to evaluate machine learning
fairness for depression detection using electroencephalogram (EEG) data. We
conduct experiments using different deep learning architectures such as
Convolutional Neural Networks (CNN), Long Short-Term Memory (LSTM) networks,
and Gated Recurrent Unit (GRU) networks across three EEG datasets: Mumtaz,
MODMA and Rest. We employ five different bias mitigation strategies at the
pre-, in- and post-processing stages and evaluate their effectiveness. Our
experimental results show that bias exists in existing EEG datasets and
algorithms for depression detection, and different bias mitigation methods
address bias at different levels across different fairness measures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear as part of the International Symposium on Biomedical
  Imaging (ISBI) 2025 proceedings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scattering approach to diffusion quantifies axonal damage in brain
  injury 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18167v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18167v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Abdollahzadeh, Ricardo Coronado-Leija, Hong-Hsi Lee, Alejandra Sierra, Els Fieremans, Dmitry S. Novikov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Early diagnosis and noninvasive monitoring of neurological disorders require
sensitivity to elusive cellular-level alterations that occur much earlier than
volumetric changes observable with the millimeter-resolution of medical imaging
modalities. Morphological changes in axons, such as axonal varicosities or
beadings, are observed in neurological disorders, as well as in development and
aging. Here, we reveal the sensitivity of time-dependent diffusion MRI (dMRI)
to axonal morphology at the micrometer scale. Scattering theory uncovers the
two parameters that determine the diffusive dynamics of water in axons: the
average reciprocal cross-section and the variance of long-range cross-sectional
fluctuations. This theoretical development allowed us to predict dMRI metrics
sensitive to axonal alterations across tens of thousands of axons in seconds
rather than months of simulations in a rat model of traumatic brain injury. Our
approach bridges the gap between micrometers and millimeters in resolution,
offering quantitative, objective biomarkers applicable to a broad spectrum of
neurological disorders.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IROAM: Improving Roadside Monocular 3D Object Detection Learning from
  Autonomous Vehicle Data Domain <span class="chip">ICRA2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18162v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18162v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe Wang, Xiaoliang Huo, Siqi Fan, Jingjing Liu, Ya-Qin Zhang, Yan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In autonomous driving, The perception capabilities of the ego-vehicle can be
improved with roadside sensors, which can provide a holistic view of the
environment. However, existing monocular detection methods designed for vehicle
cameras are not suitable for roadside cameras due to viewpoint domain gaps. To
bridge this gap and Improve ROAdside Monocular 3D object detection, we propose
IROAM, a semantic-geometry decoupled contrastive learning framework, which
takes vehicle-side and roadside data as input simultaneously. IROAM has two
significant modules. In-Domain Query Interaction module utilizes a transformer
to learn content and depth information for each domain and outputs object
queries. Cross-Domain Query Enhancement To learn better feature representations
from two domains, Cross-Domain Query Enhancement decouples queries into
semantic and geometry parts and only the former is used for contrastive
learning. Experiments demonstrate the effectiveness of IROAM in improving
roadside detector's performance. The results validate that IROAM has the
capabilities to learn cross-domain information.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 5 figures, ICRA2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Using Computer Vision for Skin Disease Diagnosis in Bangladesh Enhancing
  Interpretability and Transparency in Deep Learning Models for Skin Cancer
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18161v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18161v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rafiul Islam, Jihad Khan Dipu, Mehedi Hasan Tusar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With over 2 million new cases identified annually, skin cancer is the most
prevalent type of cancer globally and the second most common in Bangladesh,
following breast cancer. Early detection and treatment are crucial for
enhancing patient outcomes; however, Bangladesh faces a shortage of
dermatologists and qualified medical professionals capable of diagnosing and
treating skin cancer. As a result, many cases are diagnosed only at advanced
stages. Research indicates that deep learning algorithms can effectively
classify skin cancer images. However, these models typically lack
interpretability, making it challenging to understand their decision-making
processes. This lack of clarity poses barriers to utilizing deep learning in
improving skin cancer detection and treatment. In this article, we present a
method aimed at enhancing the interpretability of deep learning models for skin
cancer classification in Bangladesh. Our technique employs a combination of
saliency maps and attention maps to visualize critical features influencing the
model's diagnoses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Audiovisual Speech Processing via MUTUD: Multimodal Training
  and Unimodal Deployment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18157v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18157v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joanna Hong, Sanjeel Parekh, Honglie Chen, Jacob Donley, Ke Tan, Buye Xu, Anurag Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building reliable speech systems often requires combining multiple
modalities, like audio and visual cues. While such multimodal solutions
frequently lead to improvements in performance and may even be critical in
certain cases, they come with several constraints such as increased sensory
requirements, computational cost, and modality synchronization, to mention a
few. These challenges constrain the direct uses of these multimodal solutions
in real-world applications. In this work, we develop approaches where the
learning happens with all available modalities but the deployment or inference
is done with just one or reduced modalities. To do so, we propose a Multimodal
Training and Unimodal Deployment (MUTUD) framework which includes a Temporally
Aligned Modality feature Estimation (TAME) module that can estimate information
from missing modality using modalities present during inference. This
innovative approach facilitates the integration of information across different
modalities, enhancing the overall inference process by leveraging the strengths
of each modality to compensate for the absence of certain modalities during
inference. We apply MUTUD to various audiovisual speech tasks and show that it
can reduce the performance gap between the multimodal and corresponding
unimodal models to a considerable extent. MUTUD can achieve this while reducing
the model size and compute compared to multimodal models, in some cases by
almost 80%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ REMOTE: Real-time Ego-motion Tracking for Various Endoscopes via
  Multimodal Visual Feature Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18124v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18124v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liangjing Shao, Benshuang Chen, Shuting Zhao, Xinrong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-time ego-motion tracking for endoscope is a significant task for
efficient navigation and robotic automation of endoscopy. In this paper, a
novel framework is proposed to perform real-time ego-motion tracking for
endoscope. Firstly, a multi-modal visual feature learning network is proposed
to perform relative pose prediction, in which the motion feature from the
optical flow, the scene features and the joint feature from two adjacent
observations are all extracted for prediction. Due to more correlation
information in the channel dimension of the concatenated image, a novel feature
extractor is designed based on an attention mechanism to integrate
multi-dimensional information from the concatenation of two continuous frames.
To extract more complete feature representation from the fused features, a
novel pose decoder is proposed to predict the pose transformation from the
concatenated feature map at the end of the framework. At last, the absolute
pose of endoscope is calculated based on relative poses. The experiment is
conducted on three datasets of various endoscopic scenes and the results
demonstrate that the proposed method outperforms state-of-the-art methods.
Besides, the inference speed of the proposed method is over 30 frames per
second, which meets the real-time requirement. The project page is here:
\href{https://remote-bmxs.netlify.app}{remote-bmxs.netlify.app}
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeepFRC: An End-to-End Deep Learning Model for Functional Registration
  and Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18116v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18116v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyuan Jiang, Yihan Hu, Wenjie Li, Pengcheng Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Functional data analysis (FDA) is essential for analyzing continuous,
high-dimensional data, yet existing methods often decouple functional
registration and classification, limiting their efficiency and performance. We
present DeepFRC, an end-to-end deep learning framework that unifies these tasks
within a single model. Our approach incorporates an alignment module that
learns time warping functions via elastic function registration and a learnable
basis representation module for dimensionality reduction on aligned data. This
integration enhances both alignment accuracy and predictive performance.
Theoretical analysis establishes that DeepFRC achieves low misalignment and
generalization error, while simulations elucidate the progression of
registration, reconstruction, and classification during training. Experiments
on real-world datasets demonstrate that DeepFRC consistently outperforms
state-of-the-art methods, particularly in addressing complex registration
challenges. Code is available at: https://github.com/Drivergo-93589/DeepFRC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lifelong 3D Mapping Framework for Hand-held & Robot-mounted LiDAR
  Mapping Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18110v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18110v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liudi Yang, Sai Manoj Prakhya, Senhua Zhu, Ziyuan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a lifelong 3D mapping framework that is modular, cloud-native by
design and more importantly, works for both hand-held and robot-mounted 3D
LiDAR mapping systems. Our proposed framework comprises of dynamic point
removal, multi-session map alignment, map change detection and map version
control. First, our sensor-setup agnostic dynamic point removal algorithm works
seamlessly with both hand-held and robot-mounted setups to produce clean static
3D maps. Second, the multi-session map alignment aligns these clean static maps
automatically, without manual parameter fine-tuning, into a single reference
frame, using a two stage approach based on feature descriptor matching and fine
registration. Third, our novel map change detection identifies positive and
negative changes between two aligned maps. Finally, the map version control
maintains a single base map that represents the current state of the
environment, and stores the detected positive and negative changes, and
boundary information. Our unique map version control system can reconstruct any
of the previous clean session maps and allows users to query changes between
any two random mapping sessions, all without storing any input raw session
maps, making it very unique. Extensive experiments are performed using
hand-held commercial LiDAR mapping devices and open-source robot-mounted LiDAR
SLAM algorithms to evaluate each module and the whole 3D lifelong mapping
framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Influence of High-Performance Image-to-Image Translation Networks on
  Clinical Visual Assessment and Outcome Prediction: Utilizing Ultrasound to
  MRI Translation in Prostate Cancer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18109v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18109v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad R. Salmanpour, Amin Mousavi, Yixi Xu, William B Weeks, Ilker Hacihaliloglu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Purpose: This study examines the core traits of image-to-image translation
(I2I) networks, focusing on their effectiveness and adaptability in everyday
clinical settings. Methods: We have analyzed data from 794 patients diagnosed
with prostate cancer (PCa), using ten prominent 2D/3D I2I networks to convert
ultrasound (US) images into MRI scans. We also introduced a new analysis of
Radiomic features (RF) via the Spearman correlation coefficient to explore
whether networks with high performance (SSIM>85%) could detect subtle RFs. Our
study further examined synthetic images by 7 invited physicians. As a final
evaluation study, we have investigated the improvement that are achieved using
the synthetic MRI data on two traditional machine learning and one deep
learning method. Results: In quantitative assessment, 2D-Pix2Pix network
substantially outperformed the other 7 networks, with an average SSIM~0.855.
The RF analysis revealed that 76 out of 186 RFs were identified using the
2D-Pix2Pix algorithm alone, although half of the RFs were lost during the
translation process. A detailed qualitative review by 7 medical doctors noted a
deficiency in low-level feature recognition in I2I tasks. Furthermore, the
study found that synthesized image-based classification outperformed US
image-based classification with an average accuracy and AUC~0.93. Conclusion:
This study showed that while 2D-Pix2Pix outperformed cutting-edge networks in
low-level feature discovery and overall error and similarity metrics, it still
requires improvement in low-level feature performance, as highlighted by Group
3. Further, the study found using synthetic image-based classification
outperformed original US image-based methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures and 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Disentangling Safe and Unsafe Corruptions via Anisotropy and Locality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18098v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18098v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ramchandran Muthukumar, Ambar Pal, Jeremias Sulam, Rene Vidal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art machine learning systems are vulnerable to small
perturbations to their input, where ``small'' is defined according to a threat
model that assigns a positive threat to each perturbation. Most prior works
define a task-agnostic, isotropic, and global threat, like the $\ell_p$ norm,
where the magnitude of the perturbation fully determines the degree of the
threat and neither the direction of the attack nor its position in space
matter. However, common corruptions in computer vision, such as blur,
compression, or occlusions, are not well captured by such threat models. This
paper proposes a novel threat model called \texttt{Projected Displacement} (PD)
to study robustness beyond existing isotropic and global threat models. The
proposed threat model measures the threat of a perturbation via its alignment
with \textit{unsafe directions}, defined as directions in the input space along
which a perturbation of sufficient magnitude changes the ground truth class
label. Unsafe directions are identified locally for each input based on
observed training data. In this way, the PD threat model exhibits anisotropy
and locality. Experiments on Imagenet-1k data indicate that, for any input, the
set of perturbations with small PD threat includes \textit{safe} perturbations
of large $\ell_p$ norm that preserve the true label, such as noise, blur and
compression, while simultaneously excluding \textit{unsafe} perturbations that
alter the true label. Unlike perceptual threat models based on embeddings of
large-vision models, the PD threat model can be readily computed for arbitrary
classification tasks without pre-training or finetuning. Further additional
task annotation such as sensitivity to image regions or concept hierarchies can
be easily integrated into the assessment of threat and thus the PD threat model
presents practitioners with a flexible, task-driven threat specification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLMs can see and hear without any training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18096v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18096v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kumar Ashutosh, Yossi Gandelsman, Xinlei Chen, Ishan Misra, Rohit Girdhar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present MILS: Multimodal Iterative LLM Solver, a surprisingly simple,
training-free approach, to imbue multimodal capabilities into your favorite
LLM. Leveraging their innate ability to perform multi-step reasoning, MILS
prompts the LLM to generate candidate outputs, each of which are scored and fed
back iteratively, eventually generating a solution to the task. This enables
various applications that typically require training specialized models on
task-specific data. In particular, we establish a new state-of-the-art on
emergent zero-shot image, video and audio captioning. MILS seamlessly applies
to media generation as well, discovering prompt rewrites to improve
text-to-image generation, and even edit prompts for style transfer! Finally,
being a gradient-free optimization approach, MILS can invert multimodal
embeddings into text, enabling applications like cross-modal arithmetic.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/facebookresearch/MILS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vision-based autonomous structural damage detection using data-driven
  methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16662v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16662v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seyyed Taghi Ataei, Parviz Mohammad Zadeh, Saeid Ataei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study addresses the urgent need for efficient and accurate damage
detection in wind turbine structures, a crucial component of renewable energy
infrastructure. Traditional inspection methods, such as manual assessments and
non-destructive testing (NDT), are often costly, time-consuming, and prone to
human error. To tackle these challenges, this research investigates advanced
deep learning algorithms for vision-based structural health monitoring (SHM). A
dataset of wind turbine surface images, featuring various damage types and
pollution, was prepared and augmented for enhanced model training. Three
algorithms-YOLOv7, its lightweight variant, and Faster R-CNN- were employed to
detect and classify surface damage. The models were trained and evaluated on a
dataset split into training, testing, and evaluation subsets (80%-10%-10%).
Results indicate that YOLOv7 outperformed the others, achieving 82.4% mAP@50
and high processing speed, making it suitable for real-time inspections. By
optimizing hyperparameters like learning rate and batch size, the models'
accuracy and efficiency improved further. YOLOv7 demonstrated significant
advancements in detection precision and execution speed, especially for
real-time applications. However, challenges such as dataset limitations and
environmental variability were noted, suggesting future work on segmentation
methods and larger datasets. This research underscores the potential of
vision-based deep learning techniques to transform SHM practices by reducing
costs, enhancing safety, and improving reliability, thus contributing to the
sustainable maintenance of critical infrastructure and supporting the longevity
of wind energy systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 8 figures. This study examines advanced deep learning
  algorithms, specifically YOLOv7, for efficient and accurate damage detection
  in wind turbine structures. It significantly enhances detection precision and
  speed for real-time inspections</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Perspectives: Comparison of Deep Learning Segmentation Models on
  Biophysical and Biomedical Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.07786v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.07786v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        J Shepard Bryan IV, Pedro Pessoa, Meyam Tavakoli, Steve Presse
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning based approaches are now widely used across biophysics to help
automate a variety of tasks including image segmentation, feature selection,
and deconvolution. However, the presence of multiple competing deep learning
architectures, each with its own unique advantages and disadvantages, makes it
challenging to select an architecture best suited for a specific application.
As such, we present a comprehensive comparison of common models. Here, we focus
on the task of segmentation assuming the typically small training dataset sizes
available from biophysics experiments and compare the following four commonly
used architectures: convolutional neural networks, U-Nets, vision transformers,
and vision state space models. In doing so, we establish criteria for
determining optimal conditions under which each model excels, thereby offering
practical guidelines for researchers and practitioners in the field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ R-LLaVA: Improving Med-VQA Understanding through Visual Region of
  Interest 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20327v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20327v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xupeng Chen, Zhixin Lai, Kangrui Ruan, Shichu Chen, Jiaxiang Liu, Zuozhu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence has made significant strides in medical visual
question answering (Med-VQA), yet prevalent studies often interpret images
holistically, overlooking the visual regions of interest that may contain
crucial information, potentially aligning with a doctor's prior knowledge that
can be incorporated with minimal annotations (e.g., bounding boxes). To address
this gap, this paper introduces R-LLaVA, designed to enhance biomedical VQA
understanding by integrating simple medical annotations as prior knowledge
directly into the image space through CLIP. These annotated visual regions of
interest are then fed into the LLaVA model during training, aiming to enrich
the model's understanding of biomedical queries. Experimental evaluation on
four standard Med-VQA datasets demonstrates R-LLaVA's superiority over existing
state-of-the-art (SoTA) methods. Additionally, to verify the model's capability
in visual comprehension, a novel multiple-choice medical visual understanding
dataset is introduced, confirming the positive impact of focusing on visual
regions of interest in advancing biomedical VQA understanding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Technical report on label-informed logit redistribution for better
  domain generalization in low-shot classification with foundation models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17595v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17595v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Behraj Khan, Tahir Syed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Confidence calibration is an emerging challenge in real-world decision
systems based on foundations models when used for downstream vision
classification tasks. Due to various reasons exposed, logit scores on the CLIP
head remain large irrespective of whether the image-language pairs reconcile.
It is difficult to address in data space, given the few-shot regime. We propose
a penalty incorporated into loss objective that penalizes incorrect
classifications whenever one is made during finetuning, by moving an amount of
log-likelihood to the true class commensurate to the relative amplitudes of the
two likelihoods. We refer to it as \textit{confidence misalignment penalty
(CMP)}. Extensive experiments on $12$ vision datasets and $5$ domain
generalization datasets supports the calibration performance of our method
against stat-of-the-art. CMP outperforms the benchmarked prompt learning
methods, demonstrating average improvement in Expected Calibration Error (ECE)
by average $6.01$\%, $4.01$ \% at minimum and $9.72$\% at maximum.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ In-Context Meta LoRA Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17635v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17635v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihua Shao, Minxi Yan, Yang Liu, Siyu Chen, Wenjie Chen, Xinwei Long, Ziyang Yan, Lei Li, Chenyu Zhang, Nicu Sebe, Hao Tang, Yan Wang, Hao Zhao, Mengzhu Wang, Jingcai Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-rank Adaptation (LoRA) has demonstrated remarkable capabilities for task
specific fine-tuning. However, in scenarios that involve multiple tasks,
training a separate LoRA model for each one results in considerable
inefficiency in terms of storage and inference. Moreover, existing parameter
generation methods fail to capture the correlations among these tasks, making
multi-task LoRA parameter generation challenging. To address these limitations,
we propose In-Context Meta LoRA (ICM-LoRA), a novel approach that efficiently
achieves task-specific customization of large language models (LLMs).
Specifically, we use training data from all tasks to train a tailored
generator, Conditional Variational Autoencoder (CVAE). CVAE takes task
descriptions as inputs and produces task-aware LoRA weights as outputs. These
LoRA weights are then merged with LLMs to create task-specialized models
without the need for additional fine-tuning. Furthermore, we utilize in-context
meta-learning for knowledge enhancement and task mapping, to capture the
relationship between tasks and parameter distributions. As a result, our method
achieves more accurate LoRA parameter generation for diverse tasks using CVAE.
ICM-LoRA enables more accurate LoRA parameter reconstruction than current
parameter reconstruction methods and is useful for implementing task-specific
enhancements of LoRA parameters. At the same time, our method occupies 283MB,
only 1\% storage compared with the original LoRA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Temporal Preference Optimization for Long-Form Video Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13919v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13919v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Li, Xiaohan Wang, Yuhui Zhang, Zeyu Wang, Serena Yeung-Levy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite significant advancements in video large multimodal models
(video-LMMs), achieving effective temporal grounding in long-form videos
remains a challenge for existing models. To address this limitation, we propose
Temporal Preference Optimization (TPO), a novel post-training framework
designed to enhance the temporal grounding capabilities of video-LMMs through
preference learning. TPO adopts a self-training approach that enables models to
differentiate between well-grounded and less accurate temporal responses by
leveraging curated preference datasets at two granularities: localized temporal
grounding, which focuses on specific video segments, and comprehensive temporal
grounding, which captures extended temporal dependencies across entire video
sequences. By optimizing on these preference datasets, TPO significantly
enhances temporal understanding while reducing reliance on manually annotated
data. Extensive experiments on three long-form video understanding
benchmarks--LongVideoBench, MLVU, and Video-MME--demonstrate the effectiveness
of TPO across two state-of-the-art video-LMMs. Notably, LLaVA-Video-TPO
establishes itself as the leading 7B model on the Video-MME benchmark,
underscoring the potential of TPO as a scalable and efficient solution for
advancing temporal reasoning in long-form video understanding. Project page:
https://ruili33.github.io/tpo_website.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLaRA: Supercharging Robot Learning Data for Vision-Language Policy <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20095v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20095v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Li, Cristina Mata, Jongwoo Park, Kumara Kahatapitiya, Yoo Sung Jang, Jinghuan Shang, Kanchana Ranasinghe, Ryan Burgert, Mu Cai, Yong Jae Lee, Michael S. Ryoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Language Models (VLMs) have recently been leveraged to generate
robotic actions, forming Vision-Language-Action (VLA) models. However, directly
adapting a pretrained VLM for robotic control remains challenging, particularly
when constrained by a limited number of robot demonstrations. In this work, we
introduce LLaRA: Large Language and Robotics Assistant, a framework that
formulates robot action policy as visuo-textual conversations and enables an
efficient transfer of a pretrained VLM into a powerful VLA, motivated by the
success of visual instruction tuning in Computer Vision. First, we present an
automated pipeline to generate conversation-style instruction tuning data for
robots from existing behavior cloning datasets, aligning robotic actions with
image pixel coordinates. Further, we enhance this dataset in a self-supervised
manner by defining six auxiliary tasks, without requiring any additional action
annotations. We show that a VLM finetuned with a limited amount of such
datasets can produce meaningful action decisions for robotic control. Through
experiments across multiple simulated and real-world tasks, we demonstrate that
LLaRA achieves state-of-the-art performance while preserving the generalization
capabilities of large language models. The code, datasets, and pretrained
models are available at https://github.com/LostXine/LLaRA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bayesian Neural Networks for One-to-Many Mapping in Image Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14265v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14265v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoxi Huang, Nantheera Anantrasirichai, Fei Ye, Zipeng Qi, RuiRui Lin, Qirui Yang, David Bull
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In image enhancement tasks, such as low-light and underwater image
enhancement, a degraded image can correspond to multiple plausible target
images due to dynamic photography conditions, such as variations in
illumination. This naturally results in a one-to-many mapping challenge. To
address this, we propose a Bayesian Enhancement Model (BEM) that incorporates
Bayesian Neural Networks (BNNs) to capture data uncertainty and produce diverse
outputs. To achieve real-time inference, we introduce a two-stage approach:
Stage I employs a BNN to model the one-to-many mappings in the low-dimensional
space, while Stage II refines fine-grained image details using a Deterministic
Neural Network (DNN). To accelerate BNN training and convergence, we introduce
a dynamic Momentum Prior. Extensive experiments on multiple low-light and
underwater image enhancement benchmarks demonstrate the superiority of our
method over deterministic models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Return of the Encoder: Maximizing Parameter Efficiency for SLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16273v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16273v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Elfeki, Rui Liu, Chad Voegele
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The dominance of large decoder-only language models has overshadowed
encoder-decoder architectures, despite their fundamental efficiency advantages
in sequence processing. For small language models (SLMs) - those with 1 billion
parameters or fewer - our systematic analysis across GPU, CPU, and NPU
platforms reveals that encoder-decoder architectures achieve 47% lower
first-token latency and 4.7x higher throughput compared to decoder-only models
on edge devices. These gains may be attributed to encoder-decoder's one-time
input processing and efficient separation of understanding and generation
phases.
  We introduce a novel knowledge distillation framework that enables
encoder-decoder models to leverage capabilities from large scalable
decoder-only teachers while preserving their architectural advantages,
achieving up to 6 average performance points improvement across diverse tasks,
with significant gains in asymmetric sequence tasks where input and output
distributions can benefit from different processing approaches.
  When combined with modern advances like Rotary Positional Embeddings (RoPE)
and Vision encoders, our systematic investigation demonstrates that
encoder-decoder architectures provide a more practical path toward deploying
capable language models in resource-constrained environments. Our findings
challenge the prevailing trend toward decoder-only scaling, showing that
architectural choices become increasingly crucial as parameter budgets
decrease, particularly for on-device and edge deployments where computational
efficiency is paramount.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 5 figures. LLMs/SLMs, encoder-decoder and decoder-only</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Zero-Shot Medical Phrase Grounding with Off-the-shelf Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.12920v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.12920v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Konstantinos Vilouras, Pedro Sanchez, Alison Q. O'Neil, Sotirios A. Tsaftaris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Localizing the exact pathological regions in a given medical scan is an
important imaging problem that traditionally requires a large amount of
bounding box ground truth annotations to be accurately solved. However, there
exist alternative, potentially weaker, forms of supervision, such as
accompanying free-text reports, which are readily available. The task of
performing localization with textual guidance is commonly referred to as phrase
grounding. In this work, we use a publicly available Foundation Model, namely
the Latent Diffusion Model, to perform this challenging task. This choice is
supported by the fact that the Latent Diffusion Model, despite being generative
in nature, contains cross-attention mechanisms that implicitly align visual and
textual features, thus leading to intermediate representations that are
suitable for the task at hand. In addition, we aim to perform this task in a
zero-shot manner, i.e., without any training on the target task, meaning that
the model's weights remain frozen. To this end, we devise strategies to select
features and also refine them via post-processing without extra learnable
parameters. We compare our proposed method with state-of-the-art approaches
which explicitly enforce image-text alignment in a joint embedding space via
contrastive learning. Results on a popular chest X-ray benchmark indicate that
our method is competitive with SOTA on different types of pathology, and even
outperforms them on average in terms of two metrics (mean IoU and AUC-ROC).
Source code will be released upon acceptance at https://github.com/vios-s.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 3 figures, IEEE J-BHI Special Issue on Foundation Models in
  Medical Imaging</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Swin <span class="highlight-title">transformer</span>s are robust to distribution and concept drift in
  endoscopy-based longitudinal rectal cancer assessment <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.03762v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.03762v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jorge Tapias Gomez, Aneesh Rangnekar, Hannah Williams, Hannah Thompson, Julio Garcia-Aguilar, Joshua Jesse Smith, Harini Veeraraghavan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Endoscopic images are used at various stages of rectal cancer treatment
starting from cancer screening, diagnosis, during treatment to assess response
and toxicity from treatments such as colitis, and at follow up to detect new
tumor or local regrowth (LR). However, subjective assessment is highly variable
and can underestimate the degree of response in some patients, subjecting them
to unnecessary surgery, or overestimate response that places patients at risk
of disease spread. Advances in deep learning has shown the ability to produce
consistent and objective response assessment for endoscopic images. However,
methods for detecting cancers, regrowth, and monitoring response during the
entire course of patient treatment and follow-up are lacking. This is because,
automated diagnosis and rectal cancer response assessment requires methods that
are robust to inherent imaging illumination variations and confounding
conditions (blood, scope, blurring) present in endoscopy images as well as
changes to the normal lumen and tumor during treatment. Hence, a hierarchical
shifted window (Swin) transformer was trained to distinguish rectal cancer from
normal lumen using endoscopy images. Swin as well as two convolutional
(ResNet-50, WideResNet-50), and vision transformer (ViT) models were trained
and evaluated on follow-up longitudinal images to detect LR on private dataset
as well as on out-of-distribution (OOD) public colonoscopy datasets to detect
pre/non-cancerous polyps. Color shifts were applied using optimal transport to
simulate distribution shifts. Swin and ResNet models were similarly accurate in
the in-distribution dataset. Swin was more accurate than other methods
(follow-up: 0.84, OOD: 0.83) even when subject to color shifts (follow-up:
0.83, OOD: 0.87), indicating capability to provide robust performance for
longitudinal cancer assessment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at SPIE Medical Imaging 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quantifying uncertainty in lung cancer segmentation with foundation
  models applied to mixed-domain <span class="highlight-title">dataset</span>s <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13113v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13113v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aneesh Rangnekar, Nishant Nadkarni, Jue Jiang, Harini Veeraraghavan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical image foundation models have shown the ability to segment organs and
tumors with minimal fine-tuning. These models are typically evaluated on
task-specific in-distribution (ID) datasets. However, reliable performance on
ID datasets does not guarantee robust generalization on out-of-distribution
(OOD) datasets. Importantly, once deployed for clinical use, it is impractical
to have `ground truth' delineations to assess ongoing performance drifts,
especially when images fall into the OOD category due to different imaging
protocols. Hence, we introduced a comprehensive set of computationally fast
metrics to evaluate the performance of multiple foundation models (Swin UNETR,
SimMIM, iBOT, SMIT) trained with self-supervised learning (SSL). All models
were fine-tuned on identical datasets for lung tumor segmentation from computed
tomography (CT) scans. The evaluation was performed on two public lung cancer
datasets (LRAD: n = 140, 5Rater: n = 21) with different image acquisitions and
tumor stages compared to training data (n = 317 public resource with stage
III-IV lung cancers) and a public non-cancer dataset containing volumetric CT
scans of patients with pulmonary embolism (n = 120). All models produced
similarly accurate tumor segmentation on the lung cancer testing datasets. SMIT
produced the highest F1-score (LRAD: 0.60, 5Rater: 0.64) and lowest entropy
(LRAD: 0.06, 5Rater: 0.12), indicating higher tumor detection rate and
confident segmentations. In the OOD dataset, SMIT misdetected the least number
of tumors, marked by a median volume occupancy of 5.67 cc compared to the best
method SimMIM of 9.97 cc. Our analysis shows that additional metrics such as
entropy and volume occupancy may help better understand model performance on
mixed domain datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at SPIE Medical Imaging 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Advancing the Understanding and Evaluation of AR-Generated Scenes: When
  Vision-Language Models Shine and Stumble 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13964v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13964v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lin Duan, Yanming Xiu, Maria Gorlatova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Augmented Reality (AR) enhances the real world by integrating virtual
content, yet ensuring the quality, usability, and safety of AR experiences
presents significant challenges. Could Vision-Language Models (VLMs) offer a
solution for the automated evaluation of AR-generated scenes? Could
Vision-Language Models (VLMs) offer a solution for the automated evaluation of
AR-generated scenes? In this study, we evaluate the capabilities of three
state-of-the-art commercial VLMs -- GPT, Gemini, and Claude -- in identifying
and describing AR scenes. For this purpose, we use DiverseAR, the first AR
dataset specifically designed to assess VLMs' ability to analyze virtual
content across a wide range of AR scene complexities. Our findings demonstrate
that VLMs are generally capable of perceiving and describing AR scenes,
achieving a True Positive Rate (TPR) of up to 93% for perception and 71% for
description. While they excel at identifying obvious virtual objects, such as a
glowing apple, they struggle when faced with seamlessly integrated content,
such as a virtual pot with realistic shadows. Our results highlight both the
strengths and the limitations of VLMs in understanding AR scenarios. We
identify key factors affecting VLM performance, including virtual content
placement, rendering quality, and physical plausibility. This study underscores
the potential of VLMs as tools for evaluating the quality of AR experiences.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DreamArtist++: Controllable One-Shot Text-to-Image Generation via
  Positive-Negative Adapter 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.11337v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.11337v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyi Dong, Pengxu Wei, Liang Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-arts text-to-image generation models such as Imagen and Stable
Diffusion Model have succeed remarkable progresses in synthesizing
high-quality, feature-rich images with high resolution guided by human text
prompts. Since certain characteristics of image content \emph{e.g.}, very
specific object entities or styles, are very hard to be accurately described by
text, some example-based image generation approaches have been proposed,
\emph{i.e.} generating new concepts based on absorbing the salient features of
a few input references. Despite of acknowledged successes, these methods have
struggled on accurately capturing the reference examples' characteristics while
keeping diverse and high-quality image generation, particularly in the one-shot
scenario (\emph{i.e.} given only one reference). To tackle this problem, we
propose a simple yet effective framework, namely DreamArtist, which adopts a
novel positive-negative prompt-tuning learning strategy on the pre-trained
diffusion model, and it has shown to well handle the trade-off between the
accurate controllability and fidelity of image generation with only one
reference example. Specifically, our proposed framework incorporates both
positive and negative embeddings or adapters and optimizes them in a joint
manner. The positive part aggressively captures the salient characteristics of
the reference image to drive diversified generation and the negative part
rectifies inadequacies from the positive part. We have conducted extensive
experiments and evaluated the proposed method from image similarity (fidelity)
and diversity, generation controllability, and style cloning. And our
DreamArtist has achieved a superior generation performance over existing
methods. Besides, our additional evaluation on extended tasks, including
concept compositions and prompt-guided image editing, demonstrates its
effectiveness for more applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PV-S3: Advancing Automatic Photovoltaic Defect Detection using
  Semi-Supervised Semantic Segmentation of Electroluminescence Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.13693v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.13693v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhishek Jha, Yogesh Rawat, Shruti Vyas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Photovoltaic (PV) systems allow us to tap into all abundant solar energy,
however they require regular maintenance for high efficiency and to prevent
degradation. Traditional manual health check, using Electroluminescence (EL)
imaging, is expensive and logistically challenging which makes automated defect
detection essential. Current automation approaches require extensive manual
expert labeling, which is time-consuming, expensive, and prone to errors. We
propose PV-S3 (Photovoltaic-Semi Supervised Segmentation), a Semi-Supervised
Learning approach for semantic segmentation of defects in EL images that
reduces reliance on extensive labeling. PV-S3 is a Deep learning model trained
using a few labeled images along with numerous unlabeled images. We introduce a
novel Semi Cross-Entropy loss function to deal with class imbalance. We
evaluate PV-S3 on multiple datasets and demonstrate its effectiveness and
adaptability. With merely 20% labeled samples, we achieve an absolute
improvement of 9.7% in IoU, 13.5% in Precision, 29.15% in Recall, and 20.42% in
F1-Score over prior state-of-the-art supervised method (which uses 100% labeled
samples) on UCF-EL dataset (largest dataset available for semantic segmentation
of EL images) showing improvement in performance while reducing the annotation
costs by 80%. For more details, visit our GitHub
repository:https://github.com/abj247/PV-S3.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ©Plug-in Authorization for Human Content Copyright Protection
  in Text-to-Image Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.11962v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.11962v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Zhou, Huishuai Zhang, Jiang Bian, Weiming Zhang, Nenghai Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the contentious issue of copyright infringement in
images generated by text-to-image models, sparking debates among AI developers,
content creators, and legal entities. State-of-the-art models create
high-quality content without crediting original creators, causing concern in
the artistic community. To mitigate this, we propose the \copyright Plug-in
Authorization framework, introducing three operations: addition, extraction,
and combination. Addition involves training a \copyright plug-in for specific
copyright, facilitating proper credit attribution. Extraction allows creators
to reclaim copyright from infringing models, and combination enables users to
merge different \copyright plug-ins. These operations act as permits,
incentivizing fair use and providing flexibility in authorization. We present
innovative approaches,"Reverse LoRA" for extraction and "EasyMerge" for
seamless combination. Experiments in artist-style replication and cartoon IP
recreation demonstrate \copyright plug-ins' effectiveness, offering a valuable
solution for human copyright protection in the age of generative AIs. The code
is available at https://github.com/zc1023/-Plug-in-Authorization.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dual Thinking and Logical Processing -- Are Multi-modal Large Language
  Models Closing the Gap with Human Vision ? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.06967v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.06967v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kailas Dayanandan, Nikhil Kumar, Anand Sinha, Brejesh Lall
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The dual thinking framework considers fast, intuitive processing and slower,
logical processing. The perception of dual thinking in vision requires images
where inferences from intuitive and logical processing differ. We introduce an
adversarial dataset to provide evidence for the dual thinking framework in
human vision, which also aids in studying the qualitative behavior of deep
learning models. The evidence underscores the importance of shape in
identifying instances in human vision. Our psychophysical studies show the
presence of multiple inferences in rapid succession, and analysis of errors
shows the early stopping of visual processing can result in missing relevant
information. Our study shows that segmentation models lack an understanding of
sub-structures, as indicated by errors related to the position and number of
sub-components. Additionally, the similarity in errors made by models and
intuitive human processing indicates that models only address intuitive
thinking in human vision. In contrast, multi-modal LLMs, including open-source
models, demonstrate tremendous progress on errors made in intuitive processing.
The models have improved performance on images that require logical reasoning
and show recognition of sub-components. However, they have not matched the
performance improvements made on errors in intuitive processing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Batch Artifact Scanning Protocol: A new method using computed
  tomography (CT) to rapidly create three-dimensional models of objects from
  large collections en masse 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.02691v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.02691v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Katrina Yezzi-Woodley, Jeff Calder, Mckenzie Sweno, Chloe Siewert, Peter J. Olver
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Within anthropology, the use of three-dimensional (3D) imaging has become
increasingly common and widespread since it broadens the available avenues for
addressing a wide range of key anthropological issues. The ease with which 3D
models can be generated and shared has major impact on research, cultural
heritage, education, science communication, and public engagement, as well as
contributing to the preservation of the physical specimens and archiving
collections in widely accessible data bases. Current scanning protocols have
the ability to create the required research quality 3D models; however, they
tend to be time and labor intensive and not practical when working with large
collections. Here we describe a streamlined Batch Artifact Scanning Protocol to
rapidly create 3D models using a medical CT scanner. While this method can be
used on a variety of material types, we have, for specificity, applied our
protocol to a large collection of experimentally broken ungulate limb bones. By
employing the Batch Artifact Scanning Protocol, we were able to efficiently
create 3D models of 2,474 bone fragments at a rate of less than 4 minutes per
specimen.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CHaRNet: Conditioned Heatmap Regression for Robust Dental Landmark
  Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13073v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13073v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        José Rodríguez-Ortega, Francisco Pérez-Hernández, Siham Tabik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Identifying anatomical landmarks in 3D dental models is vital for orthodontic
treatment, yet manual placement is complex and time-consuming. Although some
machine learning approaches have been proposed for automatic tooth landmark
detection in 3D Intraoral Scans (IOS), none provide a fully end-to-end solution
that bypasses teeth segmentation, limiting practical applicability. We
introduce CHaRNet (Conditioned Heatmap Regression Network), the first fully
end-to-end deep learning framework for tooth landmark detection in 3D IOS.
Unlike traditional two-stage workflows that segment teeth before detecting
landmarks, CHaRNet directly operates on the input point cloud, thus reducing
complexity and computational overhead. Our method integrates four modules: (1)
a point cloud encoder, (2) a point cloud decoder with a heatmap regression
head, (3) a teeth presence classification head, and (4) the novel Conditioned
Heatmap Regression (CHaR) module. By leveraging teeth presence classification,
the CHaR module dynamically adapts to missing teeth and enhances detection
accuracy in complex dental models. We evaluate CHaRNet using five point cloud
learning algorithms on a clinical dataset of 1,214 annotated 3D models. Both
the dataset and code will be publicly released to address the lack of open
datasets in orthodontics and inspire further research. CHaRNet achieves a Mean
Euclidean Distance Error (MEDE) of 0.51 mm on typical dental models and 1.28 mm
across all dentition types, with corresponding Mean Success Rates (MSR) of
87.06% and 82.40%, respectively. Notably, it exhibits robust performance on
irregular geometries, including models with missing teeth. This end-to-end
approach streamlines orthodontic workflows, enhances 3D IOS analysis precision,
and supports efficient computer-assisted treatment planning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neuromorphic spatiotemporal optical flow: Enabling ultrafast visual
  perception beyond human capabilities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15345v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15345v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengbo Wang, Jingwen Zhao, Tongming Pu, Liangbing Zhao, Xiaoyu Guo, Yue Cheng, Cong Li, Weihao Ma, Chenyu Tang, Zhenyu Xu, Ningli Wang, Luigi Occhipinti, Arokia Nathan, Ravinder Dahiya, Huaqiang Wu, Li Tao, Shuo Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optical flow, inspired by the mechanisms of biological visual systems,
calculates spatial motion vectors within visual scenes that are necessary for
enabling robotics to excel in complex and dynamic working environments.
However, current optical flow algorithms, despite human-competitive task
performance on benchmark datasets, remain constrained by unacceptable time
delays (~0.6 seconds per inference, 4X human processing speed) in practical
deployment. Here, we introduce a neuromorphic optical flow approach that
addresses delay bottlenecks by encoding temporal information directly in a
synaptic transistor array to assist spatial motion analysis. Compared to
conventional spatial-only optical flow methods, our spatiotemporal neuromorphic
optical flow offers the spatial-temporal consistency of motion information,
rapidly identifying regions of interest in as little as 1-2 ms using the
temporal motion cues derived from the embedded temporal information in the
two-dimensional floating gate synaptic transistors. Thus, the visual input can
be selectively filtered to achieve faster velocity calculations and various
task execution. At the hardware level, due to the atomically sharp interfaces
between distinct functional layers in two-dimensional van der Waals
heterostructures, the synaptic transistor offers high-frequency response (~100
{\mu}s), robust non-volatility (>10000 s), and excellent endurance (>8000
cycles), enabling robust visual processing. In software benchmarks, our system
outperforms state-of-the-art algorithms with a 400% speedup, frequently
surpassing human-level performance while maintaining or enhancing accuracy by
utilizing the temporal priors provided by the embedded temporal information.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Learning With Sine-Activated Low-rank Matrices <span class="chip">ICLR
  2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.19243v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.19243v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiping Ji, Hemanth Saratchandran, Cameron Gordon, Zeyu Zhang, Simon Lucey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-rank decomposition has emerged as a vital tool for enhancing parameter
efficiency in neural network architectures, gaining traction across diverse
applications in machine learning. These techniques significantly lower the
number of parameters, striking a balance between compactness and performance.
However, a common challenge has been the compromise between parameter
efficiency and the accuracy of the model, where reduced parameters often lead
to diminished accuracy compared to their full-rank counterparts. In this work,
we propose a novel theoretical framework that integrates a sinusoidal function
within the low-rank decomposition process. This approach not only preserves the
benefits of the parameter efficiency characteristic of low-rank methods but
also increases the decomposition's rank, thereby enhancing model performance.
Our method proves to be a plug in enhancement for existing low-rank models, as
evidenced by its successful application in Vision Transformers (ViT), Large
Language Models (LLMs), Neural Radiance Fields (NeRF) and 3D shape modelling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors contributed equally. Paper accepted at ICLR
  2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AniDoc: Animation Creation Made Easier 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14173v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14173v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihao Meng, Hao Ouyang, Hanlin Wang, Qiuyu Wang, Wen Wang, Ka Leong Cheng, Zhiheng Liu, Yujun Shen, Huamin Qu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The production of 2D animation follows an industry-standard workflow,
encompassing four essential stages: character design, keyframe animation,
in-betweening, and coloring. Our research focuses on reducing the labor costs
in the above process by harnessing the potential of increasingly powerful
generative AI. Using video diffusion models as the foundation, AniDoc emerges
as a video line art colorization tool, which automatically converts sketch
sequences into colored animations following the reference character
specification. Our model exploits correspondence matching as an explicit
guidance, yielding strong robustness to the variations (e.g., posture) between
the reference character and each line art frame. In addition, our model could
even automate the in-betweening process, such that users can easily create a
temporally consistent animation by simply providing a character image as well
as the start and end sketches. Our code is available at:
https://yihao-meng.github.io/AniDoc_demo.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page and code: https://yihao-meng.github.io/AniDoc_demo</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can Location Embeddings Enhance Super-Resolution of Satellite Imagery? <span class="chip">WACV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15847v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15847v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Panangian, Ksenia Bittner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Publicly available satellite imagery, such as Sentinel- 2, often lacks the
spatial resolution required for accurate analysis of remote sensing tasks
including urban planning and disaster response. Current super-resolution
techniques are typically trained on limited datasets, leading to poor
generalization across diverse geographic regions. In this work, we propose a
novel super-resolution framework that enhances generalization by incorporating
geographic context through location embeddings. Our framework employs
Generative Adversarial Networks (GANs) and incorporates techniques from
diffusion models to enhance image quality. Furthermore, we address tiling
artifacts by integrating information from neighboring images, enabling the
generation of seamless, high-resolution outputs. We demonstrate the
effectiveness of our method on the building segmentation task, showing
significant improvements over state-of-the-art methods and highlighting its
potential for real-world applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE/CVF Winter Conference on Applications of Computer
  Vision (WACV)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to utilize image second-order derivative information for crisp
  edge detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.05779v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.05779v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changsong Liu, Yimeng Fan, Mingyang Li, Wei Zhang, Yanyan Liu, Yuming Li, Wenlin Li, Liang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Edge detection is a fundamental task in computer vision. It has made great
progress under the development of deep convolutional neural networks (DCNNs),
some of which have achieved a beyond human-level performance. However, recent
top-performing edge detection methods tend to generate thick and noisy edge
lines. In this work, we solve this problem from two aspects: (1) the lack of
prior knowledge regarding image edges, and (2) the issue of imbalanced pixel
distribution. We propose a second-order derivative-based multi-scale contextual
enhancement module (SDMCM) to help the model locate true edge pixels accurately
by introducing the edge prior knowledge. We also construct a hybrid focal loss
function (HFL) to alleviate the imbalanced distribution issue. In addition, we
employ the conditionally parameterized convolution (CondConv) to develop a
novel boundary refinement module (BRM), which can further refine the final
output edge maps. In the end, we propose a U-shape network named LUS-Net which
is based on the SDMCM and BRM for crisp edge detection. We perform extensive
experiments on three standard benchmarks, and the experiment results illustrate
that our method can predict crisp and clean edge maps and achieves
state-of-the-art performance on the BSDS500 dataset (ODS=0.829), NYUD-V2
dataset (ODS=0.768), and BIPED dataset (ODS=0.903).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Boosting Weak Positives for Text Based Person Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17586v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17586v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akshay Modi, Ashhar Aziz, Nilanjana Chatterjee, A V Subramanyam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large vision-language models have revolutionized cross-modal object
retrieval, but text-based person search (TBPS) remains a challenging task due
to limited data and fine-grained nature of the task. Existing methods primarily
focus on aligning image-text pairs into a common representation space, often
disregarding the fact that real world positive image-text pairs share a varied
degree of similarity in between them. This leads models to prioritize easy
pairs, and in some recent approaches, challenging samples are discarded as
noise during training. In this work, we introduce a boosting technique that
dynamically identifies and emphasizes these challenging samples during
training. Our approach is motivated from classical boosting technique and
dynamically updates the weights of the weak positives, wherein, the rank-1
match does not share the identity of the query. The weight allows these
misranked pairs to contribute more towards the loss and the network has to pay
more attention towards such samples. Our method achieves improved performance
across four pedestrian datasets, demonstrating the effectiveness of our
proposed module.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Swin fMRI <span class="highlight-title">Transformer</span> Predicts Early Neurodevelopmental Outcomes from
  Neonatal fMRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07783v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07783v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patrick Styll, Dowon Kim, Jiook Cha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Brain development in the first few months of human life is a critical phase
characterized by rapid structural growth and functional organization.
Accurately predicting developmental outcomes during this time is crucial for
identifying delays and enabling timely interventions. This study introduces the
SwiFT (Swin 4D fMRI Transformer) model, designed to predict Bayley-III
composite scores using neonatal fMRI from the Developing Human Connectome
Project (dHCP). To enhance predictive accuracy, we apply dimensionality
reduction via group independent component analysis (ICA) and pretrain SwiFT on
large adult fMRI datasets to address the challenges of limited neonatal data.
Our analysis shows that SwiFT significantly outperforms baseline models in
predicting cognitive, motor, and language outcomes, leveraging both
single-label and multi-label prediction strategies. The model's attention-based
architecture processes spatiotemporal data end-to-end, delivering superior
predictive performance. Additionally, we use Integrated Gradients with
Smoothgrad sQuare (IG-SQ) to interpret predictions, identifying neural spatial
representations linked to early cognitive and behavioral development. These
findings underscore the potential of Transformer models to advance
neurodevelopmental research and clinical practice.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>fMRI Transformer, Developing Human Connectome Project, Bayley Scales
  of Infant Development, Personalized Therapy, XAI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Assessing the Capability of YOLO- and <span class="highlight-title">Transformer</span>-based Object Detectors
  for Real-time Weed Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17387v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17387v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alicia Allmendinger, Ahmet Oğuz Saltık, Gerassimos G. Peteinatos, Anthony Stein, Roland Gerhards
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spot spraying represents an efficient and sustainable method for reducing the
amount of pesticides, particularly herbicides, used in agricultural fields. To
achieve this, it is of utmost importance to reliably differentiate between
crops and weeds, and even between individual weed species in situ and under
real-time conditions. To assess suitability for real-time application,
different object detection models that are currently state-of-the-art are
compared. All available models of YOLOv8, YOLOv9, YOLOv10, and RT-DETR are
trained and evaluated with images from a real field situation. The images are
separated into two distinct datasets: In the initial data set, each species of
plants is trained individually; in the subsequent dataset, a distinction is
made between monocotyledonous weeds, dicotyledonous weeds, and three chosen
crops. The results demonstrate that while all models perform equally well in
the metrics evaluated, the YOLOv9 models, particularly the YOLOv9s and YOLOv9e,
stand out in terms of their strong recall scores (66.58 % and 72.36 %), as well
as mAP50 (73.52 % and 79.86 %), and mAP50-95 (43.82 % and 47.00 %) in dataset
2. However, the RT-DETR models, especially RT-DETR-l, excel in precision with
reaching 82.44 \% on dataset 1 and 81.46 % in dataset 2, making them
particularly suitable for scenarios where minimizing false positives is
critical. In particular, the smallest variants of the YOLO models (YOLOv8n,
YOLOv9t, and YOLOv10n) achieve substantially faster inference times down to
7.58 ms for dataset 2 on the NVIDIA GeForce RTX 4090 GPU for analyzing one
frame, while maintaining competitive accuracy, highlighting their potential for
deployment in resource-constrained embedded computing devices as typically used
in productive setups.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring AI-based System Design for Pixel-level Protected Health
  Information Detection in Medical Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09552v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09552v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tuan Truong, Ivo M. Baltruschat, Mark Klemens, Grit Werner, Matthias Lenga
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Purpose: This study aims to evaluate different setups of an AI-based solution
to detect Protected Health Information (PHI) in medical images.
  Materials and Methods: Text from eight PHI and eight non-PHI categories are
simulated and incorporated into a curated dataset comprising 1,000 medical
images across four modalities: CT, X-ray, bone scan, and MRI. The proposed PHI
detection pipeline comprises three key components: text localization,
extraction, and analysis. Three vision and language models, YOLOv11, EasyOCR,
and GPT-4o, are benchmarked in different setups corresponding to three key
components. The performance is evaluated with classification metrics, including
precision, recall, F1 score, and accuracy.
  Results: All four setups demonstrate strong performance in detecting PHI
imprints, with all metrics exceeding 0.9. The setup that utilizes YOLOv11 for
text localization and GPT-4o for text extraction and analysis achieves the
highest performance in PHI detection. However, this setup incurs the highest
cost due to the increased number of generated tokens associated with GPT-4o
model. Conversely, the setup using solely GPT-4o for the end-to-end pipeline
exhibits the lowest performance but showcases the feasibility of multi-modal
models in solving complex tasks.
  Conclusion: For optimal text localization and extraction, it is recommended
to fine-tune an object detection model and utilize built-in Optical Character
Recognition (OCR) software. Large language models like GPT-4o can be
effectively leveraged to reason about and semantically analyze the PHI content.
Although the vision capability of GPT-4o is promising for reading image crops,
it remains limited for end-to-end pipeline applications with whole images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond the Veil of Similarity: Quantifying Semantic Continuity in
  Explainable AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12950v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12950v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Huang, Emanuele Mezzi, Osman Mutlu, Miltiadis Kofinas, Vidya Prasad, Shadnan Azwad Khan, Elena Ranguelova, Niki van Stein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel metric for measuring semantic continuity in Explainable
AI methods and machine learning models. We posit that for models to be truly
interpretable and trustworthy, similar inputs should yield similar
explanations, reflecting a consistent semantic understanding. By leveraging XAI
techniques, we assess semantic continuity in the task of image recognition. We
conduct experiments to observe how incremental changes in input affect the
explanations provided by different XAI methods. Through this approach, we aim
to evaluate the models' capability to generalize and abstract semantic concepts
accurately and to evaluate different XAI methods in correctly capturing the
model behaviour. This paper contributes to the broader discourse on AI
interpretability by proposing a quantitative measure for semantic continuity
for XAI methods, offering insights into the models' and explainers' internal
reasoning processes, and promoting more reliable and transparent AI systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, accepted at the world conference of explainable AI, 2024,
  Malta</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LMFusion: Adapting <span class="highlight-title">Pretrain</span>ed Language Models for Multimodal Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15188v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15188v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weijia Shi, Xiaochuang Han, Chunting Zhou, Weixin Liang, Xi Victoria Lin, Luke Zettlemoyer, Lili Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present LMFusion, a framework for empowering pretrained text-only large
language models (LLMs) with multimodal generative capabilities, enabling them
to understand and generate both text and images in arbitrary sequences.
LMFusion leverages existing Llama-3's weights for processing texts
autoregressively while introducing additional and parallel transformer modules
for processing images with diffusion. During training, the data from each
modality is routed to its dedicated modules: modality-specific feedforward
layers, query-key-value projections, and normalization layers process each
modality independently, while the shared self-attention layers allow
interactions across text and image features. By freezing the text-specific
modules and only training the image-specific modules, LMFusion preserves the
language capabilities of text-only LLMs while developing strong visual
understanding and generation abilities. Compared to methods that pretrain
multimodal generative models from scratch, our experiments demonstrate that,
LMFusion improves image understanding by 20% and image generation by 3.6% using
only 50% of the FLOPs while maintaining Llama-3's language capabilities. We
also demonstrate that this framework can adapt existing vision-language models
with multimodal generation ability. Overall, this framework not only leverages
existing computational investments in text-only LLMs but also enables the
parallel development of language and vision capabilities, presenting a
promising direction for efficient multimodal model development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Name change: LlamaFusion to LMFusion</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SpectralKD: A Unified Framework for Interpreting and Distilling Vision
  <span class="highlight-title">Transformer</span>s via Spectral Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19055v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19055v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huiyuan Tian, Bonan Xu, Shijian Li, Gang Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge Distillation (KD) has achieved widespread success in compressing
large Vision Transformers (ViTs), but a unified theoretical framework for both
ViTs and KD is still lacking. In this paper, we propose SpectralKD, a novel
unified analytical framework that offers deeper insights into ViTs and
optimizes KD via spectral analysis. Our model-wise analysis reveals that CaiT
concentrates information in their first and last few layers, informing optimal
layer selection for KD. Surprisingly, our layer-wise analysis discovers that
Swin Transformer and CaiT exhibit similar spectral encoding patterns despite
their architectural differences, leading to feature map alignment guideline.
Building on these insights, we propose a simple yet effective spectral
alignment method for KD. Benefiting from the deeper understanding by above
analysis results, even such a simple strategy achieves state-of-the-art
performance on ImageNet-1K without introducing any trainable parameters,
improving DeiT-Tiny by $+5.2\%$ and Swin-Tiny by $+1.4\%$ in top-1 accuracy.
Furthermore, our post-training analysis reveals that distilled students can
reproduce spectral patterns similar to their teachers, opening a new area we
term ``distillation dynamics". Code and experimental logs are available in
https://github.com/thy960112/SpectralKD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Interactive 3D Multi-Object Removal 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17636v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17636v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingcheng Ni, Weiguang Zhao, Daniel Wang, Ziyao Zeng, Chenyu You, Alex Wong, Kaizhu Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object removal is of great significance to 3D scene understanding, essential
for applications in content filtering and scene editing. Current mainstream
methods primarily focus on removing individual objects, with a few methods
dedicated to eliminating an entire area or all objects of a certain category.
They however confront the challenge of insufficient granularity and flexibility
for real-world applications, where users demand tailored excision and
preservation of objects within defined zones. In addition, most of the current
methods require kinds of priors when addressing multi-view inpainting, which is
time-consuming. To address these limitations, we propose an efficient and
user-friendly pipeline for 3D multi-object removal, enabling users to flexibly
select areas and define objects for removal or preservation. Concretely, to
ensure object consistency and correspondence across multiple views, we propose
a novel mask matching and refinement module, which integrates homography-based
warping with high-confidence anchor points for segmentation. By leveraging the
IoU joint shape context distance loss, we enhance the accuracy of warped masks
and improve subsequent inpainting processes. Considering the current immaturity
of 3D multi-object removal, we provide a new evaluation dataset to bridge the
developmental void. Experimental results demonstrate that our method
significantly reduces computational costs, achieving processing speeds more
than 80% faster than state-of-the-art methods while maintaining equivalent or
higher reconstruction quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ QADM-Net: Multi-Level Quality-Adaptive Dynamic Network for Reliable
  Multimodal Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14489v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14489v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shu Shen, Tong Zhang, C. L. Philip Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal machine learning has achieved remarkable progress in many
scenarios, but its reliability is undermined by varying sample quality. In this
paper, we find that current multimodal classification methods lack dynamic
networks for sample-specific depth and parameters to achieve reliable
inference. To this end, a novel framework for multimodal reliable
classification termed Multi-Level Quality-Adaptive Dynamic Multimodal Network
(QADM-Net) is proposed. QADM-Net first adopts a novel approach based on
noise-free prototypes and a classifier-free design to reliably estimate the
quality of each sample at both modality and feature levels. It then achieves
sample-specific network depth via the \textbf{\textit{Global Confidence
Normalized Depth (GCND)}} mechanism. By normalizing depth across modalities and
samples, \textit{\textbf{GCND}} effectively mitigates the impact of challenging
modality inputs on dynamic depth reliability. Furthermore, QADM-Net provides
sample-adaptive network parameters via the \textbf{\textit{Layer-wise Greedy
Parameter (LGP)}} mechanism driven by feature-level quality. The cross-modality
layer-wise greedy strategy in \textbf{\textit{LGP}} designs a reliable
parameter prediction paradigm for multimodal networks with variable depths for
the first time. Experiments conducted on four datasets demonstrate that
QADM-Net significantly outperforms state-of-the-art methods in classification
performance and reliability, exhibiting strong adaptability to data with
diverse quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EliGen: Entity-Level Controlled Image Generation with Regional Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.01097v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.01097v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hong Zhang, Zhongjie Duan, Xingjun Wang, Yingda Chen, Yu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in diffusion models have significantly advanced
text-to-image generation, yet global text prompts alone remain insufficient for
achieving fine-grained control over individual entities within an image. To
address this limitation, we present EliGen, a novel framework for Entity-level
controlled image Generation. Firstly, we put forward regional attention, a
mechanism for diffusion transformers that requires no additional parameters,
seamlessly integrating entity prompts and arbitrary-shaped spatial masks. By
contributing a high-quality dataset with fine-grained spatial and semantic
entity-level annotations, we train EliGen to achieve robust and accurate
entity-level manipulation, surpassing existing methods in both spatial
precision and image quality. Additionally, we propose an inpainting fusion
pipeline, extending its capabilities to multi-entity image inpainting tasks. We
further demonstrate its flexibility by integrating it with other open-source
models such as IP-Adapter, In-Context LoRA and MLLM, unlocking new creative
possibilities. The source code, model, and dataset are published at
https://github.com/modelscope/DiffSynth-Studio.git.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ARDuP: Active Region Video Diffusion for Universal Policies <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13301v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13301v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuaiyi Huang, Mara Levy, Zhenyu Jiang, Anima Anandkumar, Yuke Zhu, Linxi Fan, De-An Huang, Abhinav Shrivastava
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential decision-making can be formulated as a text-conditioned video
generation problem, where a video planner, guided by a text-defined goal,
generates future frames visualizing planned actions, from which control actions
are subsequently derived. In this work, we introduce Active Region Video
Diffusion for Universal Policies (ARDuP), a novel framework for video-based
policy learning that emphasizes the generation of active regions, i.e.
potential interaction areas, enhancing the conditional policy's focus on
interactive areas critical for task execution. This innovative framework
integrates active region conditioning with latent diffusion models for video
planning and employs latent representations for direct action decoding during
inverse dynamic modeling. By utilizing motion cues in videos for automatic
active region discovery, our method eliminates the need for manual annotations
of active regions. We validate ARDuP's efficacy via extensive experiments on
simulator CLIPort and the real-world dataset BridgeData v2, achieving notable
improvements in success rates and generating convincingly realistic video
plans.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IROS 2024 (Oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ContourFormer:Real-Time Contour-Based End-to-End Instance Segmentation
  <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17688v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17688v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiwei Yao, Chen Li, Minjun Xiong, Wenbo Dong, Hao Chen, Xiong Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents Contourformer, a real-time contour-based instance
segmentation algorithm. The method is fully based on the DETR paradigm and
achieves end-to-end inference through iterative and progressive mechanisms to
optimize contours. To improve efficiency and accuracy, we develop two novel
techniques: sub-contour decoupling mechanisms and contour fine-grained
distribution refinement. In the sub-contour decoupling mechanism, we propose a
deformable attention-based module that adaptively selects sampling regions
based on the current predicted contour, enabling more effective capturing of
object boundary information. Additionally, we design a multi-stage optimization
process to enhance segmentation precision by progressively refining
sub-contours. The contour fine-grained distribution refinement technique aims
to further improve the ability to express fine details of contours. These
innovations enable Contourformer to achieve stable and precise segmentation for
each instance while maintaining real-time performance. Extensive experiments
demonstrate the superior performance of Contourformer on multiple benchmark
datasets, including SBD, COCO, and KINS. We conduct comprehensive evaluations
and comparisons with existing state-of-the-art methods, showing significant
improvements in both accuracy and inference speed. This work provides a new
solution for contour-based instance segmentation tasks and lays a foundation
for future research, with the potential to become a strong baseline method in
this field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VaLID: Verification as Late Integration of Detections for LiDAR-Camera
  Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15529v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15529v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vanshika Vats, Marzia Binta Nizam, James Davis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vehicle object detection benefits from both LiDAR and camera data, with LiDAR
offering superior performance in many scenarios. Fusion of these modalities
further enhances accuracy, but existing methods often introduce complexity or
dataset-specific dependencies. In our study, we propose a model-adaptive
late-fusion method, VaLID, which validates whether each predicted bounding box
is acceptable or not. Our method verifies the higher-performing, yet overly
optimistic LiDAR model detections using camera detections that are obtained
from either specially trained, general, or open-vocabulary models. VaLID uses a
simple multi-layer perceptron trained with a high recall bias to reduce the
false predictions made by the LiDAR detector, while still preserving the true
ones. Evaluating with multiple combinations of LiDAR and camera detectors on
the KITTI dataset, we reduce false positives by an average of 63.9%, thus
outperforming the individual detectors on 3D average precision (3DAP). Our
approach is model-adaptive and demonstrates state-of-the-art competitive
performance even when using generic camera detectors that were not trained
specifically for this dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quantum Down Sampling Filter for Variational Auto-encoder 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06259v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06259v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Farina Riaz, Fakhar Zaman, Hajime Suzuki, Sharif Abuadbba, David Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Variational Autoencoders (VAEs) are essential tools in generative modeling
and image reconstruction, with their performance heavily influenced by the
encoder-decoder architecture. This study aims to improve the quality of
reconstructed images by enhancing their resolution and preserving finer
details, particularly when working with low-resolution inputs (16x16 pixels),
where traditional VAEs often yield blurred or in-accurate results. To address
this, we propose a hybrid model that combines quantum computing techniques in
the VAE encoder with convolutional neural networks (CNNs) in the decoder. By
upscaling the resolution from 16x16 to 32x32 during the encoding process, our
approach evaluates how the model reconstructs images with enhanced resolution
while maintaining key features and structures. This method tests the model's
robustness in handling image reconstruction and its ability to preserve
essential details despite training on lower-resolution data. We evaluate our
proposed down sampling filter for Quantum VAE (Q-VAE) on the MNIST and USPS
datasets and compare it with classical VAEs and a variant called Classical
Direct Passing VAE (CDP-VAE), which uses windowing pooling filters in the
encoding process. Performance is assessed using metrics such as the Frechet
Inception Distance (FID) and Mean Squared Error (MSE), which measure the
fidelity of reconstructed images. Our results demonstrate that the Q-VAE
consistently outperforms both the Classical VAE and CDP-VAE, achieving
significantly lower FID and MSE scores. Additionally, CDP-VAE yields better
performance than C-VAE. These findings highlight the potential of
quantum-enhanced VAEs to improve image reconstruction quality by enhancing
resolution and preserving essential features, offering a promising direction
for future applications in computer vision and synthetic data generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PixelMan: Consistent Object Editing with Diffusion Models via Pixel
  Manipulation and Generation <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14283v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14283v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liyao Jiang, Negar Hassanpour, Mohammad Salameh, Mohammadreza Samadi, Jiao He, Fengyu Sun, Di Niu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research explores the potential of Diffusion Models (DMs) for
consistent object editing, which aims to modify object position, size, and
composition, etc., while preserving the consistency of objects and background
without changing their texture and attributes. Current inference-time methods
often rely on DDIM inversion, which inherently compromises efficiency and the
achievable consistency of edited images. Recent methods also utilize energy
guidance which iteratively updates the predicted noise and can drive the
latents away from the original image, resulting in distortions. In this paper,
we propose PixelMan, an inversion-free and training-free method for achieving
consistent object editing via Pixel Manipulation and generation, where we
directly create a duplicate copy of the source object at target location in the
pixel space, and introduce an efficient sampling approach to iteratively
harmonize the manipulated object into the target location and inpaint its
original location, while ensuring image consistency by anchoring the edited
image to be generated to the pixel-manipulated image as well as by introducing
various consistency-preserving optimization techniques during inference.
Experimental evaluations based on benchmark datasets as well as extensive
visual comparisons show that in as few as 16 inference steps, PixelMan
outperforms a range of state-of-the-art training-based and training-free
methods (usually requiring 50 steps) on multiple consistent object editing
tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2025; version includes supplementary material; 27 Pages, 15
  Figures, 6 Tables</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">15</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can we Retrieve Everything All at Once? ARM: An Alignment-Oriented
  LLM-based Retrieval Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18539v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18539v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter Baile Chen, Yi Zhang, Michael Cafarella, Dan Roth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world open-domain questions can be complicated, particularly when
answering them involves information from multiple information sources. LLMs
have demonstrated impressive performance in decomposing complex tasks into
simpler steps, and previous work has used it for better retrieval in support of
complex questions. However, LLM's decomposition of questions is unaware of what
data is available and how data is organized, often leading to a sub-optimal
retrieval performance. Recent effort in agentic RAG proposes to perform
retrieval in an iterative fashion, where a followup query is derived as an
action based on previous rounds of retrieval. While this provides one way of
interacting with the data collection, agentic RAG's exploration of data is
inefficient because successive queries depend on previous results rather than
being guided by the organization of available data in the collection. To
address this problem, we propose an LLM-based retrieval method -- ARM, that
aims to better align the question with the organization of the data collection
by exploring relationships among data objects beyond matching the utterance of
the query, thus leading to a retrieve-all-at-once solution for complex queries.
We evaluated ARM on two datasets, Bird and OTT-QA. On Bird, it outperforms
standard RAG with query decomposition by up to 5.2 pt in execution accuracy and
agentic RAG (ReAct) by up to 15.9 pt. On OTT-QA, it achieves up to 5.5 pt and
19.3 pt higher F1 match scores compared to these approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Illusions of Relevance: Using Content Injection Attacks to Deceive
  Retrievers, Rerankers, and LLM Judges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18536v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18536v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manveer Singh Tamber, Jimmy Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Consider a scenario in which a user searches for information, only to
encounter texts flooded with misleading or non-relevant content. This scenario
exemplifies a simple yet potent vulnerability in neural Information Retrieval
(IR) pipelines: content injection attacks. We find that embedding models for
retrieval, rerankers, and large language model (LLM) relevance judges are
vulnerable to these attacks, in which adversaries insert misleading text into
passages to manipulate model judgements. We identify two primary threats: (1)
inserting unrelated or harmful content within passages that still appear
deceptively "relevant", and (2) inserting entire queries or key query terms
into passages to boost their perceived relevance. While the second tactic has
been explored in prior research, we present, to our knowledge, the first
empirical analysis of the first threat, demonstrating how state-of-the-art
models can be easily misled. Our study systematically examines the factors that
influence an attack's success, such as the placement of injected content and
the balance between relevant and non-relevant material. Additionally, we
explore various defense strategies, including adversarial passage classifiers,
retriever fine-tuning to discount manipulated content, and prompting LLM judges
to adopt a more cautious approach. However, we find that these countermeasures
often involve trade-offs, sacrificing effectiveness for attack robustness and
sometimes penalizing legitimate documents in the process. Our findings
highlight the need for stronger defenses against these evolving adversarial
strategies to maintain the trustworthiness of IR systems. We release our code
and scripts to facilitate further research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RbFT: Robust Fine-tuning for Retrieval-Augmented Generation against
  Retrieval Defects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18365v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18365v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiteng Tu, Weihang Su, Yujia Zhou, Yiqun Liu, Qingyao Ai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) enhances large language models (LLMs) by
integrating external knowledge retrieved from a knowledge base. However, its
effectiveness is fundamentally constrained by the reliability of both the
retriever and the knowledge base. In real-world scenarios, imperfections in
these components often lead to the retrieval of noisy, irrelevant, or
misleading counterfactual information, ultimately undermining the
trustworthiness of RAG systems. To address this challenge, we propose Robust
Fine-Tuning (RbFT), a method designed to enhance the resilience of LLMs against
retrieval defects through two targeted fine-tuning tasks. Experimental results
demonstrate that RbFT significantly improves the robustness of RAG systems
across diverse retrieval conditions, surpassing existing methods while
maintaining high inference efficiency and compatibility with other robustness
techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Citation Recommendation based on Argumentative Zoning of User Queries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18292v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18292v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shutian Ma, Chengzhi Zhang, Heng Zhang, Zheng Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Citation recommendation aims to locate the important papers for scholars to
cite. When writing the citing sentences, the authors usually hold different
citing intents, which are referred to citation function in citation analysis.
Since argumentative zoning is to identify the argumentative and rhetorical
structure in scientific literature, we want to use this information to improve
the citation recommendation task. In this paper, a multi-task learning model is
built for citation recommendation and argumentative zoning classification. We
also generated an annotated corpus of the data from PubMed Central based on a
new argumentative zoning schema. The experimental results show that, by
considering the argumentative information in the citing sentence, citation
recommendation model will get better performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Collecting Cost-Effective, High-Quality Truthfulness Assessments with
  LLM Summarized Evidence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18265v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18265v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Roitero, Dustin Wright, Michael Soprano, Isabelle Augenstein, Stefano Mizzaro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the degradation of guardrails against mis- and disinformation online, it
is more critical than ever to be able to effectively combat it. In this paper,
we explore the efficiency and effectiveness of using crowd-sourced truthfulness
assessments based on condensed, large language model (LLM) generated summaries
of online sources. We compare the use of generated summaries to the use of
original web pages in an A/B testing setting, where we employ a large and
diverse pool of crowd-workers to perform the truthfulness assessment. We
evaluate the quality of assessments, the efficiency with which assessments are
performed, and the behavior and engagement of participants. Our results
demonstrate that the Summary modality, which relies on summarized evidence,
offers no significant change in assessment accuracy over the Standard modality,
while significantly increasing the speed with which assessments are performed.
Workers using summarized evidence produce a significantly higher number of
assessments in the same time frame, reducing the cost needed to acquire
truthfulness assessments. Additionally, the Summary modality maximizes both the
inter-annotator agreements as well as the reliance on and perceived usefulness
of evidence, demonstrating the utility of summarized evidence without
sacrificing the quality of assessments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages; 7 figures; 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Behavior Modeling Space Reconstruction for E-Commerce Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18216v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18216v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yejing Wang, Chi Zhang, Xiangyu Zhao, Qidong Liu, Maolin Wang, Xuewei Tao, Zitao Liu, Xing Shi, Xudong Yang, Ling Zhong, Wei Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Delivering superior search services is crucial for enhancing customer
experience and driving revenue growth. Conventionally, search systems model
user behaviors by combining user preference and query item relevance
statically, often through a fixed logical 'and' relationship. This paper
reexamines existing approaches through a unified lens using both causal graphs
and Venn diagrams, uncovering two prevalent yet significant issues: entangled
preference and relevance effects, and a collapsed modeling space. To surmount
these challenges, our research introduces a novel framework, DRP, which
enhances search accuracy through two components to reconstruct the behavior
modeling space. Specifically, we implement preference editing to proactively
remove the relevance effect from preference predictions, yielding untainted
user preferences. Additionally, we employ adaptive fusion, which dynamically
adjusts fusion criteria to align with the varying patterns of relevance and
preference, facilitating more nuanced and tailored behavior predictions within
the reconstructed modeling space. Empirical validation on two public datasets
and a proprietary search dataset underscores the superiority of our proposed
methodology, demonstrating marked improvements in performance over existing
approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hashtag Re-Appropriation for Audience Control on Recommendation-Driven
  Social Media Xiaohongshu (rednote) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18210v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18210v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruyuan Wan, Lingbo Tong, Tiffany Knearem, Toby Jia-Jun Li, Ting-Hao 'Kenneth' Huang, Qunfang Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Algorithms have played a central role in personalized recommendations on
social media. However, they also present significant obstacles for content
creators trying to predict and manage their audience reach. This issue is
particularly challenging for marginalized groups seeking to maintain safe
spaces. Our study explores how women on Xiaohongshu (rednote), a
recommendation-driven social platform, proactively re-appropriate hashtags
(e.g., #Baby Supplemental Food) by using them in posts unrelated to their
literal meaning. The hashtags were strategically chosen from topics that would
be uninteresting to the male audience they wanted to block. Through a
mixed-methods approach, we analyzed the practice of hashtag re-appropriation
based on 5,800 collected posts and interviewed 24 active users from diverse
backgrounds to uncover users' motivations and reactions towards the
re-appropriation. This practice highlights how users can reclaim agency over
content distribution on recommendation-driven platforms, offering insights into
self-governance within algorithmic-centered power structures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Investigating Tax Evasion Emergence Using Dual Large Language Model and
  Deep Reinforcement Learning Powered Agent-based Simulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18177v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18177v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Teddy Lazebnik, Labib Shami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tax evasion, usually the largest component of an informal economy, is a
persistent challenge over history with significant socio-economic implications.
Many socio-economic studies investigate its dynamics, including influencing
factors, the role and influence of taxation policies, and the prediction of the
tax evasion volume over time. These studies assumed such behavior is given, as
observed in the real world, neglecting the "big bang" of such activity in a
population. To this end, computational economy studies adopted developments in
computer simulations, in general, and recent innovations in artificial
intelligence (AI), in particular, to simulate and study informal economy
appearance in various socio-economic settings. This study presents a novel
computational framework to examine the dynamics of tax evasion and the
emergence of informal economic activity. Employing an agent-based simulation
powered by Large Language Models and Deep Reinforcement Learning, the framework
is uniquely designed to allow informal economic behaviors to emerge
organically, without presupposing their existence or explicitly signaling
agents about the possibility of evasion. This provides a rigorous approach for
exploring the socio-economic determinants of compliance behavior. The
experimental design, comprising model validation and exploratory phases,
demonstrates the framework's robustness in replicating theoretical economic
behaviors. Findings indicate that individual personality traits, external
narratives, enforcement probabilities, and the perceived efficiency of public
goods provision significantly influence both the timing and extent of informal
economic activity. The results underscore that efficient public goods provision
and robust enforcement mechanisms are complementary; neither alone is
sufficient to curtail informal activity effectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HyperZero: A Customized End-to-End Auto-Tuning System for Recommendation
  with Hourly Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18126v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18126v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xufeng Cai, Ziwei Guan, Lei Yuan, Ali Selman Aydin, Tengyu Xu, Boying Liu, Wenbo Ren, Renkai Xiang, Songyi He, Haichuan Yang, Serena Li, Mingze Gao, Yue Weng, Ji Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern recommendation systems can be broadly divided into two key stages: the
ranking stage, where the system predicts various user engagements (e.g.,
click-through rate, like rate, follow rate, watch time), and the value model
stage, which aggregates these predictive scores through a function (e.g., a
linear combination defined by a weight vector) to measure the value of each
content by a single numerical score. Both stages play roughly equally important
roles in real industrial systems; however, how to optimize the model weights
for the second stage still lacks systematic study. This paper focuses on
optimizing the second stage through auto-tuning technology. Although general
auto-tuning systems and solutions - both from established production practices
and open-source solutions - can address this problem, they typically require
weeks or even months to identify a feasible solution. Such prolonged tuning
processes are unacceptable in production environments for recommendation
systems, as suboptimal value models can severely degrade user experience. An
effective auto-tuning solution is required to identify a viable model within
2-3 days, rather than the extended timelines typically associated with existing
approaches. In this paper, we introduce a practical auto-tuning system named
HyperZero that addresses these time constraints while effectively solving the
unique challenges inherent in modern recommendation systems. Moreover, this
framework has the potential to be expanded to broader tuning tasks within
recommendation systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Minimax Group Fairness in Sequential Recommendation <span class="chip">ECIR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18117v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18117v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Krishna Acharya, David Wardrope, Timos Korres, Aleksandr Petrov, Anders Uhrenholt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training sequential recommenders such as SASRec with uniform sample weights
achieves good overall performance but can fall short on specific user groups.
One such example is popularity bias, where mainstream users receive better
recommendations than niche content viewers. To improve recommendation quality
across diverse user groups, we explore three Distributionally Robust
Optimization(DRO) methods: Group DRO, Streaming DRO, and Conditional Value at
Risk (CVaR) DRO. While Group and Streaming DRO rely on group annotations and
struggle with users belonging to multiple groups, CVaR does not require such
annotations and can naturally handle overlapping groups. In experiments on two
real-world datasets, we show that the DRO methods outperform standard training,
with CVaR delivering the best results. Additionally, we find that Group and
Streaming DRO are sensitive to the choice of group used for loss computation.
Our contributions include (i) a novel application of CVaR to recommenders, (ii)
showing that the DRO methods improve group metrics as well as overall
performance, and (iii) demonstrating CVaR's effectiveness in the practical
scenario of intersecting user groups.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted to the IR for Good track at ECIR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LemmaHead: RAG Assisted Proof Generation Using Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15797v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15797v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianbo Yang, Mingqi Yang, Hongyi Zhao, Tianshuo Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing the logic necessary to solve mathematical problems or write
mathematical proofs is one of the more difficult objectives for large language
models (LLMS). Currently, the most popular methods in literature consists of
fine-tuning the model on written mathematical content such as academic
publications and textbooks, so that the model can learn to emulate the style of
mathematical writing. In this project, we explore the effectiveness of using
retrieval augmented generation (RAG) to address gaps in the mathematical
reasoning of LLMs. We develop LemmaHead, a RAG knowledge base that supplements
queries to the model with relevant mathematical context, with particular focus
on context from published textbooks. To measure our model's performance in
mathematical reasoning, our testing paradigm focuses on the task of automated
theorem proving via generating proofs to a given mathematical claim in the Lean
formal language.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging Multimodal LLM for Inspirational User Interface Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17799v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17799v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seokhyeon Park, Yumin Song, Soohyun Lee, Jaeyoung Kim, Jinwook Seo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inspirational search, the process of exploring designs to inform and inspire
new creative work, is pivotal in mobile user interface (UI) design. However,
exploring the vast space of UI references remains a challenge. Existing
AI-based UI search methods often miss crucial semantics like target users or
the mood of apps. Additionally, these models typically require metadata like
view hierarchies, limiting their practical use. We used a multimodal large
language model (MLLM) to extract and interpret semantics from mobile UI images.
We identified key UI semantics through a formative study and developed a
semantic-based UI search system. Through computational and human evaluations,
we demonstrate that our approach significantly outperforms existing UI
retrieval methods, offering UI designers a more enriched and contextually
relevant search experience. We enhance the understanding of mobile UI design
semantics and highlight MLLMs' potential in inspirational search, providing a
rich dataset of UI semantics for future studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings of the SIGCHI Conference on Human Factors in Computing
  Systems (CHI '25)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hierarchical Time-Aware Mixture of Experts for Multi-Modal Sequential
  Recommendation <span class="chip">WWW 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14269v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14269v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengzhe Zhang, Liyi Chen, Dazhong Shen, Chao Wang, Hui Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal sequential recommendation (SR) leverages multi-modal data to
learn more comprehensive item features and user preferences than traditional SR
methods, which has become a critical topic in both academia and industry.
Existing methods typically focus on enhancing multi-modal information utility
through adaptive modality fusion to capture the evolving of user preference
from user-item interaction sequences. However, most of them overlook the
interference caused by redundant interest-irrelevant information contained in
rich multi-modal data. Additionally, they primarily rely on implicit temporal
information based solely on chronological ordering, neglecting explicit
temporal signals that could more effectively represent dynamic user interest
over time. To address these limitations, we propose a Hierarchical time-aware
Mixture of experts for multi-modal Sequential Recommendation (HM4SR) with a
two-level Mixture of Experts (MoE) and a multi-task learning strategy.
Specifically, the first MoE, named Interactive MoE, extracts essential user
interest-related information from the multi-modal data of each item. Then, the
second MoE, termed Temporal MoE, captures user dynamic interests by introducing
explicit temporal embeddings from timestamps in modality encoding. To further
address data sparsity, we propose three auxiliary supervision tasks:
sequence-level category prediction (CP) for item feature understanding,
contrastive learning on ID (IDCL) to align sequence context with user
interests, and placeholder contrastive learning (PCL) to integrate temporal
information with modalities for dynamic interest modeling. Extensive
experiments on four public datasets verify the effectiveness of HM4SR compared
to several state-of-the-art approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to WWW 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative Information Retrieval Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.08137v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.08137v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marwah Alaofi, Negar Arabzadeh, Charles L. A. Clarke, Mark Sanderson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this chapter, we consider generative information retrieval evaluation from
two distinct but interrelated perspectives. First, large language models (LLMs)
themselves are rapidly becoming tools for evaluation, with current research
indicating that LLMs may be superior to crowdsource workers and other paid
assessors on basic relevance judgement tasks. We review past and ongoing
related research, including speculation on the future of shared task
initiatives, such as TREC, and a discussion on the continuing need for human
assessments. Second, we consider the evaluation of emerging LLM-based
generative information retrieval (GenIR) systems, including retrieval augmented
generation (RAG) systems. We consider approaches that focus both on the
end-to-end evaluation of GenIR systems and on the evaluation of a retrieval
component as an element in a RAG system. Going forward, we expect the
evaluation of GenIR systems to be at least partially based on LLM-based
assessment, creating an apparent circularity, with a system seemingly
evaluating its own output. We resolve this apparent circularity in two ways: 1)
by viewing LLM-based assessment as a form of "slow search", where a slower IR
system is used for evaluation and training of a faster production IR system;
and 2) by recognizing a continuing need to ground evaluation in human
assessment, even if the characteristics of that human assessment must change.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This chapter is part of the book Information Access in the Era of
  Generative AI, co-edited by Chirag Shah and Ryen White</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MixRec: Individual and Collective Mixing Empowers Data Augmentation for
  Recommender Systems <span class="chip">WWW'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13579v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13579v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Zhang, Yiwen Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The core of the general recommender systems lies in learning high-quality
embedding representations of users and items to investigate their positional
relations in the feature space. Unfortunately, data sparsity caused by
difficult-to-access interaction data severely limits the effectiveness of
recommender systems. Faced with such a dilemma, various types of
self-supervised learning methods have been introduced into recommender systems
in an attempt to alleviate the data sparsity through distribution modeling or
data augmentation. However, most data augmentation relies on elaborate manual
design, which is not only not universal, but the bloated and redundant
augmentation process may significantly slow down model training progress. To
tackle these limitations, we propose a novel Dual Mixing-based Recommendation
Framework (MixRec) to empower data augmentation as we wish. Specifically, we
propose individual mixing and collective mixing, respectively. The former aims
to provide a new positive sample that is unique to the target (user or item)
and to make the pair-wise recommendation loss benefit from it, while the latter
aims to portray a new sample that contains group properties in a batch. The two
mentioned mixing mechanisms allow for data augmentation with only one parameter
that does not need to be set multiple times and can be done in linear time
complexity. Besides, we propose the dual-mixing contrastive learning to
maximize the utilization of these new-constructed samples to enhance the
consistency between pairs of positive samples. Experimental results on four
real-world datasets demonstrate the advantages of MixRec in terms of
effectiveness, simplicity, efficiency, and scalability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by WWW'25</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">150</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeltaLLM: Compress LLMs with Low-Rank Deltas between Shared Weights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18596v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18596v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liana Mikaelyan, Ayyoob Imani, Mathew Salvaris, Parth Pathak, Mohsen Fayyaz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce DeltaLLM, a new post-training compression technique to reduce
the memory footprint of LLMs. We propose an alternative way of structuring LLMs
with weight sharing between layers in subsequent Transformer blocks, along with
additional low-rank difference matrices between them. For training, we adopt
the progressing module replacement method and show that the lightweight
training of the low-rank modules with approximately 30M-40M tokens is
sufficient to achieve performance on par with LLMs of comparable sizes trained
from scratch. We release the resultant models, DeltaLLAMA and DeltaPHI, with a
12% parameter reduction, retaining 90% of the performance of the base Llama and
Phi models on common knowledge and reasoning benchmarks. Our method also
outperforms compression techniques JointDrop, LaCo, ShortGPT and SliceGPT with
the same number of parameters removed. For example, DeltaPhi 2.9B with a 24%
reduction achieves similar average zero-shot accuracies as recovery fine-tuned
SlicedPhi 3.3B with a 12% reduction, despite being approximately 400M
parameters smaller with no fine-tuning applied. This work provides new insights
into LLM architecture design and compression methods when storage space is
critical.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diffusion Autoencoders are Scalable Image Tokenizers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18593v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18593v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinbo Chen, Rohit Girdhar, Xiaolong Wang, Sai Saketh Rambhatla, Ishan Misra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tokenizing images into compact visual representations is a key step in
learning efficient and high-quality image generative models. We present a
simple diffusion tokenizer (DiTo) that learns compact visual representations
for image generation models. Our key insight is that a single learning
objective, diffusion L2 loss, can be used for training scalable image
tokenizers. Since diffusion is already widely used for image generation, our
insight greatly simplifies training such tokenizers. In contrast, current
state-of-the-art tokenizers rely on an empirically found combination of
heuristics and losses, thus requiring a complex training recipe that relies on
non-trivially balancing different losses and pretrained supervised models. We
show design decisions, along with theoretical grounding, that enable us to
scale DiTo for learning competitive image representations. Our results show
that DiTo is a simpler, scalable, and self-supervised alternative to the
current state-of-the-art image tokenizer which is supervised. DiTo achieves
competitive or better quality than state-of-the-art in image reconstruction and
downstream image generation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://yinboc.github.io/dito/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advances in Multimodal Adaptation and Generalization: From Traditional
  Approaches to Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18592v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18592v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Dong, Moru Liu, Kaiyang Zhou, Eleni Chatzi, Juho Kannala, Cyrill Stachniss, Olga Fink
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real-world scenarios, achieving domain adaptation and generalization poses
significant challenges, as models must adapt to or generalize across unknown
target distributions. Extending these capabilities to unseen multimodal
distributions, i.e., multimodal domain adaptation and generalization, is even
more challenging due to the distinct characteristics of different modalities.
Significant progress has been made over the years, with applications ranging
from action recognition to semantic segmentation. Besides, the recent advent of
large-scale pre-trained multimodal foundation models, such as CLIP, has
inspired works leveraging these models to enhance adaptation and generalization
performances or adapting them to downstream tasks. This survey provides the
first comprehensive review of recent advances from traditional approaches to
foundation models, covering: (1) Multimodal domain adaptation; (2) Multimodal
test-time adaptation; (3) Multimodal domain generalization; (4) Domain
adaptation and generalization with the help of multimodal foundation models;
and (5) Adaptation of multimodal foundation models. For each topic, we formally
define the problem and thoroughly review existing methods. Additionally, we
analyze relevant datasets and applications, highlighting open challenges and
potential future research directions. We maintain an active repository that
contains up-to-date literature at
https://github.com/donghao51/Awesome-Multimodal-Adaptation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page:
  https://github.com/donghao51/Awesome-Multimodal-Adaptation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accuracy and Robustness of Weight-Balancing Methods for Training PINNs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18582v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18582v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthieu Barreau, Haoming Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Physics-Informed Neural Networks (PINNs) have emerged as powerful tools for
integrating physics-based models with data by minimizing both data and physics
losses. However, this multi-objective optimization problem is notoriously
challenging, with some benchmark problems leading to unfeasible solutions. To
address these issues, various strategies have been proposed, including adaptive
weight adjustments in the loss function. In this work, we introduce clear
definitions of accuracy and robustness in the context of PINNs and propose a
novel training algorithm based on the Primal-Dual (PD) optimization framework.
Our approach enhances the robustness of PINNs while maintaining comparable
performance to existing weight-balancing methods. Numerical experiments
demonstrate that the PD method consistently achieves reliable solutions across
all investigated cases and can be easily implemented, facilitating its
practical adoption. The code is available at
https://github.com/haoming-SHEN/Accuracy-and-Robustness-of-Weight-Balancing-Methods-for-Training-PINNs.git.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bias-variance decompositions: the exclusive privilege of Bregman
  divergences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18581v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18581v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tom Heskes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bias-variance decompositions are widely used to understand the generalization
performance of machine learning models. While the squared error loss permits a
straightforward decomposition, other loss functions - such as zero-one loss or
$L_1$ loss - either fail to sum bias and variance to the expected loss or rely
on definitions that lack the essential properties of meaningful bias and
variance. Recent research has shown that clean decompositions can be achieved
for the broader class of Bregman divergences, with the cross-entropy loss as a
special case. However, the necessary and sufficient conditions for these
decompositions remain an open question.
  In this paper, we address this question by studying continuous, nonnegative
loss functions that satisfy the identity of indiscernibles under mild
regularity conditions. We prove that so-called $g$-Bregman divergences are the
only such loss functions that have a clean bias-variance decomposition. A
$g$-Bregman divergence can be transformed into a standard Bregman divergence
through an invertible change of variables. This makes the squared Mahalanobis
distance, up to such a variable transformation, the only symmetric loss
function with a clean bias-variance decomposition. We also examine the impact
of relaxing the restrictions on the loss functions and how this affects our
results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Node Classification and Search on the Rubik's Cube Graph with GNNs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18580v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18580v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessandro Barro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study focuses on the application of deep geometric models to solve the
3x3x3 Rubik's Cube. We begin by discussing the cube's graph representation and
defining distance as the model's optimization objective. The distance
approximation task is reformulated as a node classification problem,
effectively addressed using Graph Neural Networks (GNNs). After training the
model on a random subgraph, the predicted classes are used to construct a
heuristic for $A^*$ search. We conclude with experiments comparing our
heuristic to that of the DeepCubeA model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ R.I.P.: Better Models by Survival of the Fittest <span class="highlight-title">Prompt</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18578v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18578v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ping Yu, Weizhe Yuan, Olga Golovneva, Tianhao Wu, Sainbayar Sukhbaatar, Jason Weston, Jing Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training data quality is one of the most important drivers of final model
quality. In this work, we introduce a method for evaluating data integrity
based on the assumption that low-quality input prompts result in high variance
and low quality responses. This is achieved by measuring the rejected response
quality and the reward gap between the chosen and rejected preference pair. Our
method, Rejecting Instruction Preferences (RIP) can be used to filter prompts
from existing training sets, or to make high quality synthetic datasets,
yielding large performance gains across various benchmarks compared to
unfiltered data. Using Llama 3.1-8B-Instruct, RIP improves AlpacaEval2 LC Win
Rate by 9.4%, Arena-Hard by 8.7%, and WildBench by 9.9%. Using Llama
3.3-70B-Instruct, RIP improves Arena-Hard from 67.5 to 82.9, which is from 18th
place to 6th overall in the leaderboard.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prediction-Powered Inference with Imputed Covariates and Nonuniform
  Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18577v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18577v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dan M. Kluger, Kerri Lu, Tijana Zrnic, Sherrie Wang, Stephen Bates
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning models are increasingly used to produce predictions that
serve as input data in subsequent statistical analyses. For example, computer
vision predictions of economic and environmental indicators based on satellite
imagery are used in downstream regressions; similarly, language models are
widely used to approximate human ratings and opinions in social science
research. However, failure to properly account for errors in the machine
learning predictions renders standard statistical procedures invalid. Prior
work uses what we call the Predict-Then-Debias estimator to give valid
confidence intervals when machine learning algorithms impute missing variables,
assuming a small complete sample from the population of interest. We expand the
scope by introducing bootstrap confidence intervals that apply when the
complete data is a nonuniform (i.e., weighted, stratified, or clustered) sample
and to settings where an arbitrary subset of features is imputed. Importantly,
the method can be applied to many settings without requiring additional
calculations. We prove that these confidence intervals are valid under no
assumptions on the quality of the machine learning model and are no wider than
the intervals obtained by methods that do not use machine learning predictions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Token-Hungry, Yet Precise: DeepSeek R1 Highlights the Need for
  Multi-Step Reasoning Over Speed in MATH 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18576v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18576v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Evgenii Evstafev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study investigates the performance of the DeepSeek R1 language model on
30 challenging mathematical problems derived from the MATH dataset, problems
that previously proved unsolvable by other models under time constraints.
Unlike prior work, this research removes time limitations to explore whether
DeepSeek R1's architecture, known for its reliance on token-based reasoning,
can achieve accurate solutions through a multi-step process. The study compares
DeepSeek R1 with four other models (gemini-1.5-flash-8b,
gpt-4o-mini-2024-07-18, llama3.1:8b, and mistral-8b-latest) across 11
temperature settings. Results demonstrate that DeepSeek R1 achieves superior
accuracy on these complex problems but generates significantly more tokens than
other models, confirming its token-intensive approach. The findings highlight a
trade-off between accuracy and efficiency in mathematical problem-solving with
large language models: while DeepSeek R1 excels in accuracy, its reliance on
extensive token generation may not be optimal for applications requiring rapid
responses. The study underscores the importance of considering task-specific
requirements when selecting an LLM and emphasizes the role of temperature
settings in optimizing performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 1 figure, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ No Equations Needed: Learning System Dynamics Without Relying on
  Closed-Form ODEs <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18563v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18563v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Krzysztof Kacprzyk, Mihaela van der Schaar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data-driven modeling of dynamical systems is a crucial area of machine
learning. In many scenarios, a thorough understanding of the model's behavior
becomes essential for practical applications. For instance, understanding the
behavior of a pharmacokinetic model, constructed as part of drug development,
may allow us to both verify its biological plausibility (e.g., the drug
concentration curve is non-negative and decays to zero) and to design dosing
guidelines. Discovery of closed-form ordinary differential equations (ODEs) can
be employed to obtain such insights by finding a compact mathematical equation
and then analyzing it (a two-step approach). However, its widespread use is
currently hindered because the analysis process may be time-consuming,
requiring substantial mathematical expertise, or even impossible if the
equation is too complex. Moreover, if the found equation's behavior does not
satisfy the requirements, editing it or influencing the discovery algorithms to
rectify it is challenging as the link between the symbolic form of an ODE and
its behavior can be elusive. This paper proposes a conceptual shift to modeling
low-dimensional dynamical systems by departing from the traditional two-step
modeling process. Instead of first discovering a closed-form equation and then
analyzing it, our approach, direct semantic modeling, predicts the semantic
representation of the dynamical system (i.e., description of its behavior)
directly from data, bypassing the need for complex post-hoc analysis. This
direct approach also allows the incorporation of intuitive inductive biases
into the optimization algorithm and editing the model's behavior directly,
ensuring that the model meets the desired specifications. Our approach not only
simplifies the modeling pipeline but also enhances the transparency and
flexibility of the resulting models compared to traditional closed-form ODEs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in the Proceedings of the Thirteenth International
  Conference on Learning Representations (ICLR 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bandits with Anytime Knapsacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18560v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18560v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eray Can Elumar, Cem Tekin, Osman Yagan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider bandits with anytime knapsacks (BwAK), a novel version of the BwK
problem where there is an \textit{anytime} cost constraint instead of a total
cost budget. This problem setting introduces additional complexities as it
mandates adherence to the constraint throughout the decision-making process. We
propose SUAK, an algorithm that utilizes upper confidence bounds to identify
the optimal mixture of arms while maintaining a balance between exploration and
exploitation. SUAK is an adaptive algorithm that strategically utilizes the
available budget in each round in the decision-making process and skips a round
when it is possible to violate the anytime cost constraint. In particular, SUAK
slightly under-utilizes the available cost budget to reduce the need for
skipping rounds. We show that SUAK attains the same problem-dependent regret
upper bound of $ O(K \log T)$ established in prior work under the simpler BwK
framework. Finally, we provide simulations to verify the utility of SUAK in
practical settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Loss Functions and Operators Generated by f-Divergences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18537v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18537v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vincent Roulet, Tianlin Liu, Nino Vieillard, Michael E. Sander, Mathieu Blondel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The logistic loss (a.k.a. cross-entropy loss) is one of the most popular loss
functions used for multiclass classification. It is also the loss function of
choice for next-token prediction in language modeling. It is associated with
the Kullback--Leibler (KL) divergence and the softargmax operator. In this
work, we propose to construct new convex loss functions based on
$f$-divergences. Our loss functions generalize the logistic loss in two
directions: i) by replacing the KL divergence with $f$-divergences and ii) by
allowing non-uniform reference measures. We instantiate our framework for
numerous $f$-divergences, recovering existing losses and creating new ones. By
analogy with the logistic loss, the loss function generated by an
$f$-divergence is associated with an operator, that we dub $f$-softargmax. We
derive a novel parallelizable bisection algorithm for computing the
$f$-softargmax associated with any $f$-divergence. On the empirical side, one
of the goals of this paper is to determine the effectiveness of loss functions
beyond the classical cross-entropy in a language model setting, including on
pre-training, post-training (SFT) and distillation. We show that the loss
function generated by the $\alpha$-divergence (which is equivalent to Tsallis
$\alpha$-negentropy in the case of unit reference measures) with $\alpha=1.5$
performs well across several tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Hybrid Data-Driven Approach For Analyzing And Predicting Inpatient
  Length Of Stay In Health Centre 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18535v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18535v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tasfia Noor Chowdhury, Sanjida Afrin Mou, Kazi Naimur Rahman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Patient length of stay (LoS) is a critical metric for evaluating the efficacy
of hospital management. The primary objectives encompass to improve efficiency
and reduce costs while enhancing patient outcomes and hospital capacity within
the patient journey. By seamlessly merging data-driven techniques with
simulation methodologies, the study proposes an all-encompassing framework for
the optimization of patient flow. Using a comprehensive dataset of 2.3 million
de-identified patient records, we analyzed demographics, diagnoses, treatments,
services, costs, and charges with machine learning models (Decision Tree,
Logistic Regression, Random Forest, Adaboost, LightGBM) and Python tools
(Spark, AWS clusters, dimensionality reduction). Our model predicts patient
length of stay (LoS) upon admission using supervised learning algorithms. This
hybrid approach enables the identification of key factors influencing LoS,
offering a robust framework for hospitals to streamline patient flow and
resource utilization. The research focuses on patient flow, corroborating the
efficacy of the approach, illustrating decreased patient length of stay within
a real healthcare environment. The findings underscore the potential of hybrid
data-driven models in transforming hospital management practices. This
innovative methodology provides generally flexible decision-making, training,
and patient flow enhancement; such a system could have huge implications for
healthcare administration and overall satisfaction with healthcare.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Differentially Private Steering for Large Language Model Alignment <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18532v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18532v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anmol Goel, Yaxi Hu, Iryna Gurevych, Amartya Sanyal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aligning Large Language Models (LLMs) with human values and away from
undesirable behaviors (such as hallucination) has become increasingly
important. Recently, steering LLMs towards a desired behavior via activation
editing has emerged as an effective method to mitigate harmful generations at
inference-time. Activation editing modifies LLM representations by preserving
information from positive demonstrations (e.g., truthful) and minimising
information from negative demonstrations (e.g., hallucinations). When these
demonstrations come from a private dataset, the aligned LLM may leak private
information contained in those private samples. In this work, we present the
first study of aligning LLM behavior with private datasets. Our work proposes
the \textit{\underline{P}rivate \underline{S}teering for LLM
\underline{A}lignment (PSA)} algorithm to edit LLM activations with
differential privacy (DP) guarantees. We conduct extensive experiments on seven
different benchmarks with open-source LLMs of different sizes (0.5B to 7B) and
model families (LlaMa, Qwen, Mistral and Gemma). Our results show that PSA
achieves DP guarantees for LLM alignment with minimal loss in performance,
including alignment metrics, open-ended text generation quality, and
general-purpose reasoning. We also develop the first Membership Inference
Attack (MIA) for evaluating and auditing the empirical privacy for the problem
of LLM steering via activation editing. Our attack is tailored for activation
editing and relies solely on the generated texts without their associated
probabilities. Our experiments support the theoretical guarantees by showing
improved guarantees for our \textit{PSA} algorithm compared to several existing
non-private techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025; Code: https://github.com/UKPLab/iclr2025-psa</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph Learning for Bidirectional Disease Contact Tracing on Real Human
  Mobility Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18531v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18531v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sofia Hurtado, Radu Marculescu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For rapidly spreading diseases where many cases show no symptoms, swift and
effective contact tracing is essential. While exposure notification
applications provide alerts on potential exposures, a fully automated system is
needed to track the infectious transmission routes. To this end, our research
leverages large-scale contact networks from real human mobility data to
identify the path of transmission. More precisely, we introduce a new
Infectious Path Centrality network metric that informs a graph learning edge
classifier to identify important transmission events, achieving an F1-score of
94%. Additionally, we explore bidirectional contact tracing, which quarantines
individuals both retroactively and proactively, and compare its effectiveness
against traditional forward tracing, which only isolates individuals after
testing positive. Our results indicate that when only 30% of symptomatic
individuals are tested, bidirectional tracing can reduce infectious effective
reproduction rate by 71%, thus significantly controlling the outbreak.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted into International Workshop on Disaster Network Science for
  Building Resilient Communities (REINFORCE) held at the Advances in Social
  Networks Analysis and Mining conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal generalisation and learning transition in extensive-width
  shallow neural networks near interpolation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18530v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18530v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jean Barbier, Francesco Camilli, Minh-Toan Nguyen, Mauro Pastore, Rudy Skerk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider a teacher-student model of supervised learning with a
fully-trained 2-layer neural network whose width $k$ and input dimension $d$
are large and proportional. We compute the Bayes-optimal generalisation error
of the network for any activation function in the regime where the number of
training data $n$ scales quadratically with the input dimension, i.e., around
the interpolation threshold where the number of trainable parameters $kd+k$ and
of data points $n$ are comparable. Our analysis tackles generic weight
distributions. Focusing on binary weights, we uncover a discontinuous phase
transition separating a "universal" phase from a "specialisation" phase. In the
first, the generalisation error is independent of the weight distribution and
decays slowly with the sampling rate $n/d^2$, with the student learning only
some non-linear combinations of the teacher weights. In the latter, the error
is weight distribution-dependent and decays faster due to the alignment of the
student towards the teacher network. We thus unveil the existence of a highly
predictive solution near interpolation, which is however potentially hard to
find.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages + appendix, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Joint Learning of Energy-based Models and their Partition Function 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18528v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18528v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael E. Sander, Vincent Roulet, Tianlin Liu, Mathieu Blondel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Energy-based models (EBMs) offer a flexible framework for parameterizing
probability distributions using neural networks. However, learning EBMs by
exact maximum likelihood estimation (MLE) is generally intractable, due to the
need to compute the partition function (normalization constant). In this paper,
we propose a novel formulation for approximately learning probabilistic EBMs in
combinatorially-large discrete spaces, such as sets or permutations. Our key
idea is to jointly learn both an energy model and its log-partition, both
parameterized as a neural network. Our approach not only provides a novel
tractable objective criterion to learn EBMs by stochastic gradient descent
(without relying on MCMC), but also a novel means to estimate the log-partition
function on unseen data points. On the theoretical side, we show that our
approach recovers the optimal MLE solution when optimizing in the space of
continuous functions. Furthermore, we show that our approach naturally extends
to the broader family of Fenchel-Young losses, allowing us to obtain the first
tractable method for optimizing the sparsemax loss in combinatorially-large
spaces. We demonstrate our approach on multilabel classification and label
ranking.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Discovery in Mathematics: Do Machines Dream of Colored Planes? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18527v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18527v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Konrad Mundinger, Max Zimmer, Aldo Kiem, Christoph Spiegel, Sebastian Pokutta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We demonstrate how neural networks can drive mathematical discovery through a
case study of the Hadwiger-Nelson problem, a long-standing open problem from
discrete geometry and combinatorics about coloring the plane avoiding
monochromatic unit-distance pairs. Using neural networks as approximators, we
reformulate this mixed discrete-continuous geometric coloring problem as an
optimization task with a probabilistic, differentiable loss function. This
enables gradient-based exploration of admissible configurations that most
significantly led to the discovery of two novel six-colorings, providing the
first improvements in thirty years to the off-diagonal variant of the original
problem (Mundinger et al., 2024a). Here, we establish the underlying machine
learning approach used to obtain these results and demonstrate its broader
applicability through additional results and numerical insights.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages main paper, 10 pages references and appendix, 17 figures, 1
  table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WILDCHAT-50M: A Deep Dive Into the Role of Synthetic Data in
  Post-Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18511v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18511v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Feuer, Chinmay Hegde
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language model (LLM) post-training, from DPO to distillation, can refine
behaviors and unlock new skills, but the open science supporting these
post-training techniques is still in its infancy. One limiting factor has been
the difficulty of conducting large-scale comparative analyses of synthetic data
generating models and LLM judges. To close this gap, we introduce WILDCHAT-50M,
the largest public chat dataset to date. We extend the existing WildChat
dataset to include responses not only from GPT, but from over 50 different
open-weight models, ranging in size from 0.5B to 104B parameters. We conduct an
extensive comparative analysis and demonstrate the potential of this dataset by
creating RE-WILD, our own public SFT mix, which outperforms the recent Tulu-3
SFT mixture from Allen AI with only 40% as many samples. Our dataset, samples
and code are available at https://github.com/penfever/wildchat-50m.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Prior Limits: Addressing Distribution Misalignment in Particle
  Filtering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18501v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18501v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwei Shi, Jingyu Hu, Yu Zhang, Mengyue Yang, Weinan Zhang, Cunjia Liu, Weiru Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Particle filtering is a Bayesian inference method and a fundamental tool in
state estimation for dynamic systems, but its effectiveness is often limited by
the constraints of the initial prior distribution, a phenomenon we define as
the Prior Boundary Phenomenon. This challenge arises when target states lie
outside the prior's support, rendering traditional particle filtering methods
inadequate for accurate estimation. Although techniques like unbounded priors
and larger particle sets have been proposed, they remain computationally
prohibitive and lack adaptability in dynamic scenarios. To systematically
overcome these limitations, we propose the Diffusion-Enhanced Particle
Filtering Framework, which introduces three key innovations: adaptive diffusion
through exploratory particles, entropy-driven regularisation to prevent weight
collapse, and kernel-based perturbations for dynamic support expansion. These
mechanisms collectively enable particle filtering to explore beyond prior
boundaries, ensuring robust state estimation for out-of-boundary targets.
Theoretical analysis and extensive experiments validate framework's
effectiveness, indicating significant improvements in success rates and
estimation accuracy across high-dimensional and non-convex scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Runway vs. Taxiway: Challenges in Automated Line Identification and
  Notation Approaches 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18494v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18494v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Parth Ganeriwala, Amy Alvarez, Abdullah AlQahtani, Siddhartha Bhattacharyya, Mohammed Abdul Hafeez Khan, Natasha Neogi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing complexity of autonomous systems has amplified the need for
accurate and reliable labeling of runway and taxiway markings to ensure
operational safety. Precise detection and labeling of these markings are
critical for tasks such as navigation, landing assistance, and ground control
automation. Existing labeling algorithms, like the Automated Line
Identification and Notation Algorithm (ALINA), have demonstrated success in
identifying taxiway markings but encounter significant challenges when applied
to runway markings. This limitation arises due to notable differences in line
characteristics, environmental context, and interference from elements such as
shadows, tire marks, and varying surface conditions. To address these
challenges, we modified ALINA by adjusting color thresholds and refining region
of interest (ROI) selection to better suit runway-specific contexts. While
these modifications yielded limited improvements, the algorithm still struggled
with consistent runway identification, often mislabeling elements such as the
horizon or non-relevant background features. This highlighted the need for a
more robust solution capable of adapting to diverse visual interferences. In
this paper, we propose integrating a classification step using a Convolutional
Neural Network (CNN) named AssistNet. By incorporating this classification
step, the detection pipeline becomes more resilient to environmental variations
and misclassifications. This work not only identifies the challenges but also
outlines solutions, paving the way for improved automated labeling techniques
essential for autonomous aviation systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at SysCon 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GuardReasoner: Towards Reasoning-based LLM Safeguards 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18492v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18492v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Liu, Hongcheng Gao, Shengfang Zhai, Jun Xia, Tianyi Wu, Zhiwei Xue, Yulin Chen, Kenji Kawaguchi, Jiaheng Zhang, Bryan Hooi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As LLMs increasingly impact safety-critical applications, ensuring their
safety using guardrails remains a key challenge. This paper proposes
GuardReasoner, a new safeguard for LLMs, by guiding the guard model to learn to
reason. Concretely, we first create the GuardReasonerTrain dataset, which
consists of 127K samples with 460K detailed reasoning steps. Then, we introduce
reasoning SFT to unlock the reasoning capability of guard models. In addition,
we present hard sample DPO to further strengthen their reasoning ability. In
this manner, GuardReasoner achieves better performance, explainability, and
generalizability. Extensive experiments and analyses on 13 benchmarks of 3
guardrail tasks demonstrate its superiority. Remarkably, GuardReasoner 8B
surpasses GPT-4o+CoT by 5.74% and LLaMA Guard 3 8B by 20.84% F1 score on
average. We release the training data, code, and models with different scales
(1B, 3B, 8B) of GuardReasoner : https://github.com/yueliu1999/GuardReasoner/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 18 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CLoQ: Enhancing Fine-Tuning of Quantized LLMs via Calibrated LoRA
  Initialization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18475v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18475v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanxia Deng, Aozhong Zhang, Naigang Wang, Selcuk Gurses, Zi Yang, Penghang Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning large language models (LLMs) using low-rank adaptation (LoRA) has
become a highly efficient approach for downstream tasks, particularly in
scenarios with limited computational resources. However, applying LoRA
techniques to quantized LLMs poses unique challenges due to the reduced
representational precision of quantized weights. In this paper, we introduce
CLoQ (Calibrated LoRA initialization for Quantized LLMs), a simplistic
initialization strategy designed to overcome these challenges. Our approach
focuses on minimizing the layer-wise discrepancy between the original LLM and
its quantized counterpart with LoRA components during initialization. By
leveraging a small calibration dataset, CLoQ quantizes a pre-trained LLM and
determines the optimal LoRA components for each layer, ensuring a strong
foundation for subsequent fine-tuning. A key contribution of this work is a
novel theoretical result that enables the accurate and closed-form construction
of these optimal LoRA components. We validate the efficacy of CLoQ across
multiple tasks such as language generation, arithmetic reasoning, and
commonsense reasoning, demonstrating that it consistently outperforms existing
LoRA fine-tuning methods for quantized LLMs, especially at ultra low-bit
widths.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Resampling Filter Design for Multirate Neural Audio Effect Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18470v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18470v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alistair Carson, Vesa Välimäki, Alec Wright, Stefan Bilbao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks have become ubiquitous in audio effects modelling, especially
for guitar amplifiers and distortion pedals. One limitation of such models is
that the sample rate of the training data is implicitly encoded in the model
weights and therefore not readily adjustable at inference. Recent work explored
modifications to recurrent neural network architecture to approximate a sample
rate independent system, enabling audio processing at a rate that differs from
the original training rate. This method works well for integer oversampling and
can reduce aliasing caused by nonlinear activation functions. For small
fractional changes in sample rate, fractional delay filters can be used to
approximate sample rate independence, but in some cases this method fails
entirely. Here, we explore the use of signal resampling at the input and output
of the neural network as an alternative solution. We investigate several
resampling filter designs and show that a two-stage design consisting of a
half-band IIR filter cascaded with a Kaiser window FIR filter can give similar
or better results to the previously proposed model adjustment method with many
fewer operations per sample and less than one millisecond of latency at typical
audio rates. Furthermore, we investigate interpolation and decimation filters
for the task of integer oversampling and show that cascaded half-band IIR and
FIR designs can be used in conjunction with the model adjustment method to
reduce aliasing in a range of distortion effect models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ adabmDCA 2.0 -- a flexible but easy-to-use package for Direct Coupling
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18456v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18456v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lorenzo Rosset, Roberto Netti, Anna Paola Muntoni, Martin Weigt, Francesco Zamponi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this methods article, we provide a flexible but easy-to-use implementation
of Direct Coupling Analysis (DCA) based on Boltzmann machine learning, together
with a tutorial on how to use it. The package \texttt{adabmDCA 2.0} is
available in different programming languages (C++, Julia, Python) usable on
different architectures (single-core and multi-core CPU, GPU) using a common
front-end interface. In addition to several learning protocols for dense and
sparse generative DCA models, it allows to directly address common downstream
tasks like residue-residue contact prediction, mutational-effect prediction,
scoring of sequence libraries and generation of artificial sequences for
sequence design. It is readily applicable to protein and RNA sequence data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Clustering Properties of <span class="highlight-title">Self-Supervised</span> Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18452v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18452v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xi Weng, Jianing An, Xudong Ma, Binhang Qi, Jie Luo, Xi Yang, Jin Song Dong, Lei Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning (SSL) methods via joint embedding architectures have
proven remarkably effective at capturing semantically rich representations with
strong clustering properties, magically in the absence of label supervision.
Despite this, few of them have explored leveraging these untapped properties to
improve themselves. In this paper, we provide an evidence through various
metrics that the encoder's output $encoding$ exhibits superior and more stable
clustering properties compared to other components. Building on this insight,
we propose a novel positive-feedback SSL method, termed Representation Soft
Assignment (ReSA), which leverages the model's clustering properties to promote
learning in a self-guided manner. Extensive experiments on standard SSL
benchmarks reveal that models pretrained with ReSA outperform other
state-of-the-art SSL methods by a significant margin. Finally, we analyze how
ReSA facilitates better clustering properties, demonstrating that it
effectively enhances clustering performance at both fine-grained and
coarse-grained levels, shaping representations that are inherently more
structured and semantically meaningful.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MolGraph-xLSTM: A graph-based dual-level xLSTM framework with multi-head
  mixture-of-experts for enhanced molecular representation and interpretability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18439v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18439v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Sun, Yutong Lu, Yan Yi Li, Zihao Jing, Carson K. Leung, Pingzhao Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predicting molecular properties is essential for drug discovery, and
computational methods can greatly enhance this process. Molecular graphs have
become a focus for representation learning, with Graph Neural Networks (GNNs)
widely used. However, GNNs often struggle with capturing long-range
dependencies. To address this, we propose MolGraph-xLSTM, a novel graph-based
xLSTM model that enhances feature extraction and effectively models molecule
long-range interactions.
  Our approach processes molecular graphs at two scales: atom-level and
motif-level. For atom-level graphs, a GNN-based xLSTM framework with jumping
knowledge extracts local features and aggregates multilayer information to
capture both local and global patterns effectively. Motif-level graphs provide
complementary structural information for a broader molecular view. Embeddings
from both scales are refined via a multi-head mixture of experts (MHMoE),
further enhancing expressiveness and performance.
  We validate MolGraph-xLSTM on 10 molecular property prediction datasets,
covering both classification and regression tasks. Our model demonstrates
consistent performance across all datasets, with improvements of up to 7.03% on
the BBBP dataset for classification and 7.54% on the ESOL dataset for
regression compared to baselines. On average, MolGraph-xLSTM achieves an AUROC
improvement of 3.18\% for classification tasks and an RMSE reduction of 3.83\%
across regression datasets compared to the baseline methods. These results
confirm the effectiveness of our model, offering a promising solution for
molecular representation learning for drug discovery.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Guaranteed confidence-band enclosures for PDE surrogates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18426v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18426v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ander Gray, Vignesh Gopakumar, Sylvain Rousseau, Sébastien Destercke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a method for obtaining statistically guaranteed confidence bands
for functional machine learning techniques: surrogate models which map between
function spaces, motivated by the need build reliable PDE emulators. The method
constructs nested confidence sets on a low-dimensional representation (an SVD)
of the surrogate model's prediction error, and then maps these sets to the
prediction space using set-propagation techniques. The result are
conformal-like coverage guaranteed prediction sets for functional surrogate
models. We use zonotopes as basis of the set construction, due to their well
studied set-propagation and verification properties. The method is model
agnostic and can thus be applied to complex Sci-ML models, including Neural
Operators, but also in simpler settings. We also elicit a technique to capture
the truncation error of the SVD, ensuring the guarantees of the method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeepExtractor: Time-domain reconstruction of signals and glitches in
  gravitational wave data with deep learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18423v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18423v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tom Dooney, Harsh Narola, Stefano Bromuri, R. Lyana Curier, Chris Van Den Broeck, Sarah Caudill, Daniel Stanley Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gravitational wave (GW) interferometers, detect faint signals from distant
astrophysical events, such as binary black hole mergers. However, their high
sensitivity also makes them susceptible to background noise, which can obscure
these signals. This noise often includes transient artifacts called "glitches"
that can mimic astrophysical signals or mask their characteristics. Fast and
accurate reconstruction of both signals and glitches is crucial for reliable
scientific inference. In this study, we present DeepExtractor, a deep learning
framework designed to reconstruct signals and glitches with power exceeding
interferometer noise, regardless of their source. We design DeepExtractor to
model the inherent noise distribution of GW interferometers, following
conventional assumptions that the noise is Gaussian and stationary over short
time scales. It operates by predicting and subtracting the noise component of
the data, retaining only the clean reconstruction. Our approach achieves
superior generalization capabilities for arbitrary signals and glitches
compared to methods that directly map inputs to the clean training waveforms.
We validate DeepExtractor's effectiveness through three experiments: (1)
reconstructing simulated glitches injected into simulated detector noise, (2)
comparing performance with the state-of-the-art BayesWave algorithm, and (3)
analyzing real data from the Gravity Spy dataset to demonstrate effective
glitch subtraction from LIGO strain data. DeepExtractor achieves a median
mismatch of only 0.9% for simulated glitches, outperforming several deep
learning baselines. Additionally, DeepExtractor surpasses BayesWave in glitch
recovery, offering a dramatic computational speedup by reconstructing one
glitch sample in approx. 0.1 seconds on a CPU, compared to BayesWave's
processing time of approx. one hour per glitch.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 16 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Causal Inference Real-Time Anomaly Detection with Synthetic Anomaly
  Monitoring (SAM) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18417v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18417v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emanuele Luzio, Moacir Antonelli Ponti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anomaly detection is essential for identifying rare and significant events
across diverse domains such as finance, cybersecurity, and network monitoring.
This paper presents Synthetic Anomaly Monitoring (SAM), an innovative approach
that applies synthetic control methods from causal inference to improve both
the accuracy and interpretability of anomaly detection processes. By modeling
normal behavior through the treatment of each feature as a control unit, SAM
identifies anomalies as deviations within this causal framework. We conducted
extensive experiments comparing SAM with established benchmark models,
including Isolation Forest, Local Outlier Factor (LOF), k-Nearest Neighbors
(kNN), and One-Class Support Vector Machine (SVM), across five diverse
datasets, including Credit Card Fraud, HTTP Dataset CSIC 2010, and KDD Cup
1999, among others. Our results demonstrate that SAM consistently delivers
robust performance, highlighting its potential as a powerful tool for real-time
anomaly detection in dynamic and complex environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 3 figures, submitted for publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Potential <span class="highlight-title">Prompt</span> Injection Attacks in Federated Military LLMs
  and Their Mitigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18416v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18416v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youngjoon Lee, Taehyun Park, Yunho Lee, Jinu Gong, Joonhyuk Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) is increasingly being adopted in military
collaborations to develop Large Language Models (LLMs) while preserving data
sovereignty. However, prompt injection attacks-malicious manipulations of input
prompts-pose new threats that may undermine operational security, disrupt
decision-making, and erode trust among allies. This perspective paper
highlights four potential vulnerabilities in federated military LLMs: secret
data leakage, free-rider exploitation, system disruption, and misinformation
spread. To address these potential risks, we propose a human-AI collaborative
framework that introduces both technical and policy countermeasures. On the
technical side, our framework uses red/blue team wargaming and quality
assurance to detect and mitigate adversarial behaviors of shared LLM weights.
On the policy side, it promotes joint AI-human policy development and
verification of security protocols. Our findings will guide future research and
emphasize proactive strategies for emerging military contexts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Consensus statement on the credibility assessment of ML predictors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18415v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18415v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessandra Aldieri, Thiranja Prasad Babarenda Gamage, Antonino Amedeo La Mattina, Yi Li, Axel Loewe, Francesco Pappalardo, Marco Viceconti Italy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid integration of machine learning (ML) predictors into in silico
medicine has revolutionized the estimation of quantities of interest (QIs) that
are otherwise challenging to measure directly. However, the credibility of
these predictors is critical, especially when they inform high-stakes
healthcare decisions. This position paper presents a consensus statement
developed by experts within the In Silico World Community of Practice. We
outline twelve key statements forming the theoretical foundation for evaluating
the credibility of ML predictors, emphasizing the necessity of causal
knowledge, rigorous error quantification, and robustness to biases. By
comparing ML predictors with biophysical models, we highlight unique challenges
associated with implicit causal knowledge and propose strategies to ensure
reliability and applicability. Our recommendations aim to guide researchers,
developers, and regulators in the rigorous assessment and deployment of ML
predictors in clinical and biomedical contexts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GBFRS: Robust Fuzzy Rough Sets via Granular-ball Computing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18413v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18413v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuyin Xia, Xiaoyu Lian, Binbin Sang, Guoyin Wang, Xinbo Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fuzzy rough set theory is effective for processing datasets with complex
attributes, supported by a solid mathematical foundation and closely linked to
kernel methods in machine learning. Attribute reduction algorithms and
classifiers based on fuzzy rough set theory exhibit promising performance in
the analysis of high-dimensional multivariate complex data. However, most
existing models operate at the finest granularity, rendering them inefficient
and sensitive to noise, especially for high-dimensional big data. Thus,
enhancing the robustness of fuzzy rough set models is crucial for effective
feature selection. Muiti-garanularty granular-ball computing, a recent
development, uses granular-balls of different sizes to adaptively represent and
cover the sample space, performing learning based on these granular-balls. This
paper proposes integrating multi-granularity granular-ball computing into fuzzy
rough set theory, using granular-balls to replace sample points. The
coarse-grained characteristics of granular-balls make the model more robust.
Additionally, we propose a new method for generating granular-balls, scalable
to the entire supervised method based on granular-ball computing. A forward
search algorithm is used to select feature sequences by defining the
correlation between features and categories through dependence functions.
Experiments demonstrate the proposed model's effectiveness and superiority over
baseline methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Segmentation of cracks in 3d images of fiber reinforced concrete using
  deep learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18405v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18405v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anna Nowacka, Katja Schladitz, Szymon Grzesiak, Matthias Pahn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cracks in concrete structures are very common and are an integral part of
this heterogeneous material. Characteristics of cracks induced by standardized
tests yield valuable information about the tested concrete formulation and its
mechanical properties. Observing cracks on the surface of the concrete
structure leaves a wealth of structural information unused. Computed tomography
enables looking into the sample without interfering or destroying the
microstructure. The reconstructed tomographic images are 3d images, consisting
of voxels whose gray values represent local X-ray absorption. In order to
identify voxels belonging to the crack, so to segment the crack structure in
the images, appropriate algorithms need to be developed. Convolutional neural
networks are known to solve this type of task very well given enough and
consistent training data. We adapted a 3d version of the well-known U-Net and
trained it on semi-synthetic 3d images of real concrete samples equipped with
simulated crack structures. Here, we explain the general approach. Moreover, we
show how to teach the network to detect also real crack systems in 3d images of
varying types of real concrete, in particular of fiber reinforced concrete.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improved Replicable Boosting with Majority-of-Majorities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18388v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18388v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kasper Green Larsen, Markus Engelund Mathiasen, Clement Svendsen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a new replicable boosting algorithm which significantly improves
the sample complexity compared to previous algorithms. The algorithm works by
doing two layers of majority voting, using an improved version of the
replicable boosting algorithm introduced by Impagliazzo et al. [2022] in the
bottom layer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Implicit Riemannian Optimism with Applications to Min-Max Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18381v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18381v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christophe Roux, David Martínez-Rubio, Sebastian Pokutta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a Riemannian optimistic online learning algorithm for Hadamard
manifolds based on inexact implicit updates. Unlike prior work, our method can
handle in-manifold constraints, and matches the best known regret bounds in the
Euclidean setting with no dependence on geometric constants, like the minimum
curvature. Building on this, we develop algorithms for g-convex, g-concave
smooth min-max problems on Hadamard manifolds. Notably, one method nearly
matches the gradient oracle complexity of the lower bound for Euclidean
problems, for the first time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Function Encoders: A Principled Approach to Transfer Learning in Hil<span class="highlight-title">bert</span>
  Spaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18373v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18373v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tyler Ingebrand, Adam J. Thorpe, Ufuk Topcu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A central challenge in transfer learning is designing algorithms that can
quickly adapt and generalize to new tasks without retraining. Yet, the
conditions of when and how algorithms can effectively transfer to new tasks is
poorly characterized. We introduce a geometric characterization of transfer in
Hilbert spaces and define three types of inductive transfer: interpolation
within the convex hull, extrapolation to the linear span, and extrapolation
outside the span. We propose a method grounded in the theory of function
encoders to achieve all three types of transfer. Specifically, we introduce a
novel training scheme for function encoders using least-squares optimization,
prove a universal approximation theorem for function encoders, and provide a
comprehensive comparison with existing approaches such as transformers and
meta-learning on four diverse benchmarks. Our experiments demonstrate that the
function encoder outperforms state-of-the-art methods on four benchmark tasks
and on all three types of transfer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Cartesian Encoding Graph Neural Network for Crystal Structures
  Property Prediction: Application to Thermal Ellipsoid Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18369v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18369v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Àlex Solé, Albert Mosella-Montoro, Joan Cardona, Silvia Gómez-Coca, Daniel Aravena, Eliseo Ruiz, Javier Ruiz-Hidalgo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In diffraction-based crystal structure analysis, thermal ellipsoids,
quantified via Anisotropic Displacement Parameters (ADPs), are critical yet
challenging to determine. ADPs capture atomic vibrations, reflecting thermal
and structural properties, but traditional computation is often expensive. This
paper introduces CartNet, a novel graph neural network (GNN) for efficiently
predicting crystal properties by encoding atomic geometry into Cartesian
coordinates alongside the crystal temperature. CartNet integrates a neighbour
equalization technique to emphasize covalent and contact interactions, and a
Cholesky-based head to ensure valid ADP predictions. We also propose a
rotational SO(3) data augmentation strategy during training to handle unseen
orientations. An ADP dataset with over 200,000 experimental crystal structures
from the Cambridge Structural Database (CSD) was curated to validate the
approach. CartNet significantly reduces computational costs and outperforms
existing methods in ADP prediction by 10.87%, while delivering a 34.77%
improvement over theoretical approaches. We further evaluated CartNet on other
datasets covering formation energy, band gap, total energy, energy above the
convex hull, bulk moduli, and shear moduli, achieving 7.71% better results on
the Jarvis Dataset and 13.16% on the Materials Project Dataset. These gains
establish CartNet as a state-of-the-art solution for diverse crystal property
predictions. Project website and online demo: https://www.ee.ub.edu/cartnet
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Learnable Multi-views Contrastive Framework with Reconstruction
  Discrepancy for Medical Time-Series 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18367v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18367v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Wang, Hongfeng Ai, Ruiqi Li, Maowei Jiang, Cheng Jiang, Chenzhong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In medical time series disease diagnosis, two key challenges are
identified.First, the high annotation cost of medical data leads to overfitting
in models trained on label-limited, single-center datasets. To address this, we
propose incorporating external data from related tasks and leveraging AE-GAN to
extract prior knowledge,providing valuable references for downstream tasks.
Second, many existing studies employ contrastive learning to derive more
generalized medical sequence representations for diagnostic tasks, usually
relying on manually designed diverse positive and negative sample
pairs.However, these approaches are complex, lack generalizability, and fail to
adaptively capture disease-specific features across different conditions.To
overcome this, we introduce LMCF (Learnable Multi-views Contrastive Framework),
a framework that integrates a multi-head attention mechanism and adaptively
learns representations from different views through inter-view and intra-view
contrastive learning strategies.Additionally, the pre-trained AE-GAN is used to
reconstruct discrepancies in the target data as disease probabilities, which
are then integrated into the contrastive learning process.Experiments on three
target datasets demonstrate that our method consistently outperforms seven
other baselines, highlighting its significant impact on healthcare applications
such as the diagnosis of myocardial infarction, Alzheimer's disease, and
Parkinson's disease.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages,6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Online Conformal Prediction under Uniform Label Noise 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18363v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18363v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huajun Xi, Kangdao Liu, Hao Zeng, Wenguang Sun, Hongxin Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conformal prediction is an emerging technique for uncertainty quantification
that constructs prediction sets guaranteed to contain the true label with a
predefined probability. Recent work develops online conformal prediction
methods that adaptively construct prediction sets to accommodate distribution
shifts. However, existing algorithms typically assume perfect label accuracy
which rarely holds in practice. In this work, we investigate the robustness of
online conformal prediction under uniform label noise with a known noise rate,
in both constant and dynamic learning rate schedules. We show that label noise
causes a persistent gap between the actual mis-coverage rate and the desired
rate $\alpha$, leading to either overestimated or underestimated coverage
guarantees. To address this issue, we propose Noise Robust Online Conformal
Prediction (dubbed NR-OCP) by updating the threshold with a novel robust
pinball los}, which provides an unbiased estimate of clean pinball loss without
requiring ground-truth labels. Our theoretical analysis shows that NR-OCP
eliminates the coverage gap in both constant and dynamic learning rate
schedules, achieving a convergence rate of $\mathcal{O}(T^{-1/2})$ for both
empirical and expected coverage errors under uniform label noise. Extensive
experiments demonstrate the effectiveness of our method by achieving both
precise coverage and improved efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MedXpertQA: Benchmarking Expert-Level Medical Reasoning and
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18362v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18362v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Zuo, Shang Qu, Yifei Li, Zhangren Chen, Xuekai Zhu, Ermo Hua, Kaiyan Zhang, Ning Ding, Bowen Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce MedXpertQA, a highly challenging and comprehensive benchmark to
evaluate expert-level medical knowledge and advanced reasoning. MedXpertQA
includes 4,460 questions spanning 17 specialties and 11 body systems. It
includes two subsets, Text for text evaluation and MM for multimodal
evaluation. Notably, MM introduces expert-level exam questions with diverse
images and rich clinical information, including patient records and examination
results, setting it apart from traditional medical multimodal benchmarks with
simple QA pairs generated from image captions. MedXpertQA applies rigorous
filtering and augmentation to address the insufficient difficulty of existing
benchmarks like MedQA, and incorporates specialty board questions to improve
clinical relevance and comprehensiveness. We perform data synthesis to mitigate
data leakage risk and conduct multiple rounds of expert reviews to ensure
accuracy and reliability. We evaluate 16 leading models on MedXpertQA.
Moreover, medicine is deeply connected to real-world decision-making, providing
a rich and representative setting for assessing reasoning abilities beyond
mathematics and code. To this end, we develop a reasoning-oriented subset to
facilitate the assessment of o1-like models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contextual Online Decision Making with Infinite-Dimensional Functional
  Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18359v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18359v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haichen Hu, Rui Ai, Stephen Bates, David Simchi-Levi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contextual sequential decision-making problems play a crucial role in machine
learning, encompassing a wide range of downstream applications such as bandits,
sequential hypothesis testing and online risk control. These applications often
require different statistical measures, including expectation, variance and
quantiles. In this paper, we provide a universal admissible algorithm framework
for dealing with all kinds of contextual online decision-making problems that
directly learns the whole underlying unknown distribution instead of focusing
on individual statistics. This is much more difficult because the dimension of
the regression is uncountably infinite, and any existing linear contextual
bandits algorithm will result in infinite regret. To overcome this issue, we
propose an efficient infinite-dimensional functional regression oracle for
contextual cumulative distribution functions (CDFs), where each data point is
modeled as a combination of context-dependent CDF basis functions. Our analysis
reveals that the decay rate of the eigenvalue sequence of the design integral
operator governs the regression error rate and, consequently, the utility
regret rate. Specifically, when the eigenvalue sequence exhibits a polynomial
decay of order $\frac{1}{\gamma}\ge 1$, the utility regret is bounded by
$\tilde{\mathcal{O}}\Big(T^{\frac{3\gamma+2}{2(\gamma+2)}}\Big)$. By setting
$\gamma=0$, this recovers the existing optimal regret rate for contextual
bandits with finite-dimensional regression and is optimal under a stronger
exponential decay assumption. Additionally, we provide a numerical method to
compute the eigenvalue sequence of the integral operator, enabling the
practical implementation of our framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contrastive Learning Meets Pseudo-label-assisted Mixup Augmentation: A
  Comprehensive Graph Representation Framework from Local to Global 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18357v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18357v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinlu Wang, Yanfeng Sun, Jiapu Wang, Junbin Gao, Shaofan Wang, Jipeng Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) have demonstrated remarkable effectiveness in
various graph representation learning tasks. However, most existing GNNs focus
primarily on capturing local information through explicit graph convolution,
often neglecting global message-passing. This limitation hinders the
establishment of a collaborative interaction between global and local
information, which is crucial for comprehensively understanding graph data. To
address these challenges, we propose a novel framework called Comprehensive
Graph Representation Learning (ComGRL). ComGRL integrates local information
into global information to derive powerful representations. It achieves this by
implicitly smoothing local information through flexible graph contrastive
learning, ensuring reliable representations for subsequent global exploration.
Then ComGRL transfers the locally derived representations to a multi-head
self-attention module, enhancing their discriminative ability by uncovering
diverse and rich global correlations. To further optimize local information
dynamically under the self-supervision of pseudo-labels, ComGRL employs a
triple sampling strategy to construct mixed node pairs and applies reliable
Mixup augmentation across attributes and structure for local contrastive
learning. This approach broadens the receptive field and facilitates
coordination between local and global representation learning, enabling them to
reinforce each other. Experimental results across six widely used graph
datasets demonstrate that ComGRL achieves excellent performance in node
classification tasks. The code could be available at
https://github.com/JinluWang1002/ComGRL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ State Stream <span class="highlight-title">Transformer</span> (SST) : Emergent Metacognitive Behaviours
  Through Latent State Persistence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18356v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18356v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thea Aviss
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the State Stream Transformer (SST), a novel LLM architecture
that reveals emergent reasoning behaviours and capabilities latent in
pretrained weights through addressing a fundamental limitation in traditional
transformer models: the lack of latent computational continuity across
autoregressive generations in the state space. SST introduces a sliding window
latent state (FFN) cache with weighted decay that maintains and evolves
persistent latent processes throughout autoregressive generations. Through
controlled experiments comparing base and SST architectures using the same
frozen weights, we demonstrate that this architectural modification alone
enables enhanced reasoning capabilities which appear best explained by some
form of potential higher-order processing, as evidenced by emergent
metacognitive behaviours. These behaviours persist under controlled conditions
designed to eliminate confounding factors such as stochastic variation or
learned response patterns. Analysis of latent state distributions and
processing dynamics provides evidence that it is solely the 'state stream' that
is responsible for these phenomena. In quantitative evaluations, the SST
achieves substantial performance improvements over the base model on two
reasoning benchmarks, reaching 89.01\% accuracy on GSM-8K (0-shot) and 91.04\%
on ARC Challenge (0-shot CoT). These findings indicate that persistent
computation in the latent state space enables fundamentally different
information processing and internal reasoning strategies, with implications for
our understanding of artificial intelligence systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transfer Learning of Surrogate Models: Integrating Domain Warping and
  Affine Transformations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18344v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18344v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuaiqun Pan, Diederick Vermetten, Manuel López-Ibáñez, Thomas Bäck, Hao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Surrogate models provide efficient alternatives to computationally demanding
real-world processes but often require large datasets for effective training. A
promising solution to this limitation is the transfer of pre-trained surrogate
models to new tasks. Previous studies have investigated the transfer of
differentiable and non-differentiable surrogate models, typically assuming an
affine transformation between the source and target functions. This paper
extends previous research by addressing a broader range of transformations,
including linear and nonlinear variations. Specifically, we consider the
combination of an unknown input warping, such as one modelled by the beta
cumulative distribution function, with an unspecified affine transformation.
Our approach achieves transfer learning by employing a limited number of data
points from the target task to optimize these transformations, minimizing
empirical loss on the transfer dataset. We validate the proposed method on the
widely used Black-Box Optimization Benchmark (BBOB) testbed and a real-world
transfer learning task from the automobile industry. The results underscore the
significant advantages of the approach, revealing that the transferred
surrogate significantly outperforms both the original surrogate and the one
built from scratch using the transfer dataset, particularly in data-scarce
scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unfaithful Probability Distributions in Binary Triple of Causality
  Directed Acyclic Graph 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18337v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18337v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingwei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Faithfulness is the foundation of probability distribution and graph in
causal discovery and causal inference. In this paper, several unfaithful
probability distribution examples are constructed in three--vertices binary
causality directed acyclic graph (DAG) structure, which are not faithful to
causal DAGs described in J.M.,Robins,et al. Uniform consistency in causal
inference. Biometrika (2003),90(3): 491--515. And the general unfaithful
probability distribution with multiple independence and conditional
independence in binary triple causal DAG is given.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stream-Based Monitoring of Algorithmic Fairness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18331v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18331v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan Baumeister, Bernd Finkbeiner, Frederik Scheerer, Julian Siber, Tobias Wagenpfeil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic decision and prediction systems are increasingly deployed in
applications where they significantly impact the livelihood of people, such as
for predicting the creditworthiness of loan applicants or the recidivism risk
of defendants. These applications have given rise to a new class of
algorithmic-fairness specifications that require the systems to decide and
predict without bias against social groups. Verifying these specifications
statically is often out of reach for realistic systems, since the systems may,
e.g., employ complex learning components, and reason over a large input space.
In this paper, we therefore propose stream-based monitoring as a solution for
verifying the algorithmic fairness of decision and prediction systems at
runtime. Concretely, we present a principled way to formalize algorithmic
fairness over temporal data streams in the specification language RTLola and
demonstrate the efficacy of this approach on a number of benchmarks. Besides
synthetic scenarios that particularly highlight its efficiency on streams with
a scaling amount of data, we notably evaluate the monitor on real-world data
from the recidivism prediction tool COMPAS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31st International Conference on Tools and Algorithms for the
  Construction and Analysis of Systems (TACAS 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Unified Perspective on the Dynamics of Deep <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18322v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18322v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valérie Castin, Pierre Ablin, José Antonio Carrillo, Gabriel Peyré
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers, which are state-of-the-art in most machine learning tasks,
represent the data as sequences of vectors called tokens. This representation
is then exploited by the attention function, which learns dependencies between
tokens and is key to the success of Transformers. However, the iterative
application of attention across layers induces complex dynamics that remain to
be fully understood. To analyze these dynamics, we identify each input sequence
with a probability measure and model its evolution as a Vlasov equation called
Transformer PDE, whose velocity field is non-linear in the probability measure.
Our first set of contributions focuses on compactly supported initial data. We
show the Transformer PDE is well-posed and is the mean-field limit of an
interacting particle system, thus generalizing and extending previous analysis
to several variants of self-attention: multi-head attention, L2 attention,
Sinkhorn attention, Sigmoid attention, and masked attention--leveraging a
conditional Wasserstein framework. In a second set of contributions, we are the
first to study non-compactly supported initial conditions, by focusing on
Gaussian initial data. Again for different types of attention, we show that the
Transformer PDE preserves the space of Gaussian measures, which allows us to
analyze the Gaussian case theoretically and numerically to identify typical
behaviors. This Gaussian analysis captures the evolution of data anisotropy
through a deep Transformer. In particular, we highlight a clustering phenomenon
that parallels previous results in the non-normalized discrete case.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Neural Theorem Proving via Fine-grained Proof Structure
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18310v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18310v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoxiong Liu, Jiacheng Sun, Zhenguo Li, Andrew C Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The synergy between deep learning models and traditional automation tools
plays a pivotal role in developing robust neural theorem provers (NTPs).
However, for proof synthesis with LLMs, previous work applies automation tools
either only when the model explicitly calls the method, or only at a single
granularity level, failing to fully exploit the power of built-in tactics and
off-the-shelf automated theorem provers. In this work, we propose ProofAug, a
novel theorem proving method that enjoys superior sample efficiency through
equipping proof-generation LLMs with automation methods in different
granularities via fine-grained structure analysis of model-generated proof
proposals. Furthermore, ProofAug serves as a versatile plug-and-play module
that seamlessly integrates with any tree-search algorithm, enabling our
construction of an efficient recursive proving (ERP) module to further enhance
performance. The superiority of our method is validated on the miniF2F-test
benchmark using the open-source deepseek-math-7b-base model and the Isabelle
proof assistant. Notably, by additionally employing a mixed prompting strategy,
we achieve a cumulative pass rate of 66.0% after curation of the dataset (61.9%
for the original version), setting a new SOTA across all proof languages with a
total sample budget of only 2100. Our code is available at
https://github.com/haoxiongliu/ProofAug.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Update Estimation and Scheduling for Over-the-Air Federated Learning
  with Energy Harvesting Devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18298v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18298v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Furkan Bagci, Busra Tegin, Mohammad Kazemi, Tolga M. Duman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study over-the-air (OTA) federated learning (FL) for energy harvesting
devices with heterogeneous data distribution over wireless fading multiple
access channel (MAC). To address the impact of low energy arrivals and data
heterogeneity on global learning, we propose user scheduling strategies.
Specifically, we develop two approaches: 1) entropy-based scheduling for known
data distributions and 2) least-squares-based user representation estimation
for scheduling with unknown data distributions at the parameter server. Both
methods aim to select diverse users, mitigating bias and enhancing convergence.
Numerical and analytical results demonstrate improved learning performance by
reducing redundancy and conserving energy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Random Feature Representation Boosting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18283v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18283v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikita Zozoulenko, Thomas Cass, Lukas Gonon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Random Feature Representation Boosting (RFRBoost), a novel
method for constructing deep residual random feature neural networks (RFNNs)
using boosting theory. RFRBoost uses random features at each layer to learn the
functional gradient of the network representation, enhancing performance while
preserving the convex optimization benefits of RFNNs. In the case of MSE loss,
we obtain closed-form solutions to greedy layer-wise boosting with random
features. For general loss functions, we show that fitting random feature
residual blocks reduces to solving a quadratically constrained least squares
problem. We demonstrate, through numerical experiments on 91 tabular datasets
for regression and classification, that RFRBoost significantly outperforms
traditional RFNNs and end-to-end trained MLP ResNets, while offering
substantial computational advantages and theoretical guarantees stemming from
boosting theory.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Sparsity for Sample-Efficient Preference Learning: A
  Theoretical Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18282v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18282v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunzhen Yao, Lie He, Michael Gastpar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper considers the sample-efficiency of preference learning, which
models and predicts human choices based on comparative judgments. The minimax
optimal estimation rate $\Theta(d/n)$ in traditional estimation theory requires
that the number of samples $n$ scales linearly with the dimensionality of the
feature space $d$. However, the high dimensionality of the feature space and
the high cost of collecting human-annotated data challenge the efficiency of
traditional estimation methods. To remedy this, we leverage sparsity in the
preference model and establish sharp estimation rates. We show that under the
sparse random utility model, where the parameter of the reward function is
$k$-sparse, the minimax optimal rate can be reduced to $\Theta(k/n \log(d/k))$.
Furthermore, we analyze the $\ell_{1}$-regularized estimator and show that it
achieves near-optimal rate under mild assumptions on the Gram matrix.
Experiments on synthetic data and LLM alignment data validate our theoretical
findings, showing that sparsity-aware methods significantly reduce sample
complexity and improve prediction accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Jailbreaking LLMs' Safeguard with Universal Magic Words for Text
  Embedding Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18280v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18280v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyu Liang, Youran Sun, Yunfeng Cai, Jun Zhu, Bo Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The security issue of large language models (LLMs) has gained significant
attention recently, with various defense mechanisms developed to prevent
harmful outputs, among which safeguards based on text embedding models serve as
a fundamental defense. Through testing, we discover that the distribution of
text embedding model outputs is significantly biased with a large mean.
Inspired by this observation, we propose novel efficient methods to search for
universal magic words that can attack text embedding models. The universal
magic words as suffixes can move the embedding of any text towards the bias
direction, therefore manipulate the similarity of any text pair and mislead
safeguards. By appending magic words to user prompts and requiring LLMs to end
answers with magic words, attackers can jailbreak the safeguard. To eradicate
this security risk, we also propose defense mechanisms against such attacks,
which can correct the biased distribution of text embeddings in a train-free
manner.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReactEmbed: A Cross-Domain Framework for Protein-Molecule Representation
  Learning via Biochemical Reaction Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18278v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18278v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amitay Sicherman, Kira Radinsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The challenge in computational biology and drug discovery lies in creating
comprehensive representations of proteins and molecules that capture their
intrinsic properties and interactions. Traditional methods often focus on
unimodal data, such as protein sequences or molecular structures, limiting
their ability to capture complex biochemical relationships. This work enhances
these representations by integrating biochemical reactions encompassing
interactions between molecules and proteins. By leveraging reaction data
alongside pre-trained embeddings from state-of-the-art protein and molecule
models, we develop ReactEmbed, a novel method that creates a unified embedding
space through contrastive learning. We evaluate ReactEmbed across diverse
tasks, including drug-target interaction, protein-protein interaction, protein
property prediction, and molecular property prediction, consistently surpassing
all current state-of-the-art models. Notably, we showcase ReactEmbed's
practical utility through successful implementation in lipid nanoparticle-based
drug delivery, enabling zero-shot prediction of blood-brain barrier
permeability for protein-nanoparticle complexes. The code and comprehensive
database of reaction pairs are available for open use at
\href{https://github.com/amitaysicherman/ReactEmbed}{GitHub}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sebra: Debiasing Through Self-Guided Bias Ranking <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18277v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18277v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adarsh Kappiyath, Abhra Chaudhuri, Ajay Jaiswal, Ziquan Liu, Yunpeng Li, Xiatian Zhu, Lu Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ranking samples by fine-grained estimates of spuriosity (the degree to which
spurious cues are present) has recently been shown to significantly benefit
bias mitigation, over the traditional binary biased-\textit{vs}-unbiased
partitioning of train sets. However, this spuriosity ranking comes with the
requirement of human supervision. In this paper, we propose a debiasing
framework based on our novel \ul{Se}lf-Guided \ul{B}ias \ul{Ra}nking
(\emph{Sebra}), that mitigates biases (spurious correlations) via an automatic
ranking of data points by spuriosity within their respective classes. Sebra
leverages a key local symmetry in Empirical Risk Minimization (ERM) training --
the ease of learning a sample via ERM inversely correlates with its
spuriousity; the fewer spurious correlations a sample exhibits, the harder it
is to learn, and vice versa. However, globally across iterations, ERM tends to
deviate from this symmetry. Sebra dynamically steers ERM to correct this
deviation, facilitating the sequential learning of attributes in increasing
order of difficulty, \ie, decreasing order of spuriosity. As a result, the
sequence in which Sebra learns samples naturally provides spuriousity rankings.
We use the resulting fine-grained bias characterization in a contrastive
learning framework to mitigate biases from multiple sources. Extensive
experiments show that Sebra consistently outperforms previous state-of-the-art
unsupervised debiasing techniques across multiple standard benchmarks,
including UrbanCars, BAR, CelebA, and ImageNet-1K. Code, pre-trained models,
and training logs are available at https://kadarsh22.github.io/sebra_iclr25/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Pre-Train</span>ed Vision-Language Model Selection and Reuse for Downstream
  Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18271v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18271v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao-Zhe Tan, Zhi Zhou, Lan-Zhe Guo, Yu-Feng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained Vision-Language Models (VLMs) are becoming increasingly popular
across various visual tasks, and several open-sourced VLM variants have been
released. However, selecting the best-performing pre-trained VLM for a specific
downstream task is challenging since no single VLM can achieve promising
performance on all downstream tasks, and evaluating all available VLMs is
impossible due to time and data limitations. To address this problem, this
paper proposes a novel paradigm to select and reuse VLM for downstream tasks,
called Model Label Learning (MLL). The proposal contains three key modules:
\emph{model labeling}, which assigns labels to each VLM to describe their
specialty and utility; \emph{model selection}, which matches the requirements
of the target task with model labels; and \emph{model reuse}, which applies
selected VLMs to the target task in an ensemble manner. The proposal is highly
computationally efficient and growable since the model labeling process is
completed target task independent and the ability could grow with the number of
candidate VLMs. We also introduce a new benchmark for evaluating VLM selection
methods, including 49 VLMs and 17 target task datasets. Experimental results
clearly demonstrate the effectiveness of the proposed method for selecting and
reusing VLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reducing Aleatoric and Epistemic Uncertainty through Multi-modal Data
  Acquisition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18268v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18268v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arthur Hoarau, Benjamin Quost, Sébastien Destercke, Willem Waegeman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To generate accurate and reliable predictions, modern AI systems need to
combine data from multiple modalities, such as text, images, audio,
spreadsheets, and time series. Multi-modal data introduces new opportunities
and challenges for disentangling uncertainty: it is commonly assumed in the
machine learning community that epistemic uncertainty can be reduced by
collecting more data, while aleatoric uncertainty is irreducible. However, this
assumption is challenged in modern AI systems when information is obtained from
different modalities. This paper introduces an innovative data acquisition
framework where uncertainty disentanglement leads to actionable decisions,
allowing sampling in two directions: sample size and data modality. The main
hypothesis is that aleatoric uncertainty decreases as the number of modalities
increases, while epistemic uncertainty decreases by collecting more
observations. We provide proof-of-concept implementations on two multi-modal
datasets to showcase our data acquisition framework, which combines ideas from
active learning, active feature acquisition and uncertainty quantification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PDE-DKL: PDE-constrained deep kernel learning in high dimensionality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18258v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18258v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weihao Yan, Christoph Brune, Mengwu Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many physics-informed machine learning methods for PDE-based problems rely on
Gaussian processes (GPs) or neural networks (NNs). However, both face
limitations when data are scarce and the dimensionality is high. Although GPs
are known for their robust uncertainty quantification in low-dimensional
settings, their computational complexity becomes prohibitive as the
dimensionality increases. In contrast, while conventional NNs can accommodate
high-dimensional input, they often require extensive training data and do not
offer uncertainty quantification. To address these challenges, we propose a
PDE-constrained Deep Kernel Learning (PDE-DKL) framework that combines DL and
GPs under explicit PDE constraints. Specifically, NNs learn a low-dimensional
latent representation of the high-dimensional PDE problem, reducing the
complexity of the problem. GPs then perform kernel regression subject to the
governing PDEs, ensuring accurate solutions and principled uncertainty
quantification, even when available data are limited. This synergy unifies the
strengths of both NNs and GPs, yielding high accuracy, robust uncertainty
estimates, and computational efficiency for high-dimensional PDEs. Numerical
experiments demonstrate that PDE-DKL achieves high accuracy with reduced data
requirements. They highlight its potential as a practical, reliable, and
scalable solver for complex PDE-based applications in science and engineering.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Statistical multi-metric evaluation and visualization of LLM system
  predictive performance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18243v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18243v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Ackerman, Eitan Farchi, Orna Raz, Assaf Toledo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The evaluation of generative or discriminative large language model
(LLM)-based systems is often a complex multi-dimensional problem. Typically, a
set of system configuration alternatives are evaluated on one or more benchmark
datasets, each with one or more evaluation metrics, which may differ between
datasets. We often want to evaluate -- with a statistical measure of
significance -- whether systems perform differently either on a given dataset
according to a single metric, on aggregate across metrics on a dataset, or
across datasets. Such evaluations can be done to support decision-making, such
as deciding whether a particular system component change (e.g., choice of LLM
or hyperparameter values) significantly improves performance over the current
system configuration, or, more generally, whether a fixed set of system
configurations (e.g., a leaderboard list) have significantly different
performances according to metrics of interest. We present a framework
implementation that automatically performs the correct statistical tests,
properly aggregates the statistical results across metrics and datasets (a
nontrivial task), and can visualize the results. The framework is demonstrated
on the multi-lingual code generation benchmark CrossCodeEval, for several
state-of-the-art LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Large Protein Language Models in Constrained Evaluation
  Scenarios within the FLIP Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18223v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18223v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manuel F. Mollon, Joaquin Gonzalez-Rodriguez, Alicia Lozano-Diez, Daniel Ramos, Doroteo T. Toledano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we expand upon the FLIP benchmark-designed for evaluating
protein fitness prediction models in small, specialized prediction tasks-by
assessing the performance of state-of-the-art large protein language models,
including ESM-2 and SaProt on the FLIP dataset. Unlike larger, more diverse
benchmarks such as ProteinGym, which cover a broad spectrum of tasks, FLIP
focuses on constrained settings where data availability is limited. This makes
it an ideal framework to evaluate model performance in scenarios with scarce
task-specific data. We investigate whether recent advances in protein language
models lead to significant improvements in such settings. Our findings provide
valuable insights into the performance of large-scale models in specialized
protein prediction tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting $Ψ$DONet: microlocally inspired filters for
  incomplete-data tomographic reconstructions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18219v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18219v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tatiana A. Bubba, Luca Ratti, Andrea Sebastiani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we revisit a supervised learning approach based on unrolling,
known as $\Psi$DONet, by providing a deeper microlocal interpretation for its
theoretical analysis, and extending its study to the case of sparse-angle
tomography. Furthermore, we refine the implementation of the original
$\Psi$DONet considering special filters whose structure is specifically
inspired by the streak artifact singularities characterizing tomographic
reconstructions from incomplete data. This allows to considerably lower the
number of (learnable) parameters while preserving (or even slightly improving)
the same quality for the reconstructions from limited-angle data and providing
a proof-of-concept for the case of sparse-angle tomographic data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HKAN: Hierarchical Kolmogorov-Arnold Network without Backpropagation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18199v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18199v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Grzegorz Dudek, Tomasz Rodak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces the Hierarchical Kolmogorov-Arnold Network (HKAN), a
novel network architecture that offers a competitive alternative to the
recently proposed Kolmogorov-Arnold Network (KAN). Unlike KAN, which relies on
backpropagation, HKAN adopts a randomized learning approach, where the
parameters of its basis functions are fixed, and linear aggregations are
optimized using least-squares regression. HKAN utilizes a hierarchical
multi-stacking framework, with each layer refining the predictions from the
previous one by solving a series of linear regression problems. This
non-iterative training method simplifies computation and eliminates sensitivity
to local minima in the loss function. Empirical results show that HKAN delivers
comparable, if not superior, accuracy and stability relative to KAN across
various regression tasks, while also providing insights into variable
importance. The proposed approach seamlessly integrates theoretical insights
with practical applications, presenting a robust and efficient alternative for
neural network modeling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fundamental Challenges in Evaluating Text2SQL Solutions and Detecting
  Their Limitations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18197v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18197v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cedric Renggli, Ihab F. Ilyas, Theodoros Rekatsinas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we dive into the fundamental challenges of evaluating Text2SQL
solutions and highlight potential failure causes and the potential risks of
relying on aggregate metrics in existing benchmarks. We identify two largely
unaddressed limitations in current open benchmarks: (1) data quality issues in
the evaluation data, mainly attributed to the lack of capturing the
probabilistic nature of translating a natural language description into a
structured query (e.g., NL ambiguity), and (2) the bias introduced by using
different match functions as approximations for SQL equivalence.
  To put both limitations into context, we propose a unified taxonomy of all
Text2SQL limitations that can lead to both prediction and evaluation errors. We
then motivate the taxonomy by providing a survey of Text2SQL limitations using
state-of-the-art Text2SQL solutions and benchmarks. We describe the causes of
limitations with real-world examples and propose potential mitigation solutions
for each category in the taxonomy. We conclude by highlighting the open
challenges encountered when deploying such mitigation strategies or attempting
to automatically apply the taxonomy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GDformer: Going Beyond Subsequence Isolation for Multivariate Time
  Series Anomaly Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18196v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18196v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingxiang Liu, Chenghao Liu, Sheng Sun, Di Yao, Yuxuan Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised anomaly detection of multivariate time series is a challenging
task, given the requirements of deriving a compact detection criterion without
accessing the anomaly points. The existing methods are mainly based on
reconstruction error or association divergence, which are both confined to
isolated subsequences with limited horizons, hardly promising unified
series-level criterion. In this paper, we propose the Global
Dictionary-enhanced Transformer (GDformer) with a renovated dictionary-based
cross attention mechanism to cultivate the global representations shared by all
normal points in the entire series. Accordingly, the cross-attention maps
reflect the correlation weights between the point and global representations,
which naturally leads to the representation-wise similarity-based detection
criterion. To foster more compact detection boundary, prototypes are introduced
to capture the distribution of normal point-global correlation weights.
GDformer consistently achieves state-of-the-art unsupervised anomaly detection
performance on five real-world benchmark datasets. Further experiments validate
the global dictionary has great transferability among various datasets. The
code is available at https://github.com/yuppielqx/GDformer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Machine Learning Fairness for Depression Detection using EEG Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18192v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18192v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Angus Man Ho Kwok, Jiaee Cheong, Sinan Kalkan, Hatice Gunes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the very first attempt to evaluate machine learning
fairness for depression detection using electroencephalogram (EEG) data. We
conduct experiments using different deep learning architectures such as
Convolutional Neural Networks (CNN), Long Short-Term Memory (LSTM) networks,
and Gated Recurrent Unit (GRU) networks across three EEG datasets: Mumtaz,
MODMA and Rest. We employ five different bias mitigation strategies at the
pre-, in- and post-processing stages and evaluate their effectiveness. Our
experimental results show that bias exists in existing EEG datasets and
algorithms for depression detection, and different bias mitigation methods
address bias at different levels across different fairness measures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear as part of the International Symposium on Biomedical
  Imaging (ISBI) 2025 proceedings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Network Modeling of Microstructure Complexity Using Digital
  Libraries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18189v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18189v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingjie Zhao, Zhiping Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Microstructure evolution in matter is often modeled numerically using field
or level-set solvers, mirroring the dual representation of spatiotemporal
complexity in terms of pixel or voxel data, and geometrical forms in vector
graphics. Motivated by this analog, as well as the structural and event-driven
nature of artificial and spiking neural networks, respectively, we evaluate
their performance in learning and predicting fatigue crack growth and Turing
pattern development. Predictions are made based on digital libraries
constructed from computer simulations, which can be replaced by experimental
data to lift the mathematical overconstraints of physics. Our assessment
suggests that the leaky integrate-and-fire neuron model offers superior
predictive accuracy with fewer parameters and less memory usage, alleviating
the accuracy-cost tradeoff in contrast to the common practices in computer
vision tasks. Examination of network architectures shows that these benefits
arise from its reduced weight range and sparser connections. The study
highlights the capability of event-driven models in tackling problems with
evolutionary bulk-phase and interface behaviors using the digital library
approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ In-Context Learning of Polynomial Kernel Regression in <span class="highlight-title">Transformer</span>s with
  GLU Layers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18187v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18187v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyuan Sun, Ali Jadbabaie, Navid Azizan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based models have demonstrated remarkable ability in in-context
learning (ICL), where they can adapt to unseen tasks from a prompt with a few
examples, without requiring parameter updates. Recent research has provided
insight into how linear Transformers can perform ICL by implementing gradient
descent estimators. In particular, it has been shown that the optimal linear
self-attention (LSA) mechanism can implement one step of gradient descent with
respect to a linear least-squares objective when trained on random linear
regression tasks.
  However, the theoretical understanding of ICL for nonlinear function classes
remains limited. In this work, we address this gap by first showing that LSA is
inherently restricted to solving linear least-squares objectives and thus, the
solutions in prior works cannot readily extend to nonlinear ICL tasks. To
overcome this limitation, drawing inspiration from modern architectures, we
study a mechanism that combines LSA with GLU-like feed-forward layers and show
that this allows the model to perform one step of gradient descent on a
polynomial kernel regression. Further, we characterize the scaling behavior of
the resulting Transformer model, highlighting the necessary model size to
effectively handle quadratic ICL tasks. Our findings highlight the distinct
roles of attention and feed-forward layers in nonlinear ICL and identify key
challenges when extending ICL to nonlinear function classes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Genetic Algorithm with Border Trades (GAB) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18184v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18184v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingchuan Lyu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel approach to improving Genetic Algorithms (GA)
in large or complex problem spaces by incorporating new chromosome patterns in
the breeding process through border trade activities. These strategies increase
chromosome diversity, preventing premature convergence and enhancing the GA's
ability to explore the solution space more effectively. Empirical evidence
demonstrates significant improvements in convergence behavior. This approach
offers a promising pathway to addressing challenges in optimizing large or
complex problem domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decentralized Projection-free Online Upper-Linearizable Optimization
  with Applications to DR-Submodular Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18183v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18183v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiyang Lu, Mohammad Pedramfar, Vaneet Aggarwal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel framework for decentralized projection-free
optimization, extending projection-free methods to a broader class of
upper-linearizable functions. Our approach leverages decentralized optimization
techniques with the flexibility of upper-linearizable function frameworks,
effectively generalizing traditional DR-submodular function optimization. We
obtain the regret of $O(T^{1-\theta/2})$ with communication complexity of
$O(T^{\theta})$ and number of linear optimization oracle calls of
$O(T^{2\theta})$ for decentralized upper-linearizable function optimization,
for any $0\le \theta \le 1$. This approach allows for the first results for
monotone up-concave optimization with general convex constraints and
non-monotone up-concave optimization with general convex constraints. Further,
the above results for first order feedback are extended to zeroth order,
semi-bandit, and bandit feedback.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Estimating Multi-chirp Parameters using Curvature-guided Langevin Monte
  Carlo 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18178v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18178v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sattwik Basu, Debottam Dutta, Yu-Lin Wei, Romit Roy Choudhury
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper considers the problem of estimating chirp parameters from a noisy
mixture of chirps. While a rich body of work exists in this area, challenges
remain when extending these techniques to chirps of higher order polynomials.
We formulate this as a non-convex optimization problem and propose a modified
Langevin Monte Carlo (LMC) sampler that exploits the average curvature of the
objective function to reliably find the minimizer. Results show that our
Curvature-guided LMC (CG-LMC) algorithm is robust and succeeds even in low SNR
regimes, making it viable for practical applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advancing Personalized Federated Learning: Integrative Approaches with
  AI for Enhanced Privacy and Customization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18174v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18174v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Cooper, Michael Geller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the age of data-driven decision making, preserving privacy while providing
personalized experiences has become paramount. Personalized Federated Learning
(PFL) offers a promising framework by decentralizing the learning process, thus
ensuring data privacy and reducing reliance on centralized data repositories.
However, the integration of advanced Artificial Intelligence (AI) techniques
within PFL remains underexplored. This paper proposes a novel approach that
enhances PFL with cutting-edge AI methodologies including adaptive
optimization, transfer learning, and differential privacy. We present a model
that not only boosts the performance of individual client models but also
ensures robust privacy-preserving mechanisms and efficient resource utilization
across heterogeneous networks. Empirical results demonstrate significant
improvements in model accuracy and personalization, along with stringent
privacy adherence, as compared to conventional federated learning models. This
work paves the way for a new era of truly personalized and privacy-conscious AI
systems, offering significant implications for industries requiring compliance
with stringent data protection regulations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2501.16758</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Continually Evolved Multimodal Foundation Models for Cancer Prognosis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18170v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18170v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Peng, Shuang Zhou, Longwei Yang, Yiran Song, Mohan Zhang, Kaixiong Zhou, Feng Xie, Mingquan Lin, Rui Zhang, Tianlong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cancer prognosis is a critical task that involves predicting patient outcomes
and survival rates. To enhance prediction accuracy, previous studies have
integrated diverse data modalities, such as clinical notes, medical images, and
genomic data, leveraging their complementary information. However, existing
approaches face two major limitations. First, they struggle to incorporate
newly arrived data with varying distributions into training, such as patient
records from different hospitals, thus rendering sub-optimal generalizability
and limited utility in real-world applications. Second, most multimodal
integration methods rely on simplistic concatenation or task-specific
pipelines, which fail to capture the complex interdependencies across
modalities. To address these, we propose a continually evolving multi-modal
foundation model. Extensive experiments on the TCGA dataset demonstrate the
effectiveness of our approach, highlighting its potential to advance cancer
prognosis by enabling robust and adaptive multimodal integration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Faster Convergence of Riemannian Stochastic Gradient Descent with
  Increasing Batch Size 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18164v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18164v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kanata Oowada, Hideaki Iiduka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many models used in machine learning have become so large that even computer
computation of the full gradient of the loss function is impractical. This has
made it necessary to efficiently train models using limited available
information, such as batch size and learning rate. We have theoretically
analyzed the use of Riemannian stochastic gradient descent (RSGD) and found
that using an increasing batch size leads to faster RSGD convergence than using
a constant batch size not only with a constant learning rate but also with a
decaying learning rate, such as cosine annealing decay and polynomial decay. In
particular, RSGD has a better convergence rate $O(\frac{1}{\sqrt{T}})$ than the
existing rate $O(\frac{\sqrt{\log T}}{\sqrt[4]{T}})$ with a diminishing
learning rate, where $T$ is the number of iterations. The results of
experiments on principal component analysis and low-rank matrix completion
problems confirmed that, except for the MovieLens dataset and a constant
learning rate, using a polynomial growth batch size or an exponential growth
batch size results in better performance than using a constant batch size.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models for Cryptocurrency Transaction Analysis: A Bitcoin
  Case Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18158v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18158v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuchen Lei, Yuexin Xiang, Qin Wang, Rafael Dowsley, Tsz Hon Yuen, Jiangshan Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cryptocurrencies are widely used, yet current methods for analyzing
transactions heavily rely on opaque, black-box models. These lack
interpretability and adaptability, failing to effectively capture behavioral
patterns. Many researchers, including us, believe that Large Language Models
(LLMs) could bridge this gap due to their robust reasoning abilities for
complex tasks. In this paper, we test this hypothesis by applying LLMs to
real-world cryptocurrency transaction graphs, specifically within the Bitcoin
network. We introduce a three-tiered framework to assess LLM capabilities:
foundational metrics, characteristic overview, and contextual interpretation.
This includes a new, human-readable graph representation format, LLM4TG, and a
connectivity-enhanced sampling algorithm, CETraS, which simplifies larger
transaction graphs. Experimental results show that LLMs excel at foundational
metrics and offer detailed characteristic overviews. Their effectiveness in
contextual interpretation suggests they can provide useful explanations of
transaction behaviors, even with limited labeled data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dual-Bounded Nonlinear Optimal Transport for Size Constrained Min Cut
  Clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18143v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18143v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangyuan Xie, Jinghui Yuan, Feiping Nie, Xuelong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Min cut is an important graph partitioning method. However, current solutions
to the min cut problem suffer from slow speeds, difficulty in solving, and
often converge to simple solutions. To address these issues, we relax the min
cut problem into a dual-bounded constraint and, for the first time, treat the
min cut problem as a dual-bounded nonlinear optimal transport problem.
Additionally, we develop a method for solving dual-bounded nonlinear optimal
transport based on the Frank-Wolfe method (abbreviated as DNF). Notably, DNF
not only solves the size constrained min cut problem but is also applicable to
all dual-bounded nonlinear optimal transport problems. We prove that for convex
problems satisfying Lipschitz smoothness, the DNF method can achieve a
convergence rate of \(\mathcal{O}(\frac{1}{t})\). We apply the DNF method to
the min cut problem and find that it achieves state-of-the-art performance in
terms of both the loss function and clustering accuracy at the fastest speed,
with a convergence rate of \(\mathcal{O}(\frac{1}{\sqrt{t}})\). Moreover, the
DNF method for the size constrained min cut problem requires no parameters and
exhibits better stability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ B3C: A Minimalist Approach to Offline Multi-Agent Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18138v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18138v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Woojun Kim, Katia Sycara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Overestimation arising from selecting unseen actions during policy evaluation
is a major challenge in offline reinforcement learning (RL). A minimalist
approach in the single-agent setting -- adding behavior cloning (BC)
regularization to existing online RL algorithms -- has been shown to be
effective; however, this approach is understudied in multi-agent settings. In
particular, overestimation becomes worse in multi-agent settings due to the
presence of multiple actions, resulting in the BC regularization-based approach
easily suffering from either over-regularization or critic divergence. To
address this, we propose a simple yet effective method, Behavior Cloning
regularization with Critic Clipping (B3C), which clips the target critic value
in policy evaluation based on the maximum return in the dataset and pushes the
limit of the weight on the RL objective over BC regularization, thereby
improving performance. Additionally, we leverage existing value factorization
techniques, particularly non-linear factorization, which is understudied in
offline settings. Integrated with non-linear value factorization, B3C
outperforms state-of-the-art algorithms on various offline multi-agent
benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tensor Completion for Surrogate Modeling of Material Property Prediction <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18137v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18137v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaan Pakala, Dawon Ahn, Evangelos Papalexakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When designing materials to optimize certain properties, there are often many
possible configurations of designs that need to be explored. For example, the
materials' composition of elements will affect properties such as strength or
conductivity, which are necessary to know when developing new materials.
Exploring all combinations of elements to find optimal materials becomes very
time consuming, especially when there are more design variables. For this
reason, there is growing interest in using machine learning (ML) to predict a
material's properties. In this work, we model the optimization of certain
material properties as a tensor completion problem, to leverage the structure
of our datasets and navigate the vast number of combinations of material
configurations. Across a variety of material property prediction tasks, our
experiments show tensor completion methods achieving 10-20% decreased error
compared with baseline ML models such as GradientBoosting and Multilayer
Perceptron (MLP), while maintaining similar training speed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2 page paper accepted to AAAI KGML 2025 bridge program</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HyperZero: A Customized End-to-End Auto-Tuning System for Recommendation
  with Hourly Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18126v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18126v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xufeng Cai, Ziwei Guan, Lei Yuan, Ali Selman Aydin, Tengyu Xu, Boying Liu, Wenbo Ren, Renkai Xiang, Songyi He, Haichuan Yang, Serena Li, Mingze Gao, Yue Weng, Ji Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern recommendation systems can be broadly divided into two key stages: the
ranking stage, where the system predicts various user engagements (e.g.,
click-through rate, like rate, follow rate, watch time), and the value model
stage, which aggregates these predictive scores through a function (e.g., a
linear combination defined by a weight vector) to measure the value of each
content by a single numerical score. Both stages play roughly equally important
roles in real industrial systems; however, how to optimize the model weights
for the second stage still lacks systematic study. This paper focuses on
optimizing the second stage through auto-tuning technology. Although general
auto-tuning systems and solutions - both from established production practices
and open-source solutions - can address this problem, they typically require
weeks or even months to identify a feasible solution. Such prolonged tuning
processes are unacceptable in production environments for recommendation
systems, as suboptimal value models can severely degrade user experience. An
effective auto-tuning solution is required to identify a viable model within
2-3 days, rather than the extended timelines typically associated with existing
approaches. In this paper, we introduce a practical auto-tuning system named
HyperZero that addresses these time constraints while effectively solving the
unique challenges inherent in modern recommendation systems. Moreover, this
framework has the potential to be expanded to broader tuning tasks within
recommendation systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Battery State of Health Estimation Using LLM Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18123v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18123v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aybars Yunusoglu, Dexter Le, Karn Tiwari, Murat Isik, I. Can Dikmen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Battery health monitoring is critical for the efficient and reliable
operation of electric vehicles (EVs). This study introduces a transformer-based
framework for estimating the State of Health (SoH) and predicting the Remaining
Useful Life (RUL) of lithium titanate (LTO) battery cells by utilizing both
cycle-based and instantaneous discharge data. Testing on eight LTO cells under
various cycling conditions over 500 cycles, we demonstrate the impact of charge
durations on energy storage trends and apply Differential Voltage Analysis
(DVA) to monitor capacity changes (dQ/dV) across voltage ranges. Our LLM model
achieves superior performance, with a Mean Absolute Error (MAE) as low as
0.87\% and varied latency metrics that support efficient processing,
demonstrating its strong potential for real-time integration into EVs. The
framework effectively identifies early signs of degradation through anomaly
detection in high-resolution data, facilitating predictive maintenance to
prevent sudden battery failures and enhance energy efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at The 26th International Symposium on Quality Electronic
  Design (ISQED'25)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VQLTI: Long-Term Tropical Cyclone Intensity Forecasting with Physical
  Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18122v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18122v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Wang, Lei Liu, Kang Chen, Tao Han, Bin Li, Lei Bai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tropical cyclone (TC) intensity forecasting is crucial for early disaster
warning and emergency decision-making. Numerous researchers have explored
deep-learning methods to address computational and post-processing issues in
operational forecasting. Regrettably, they exhibit subpar long-term forecasting
capabilities. We use two strategies to enhance long-term forecasting. (1) By
enhancing the matching between TC intensity and spatial information, we can
improve long-term forecasting performance. (2) Incorporating physical knowledge
and physical constraints can help mitigate the accumulation of forecasting
errors. To achieve the above strategies, we propose the VQLTI framework. VQLTI
transfers the TC intensity information to a discrete latent space while
retaining the spatial information differences, using large-scale spatial
meteorological data as conditions. Furthermore, we leverage the forecast from
the weather prediction model FengWu to provide additional physical knowledge
for VQLTI. Additionally, we calculate the potential intensity (PI) to impose
physical constraints on the latent variables. In the global long-term TC
intensity forecasting, VQLTI achieves state-of-the-art results for the 24h to
120h, with the MSW (Maximum Sustained Wind) forecast error reduced by
35.65%-42.51% compared to ECMWF-IFS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal <span class="highlight-title">Survey</span> Design for Private Mean Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18121v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18121v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu-Wei Chen, Raghu Pasupathy, Jordan A. Awan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work identifies the first privacy-aware stratified sampling scheme that
minimizes the variance for general private mean estimation under the Laplace,
Discrete Laplace (DLap) and Truncated-Uniform-Laplace (TuLap) mechanisms within
the framework of differential privacy (DP). We view stratified sampling as a
subsampling operation, which amplifies the privacy guarantee; however, to have
the same final privacy guarantee for each group, different nominal privacy
budgets need to be used depending on the subsampling rate. Ignoring the effect
of DP, traditional stratified sampling strategies risk significant variance
inflation. We phrase our optimal survey design as an optimization problem,
where we determine the optimal subsampling sizes for each group with the goal
of minimizing the variance of the resulting estimator. We establish strong
convexity of the variance objective, propose an efficient algorithm to identify
the integer-optimal design, and offer insights on the structure of the optimal
design.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ More Expressive Attention with Negative Weights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07176v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07176v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ang Lv, Ruobing Xie, Shuaipeng Li, Jiayi Liao, Xingwu Sun, Zhanhui Kang, Di Wang, Rui Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel attention mechanism, named Cog Attention, that enables
attention weights to be negative for enhanced expressiveness, which stems from
two key factors: (1) Cog Attention enhances parameter flexibility. For example,
unlike traditional softmax attention heads that use a static output-value (OV)
matrix to delete or copy inputs that the heads attend to, Cog Attention
naturally learns to use the sign of dynamic query-key (QK) inner products to
represent these operations. This enables Cog Attention to perform multiple
operations simultaneously within a single head. Meanwhile, Cog Attention's OV
matrix can focus more on refinement or modification. (2) Cog Attention enhances
the model's robustness against representational collapse by preventing the
``over-squashing'' of earlier tokens into later positions. We develop
Transformer-like models which use Cog Attention as attention modules, including
decoder-only models at various scales for language modeling and U-ViT diffusion
models for image generation. Experiments show that models using Cog Attention
exhibit superior performance compared to those employing traditional softmax
attention modules. Our approach suggests a promising research direction for
rethinking and breaking the entrenched constraints of traditional softmax
attention, such as the requirement for non-negative weights.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Verify with Caution: The Pitfalls of Relying on Imperfect Factuality
  Metrics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14883v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14883v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ameya Godbole, Robin Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Improvements in large language models have led to increasing optimism that
they can serve as reliable evaluators of natural language generation outputs.
In this paper, we challenge this optimism by thoroughly re-evaluating five
state-of-the-art factuality metrics on a collection of 11 datasets for
summarization, retrieval-augmented generation, and question answering. We find
that these evaluators are inconsistent with each other and often misestimate
system-level performance, both of which can lead to a variety of pitfalls. We
further show that these metrics exhibit biases against highly paraphrased
outputs and outputs that draw upon faraway parts of the source documents. We
urge users of these factuality metrics to proceed with caution and manually
validate the reliability of these metrics in their domain of interest before
proceeding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v2: Added Acknowledgements to funding sources and advisors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ S-LoRA: Scalable Low-Rank Adaptation for Class Incremental Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13198v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13198v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yichen Wu, Hongming Piao, Long-Kai Huang, Renzhen Wang, Wanhua Li, Hanspeter Pfister, Deyu Meng, Kede Ma, Ying Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual Learning with foundation models has recently emerged as a promising
approach to harnessing the power of pre-trained models for sequential tasks.
Existing prompt-based methods generally use a gating mechanism to select
relevant prompts aligned with the test query for further processing. However,
the success of these methods largely depends on the precision of the gating
mechanism, which becomes less scalable with additional computational overhead
as tasks increases. To overcome these issues, we propose a Scalable Low-Rank
Adaptation (S-LoRA) method for CL (in particular class incremental learning),
which incrementally decouples the learning of the direction and magnitude of
LoRA parameters. S-LoRA supports efficient inference by employing the
last-stage trained model for direct testing without a gating process. Our
theoretical and empirical analysis demonstrates that S-LoRA tends to follow a
low-loss trajectory that converges to an overlapped low-loss region, resulting
in an excellent stability-plasticity trade-off in CL. Furthermore, based on our
findings, we develop variants of S-LoRA with further improved scalability.
Extensive experiments across multiple CL benchmarks and various foundation
models consistently validate the effectiveness of S-LoRA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A conditional gradient homotopy method with applications to Semidefinite
  Programming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.03101v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.03101v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pavel Dvurechensky, Gabriele Iommazzo, Shimrit Shtern, Mathias Staudigl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new homotopy-based conditional gradient method for solving
convex optimization problems with a large number of simple conic constraints.
Instances of this template naturally appear in semidefinite programming
problems arising as convex relaxations of combinatorial optimization problems.
Our method is a double-loop algorithm in which the conic constraint is treated
via a self-concordant barrier, and the inner loop employs a conditional
gradient algorithm to approximate the analytic central path, while the outer
loop updates the accuracy imposed on the temporal solution and the homotopy
parameter. Our theoretical iteration complexity is competitive when confronted
to state-of-the-art SDP solvers, with the decisive advantage of cheap
projection-free subroutines. Preliminary numerical experiments are provided for
illustrating the practical performance of the method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Largely revised and extended version. Submitted for Publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Density Matrix Emulation of Quantum Recurrent Neural Networks for
  Multivariate Time Series Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.20671v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.20671v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        José Daniel Viqueira, Daniel Faílde, Mariamo M. Juane, Andrés Gómez, David Mera
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantum Recurrent Neural Networks (QRNNs) are robust candidates for modelling
and predicting future values in multivariate time series. However, the
effective implementation of some QRNN models is limited by the need for
mid-circuit measurements. Those increase the requirements for quantum hardware,
which in the current NISQ era does not allow reliable computations. Emulation
arises as the main near-term alternative to explore the potential of QRNNs, but
existing quantum emulators are not dedicated to circuits with multiple
intermediate measurements. In this context, we design a specific emulation
method that relies on density matrix formalism. Using a compact tensor
notation, we provide the mathematical formulation of the operator-sum
representation involved. This allows us to show how the present and past
information from a time series is transmitted through the circuit, and how to
reduce the computational cost in every time step of the emulated network. In
addition, we derive the analytical gradient and the Hessian of the network
outputs with respect to its trainable parameters, which are needed when the
outputs have stochastic noise due to hardware errors and a finite number of
circuit shots (sampling). We finally test the presented methods using a
hardware-efficient ansatz and four diverse datasets that include univariate and
multivariate time series, with and without sampling noise. In addition, we
compare the model with other existing quantum and classical approaches. Our
results show how QRNNs can be trained with numerical and analytical gradients
to make accurate predictions of future values by capturing non-trivial patterns
of input series with different complexities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Temporal Preference Optimization for Long-Form Video Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13919v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13919v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Li, Xiaohan Wang, Yuhui Zhang, Zeyu Wang, Serena Yeung-Levy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite significant advancements in video large multimodal models
(video-LMMs), achieving effective temporal grounding in long-form videos
remains a challenge for existing models. To address this limitation, we propose
Temporal Preference Optimization (TPO), a novel post-training framework
designed to enhance the temporal grounding capabilities of video-LMMs through
preference learning. TPO adopts a self-training approach that enables models to
differentiate between well-grounded and less accurate temporal responses by
leveraging curated preference datasets at two granularities: localized temporal
grounding, which focuses on specific video segments, and comprehensive temporal
grounding, which captures extended temporal dependencies across entire video
sequences. By optimizing on these preference datasets, TPO significantly
enhances temporal understanding while reducing reliance on manually annotated
data. Extensive experiments on three long-form video understanding
benchmarks--LongVideoBench, MLVU, and Video-MME--demonstrate the effectiveness
of TPO across two state-of-the-art video-LMMs. Notably, LLaVA-Video-TPO
establishes itself as the leading 7B model on the Video-MME benchmark,
underscoring the potential of TPO as a scalable and efficient solution for
advancing temporal reasoning in long-form video understanding. Project page:
https://ruili33.github.io/tpo_website.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLaRA: Supercharging Robot Learning Data for Vision-Language Policy <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20095v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20095v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Li, Cristina Mata, Jongwoo Park, Kumara Kahatapitiya, Yoo Sung Jang, Jinghuan Shang, Kanchana Ranasinghe, Ryan Burgert, Mu Cai, Yong Jae Lee, Michael S. Ryoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Language Models (VLMs) have recently been leveraged to generate
robotic actions, forming Vision-Language-Action (VLA) models. However, directly
adapting a pretrained VLM for robotic control remains challenging, particularly
when constrained by a limited number of robot demonstrations. In this work, we
introduce LLaRA: Large Language and Robotics Assistant, a framework that
formulates robot action policy as visuo-textual conversations and enables an
efficient transfer of a pretrained VLM into a powerful VLA, motivated by the
success of visual instruction tuning in Computer Vision. First, we present an
automated pipeline to generate conversation-style instruction tuning data for
robots from existing behavior cloning datasets, aligning robotic actions with
image pixel coordinates. Further, we enhance this dataset in a self-supervised
manner by defining six auxiliary tasks, without requiring any additional action
annotations. We show that a VLM finetuned with a limited amount of such
datasets can produce meaningful action decisions for robotic control. Through
experiments across multiple simulated and real-world tasks, we demonstrate that
LLaRA achieves state-of-the-art performance while preserving the generalization
capabilities of large language models. The code, datasets, and pretrained
models are available at https://github.com/LostXine/LLaRA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fold Bifurcation Identification through Scientific Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.14210v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.14210v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giuseppe Habib, Ádám Horváth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study employs scientific machine learning to identify transient time
series of dynamical systems near a fold bifurcation of periodic solutions. The
unique aspect of this work is that a convolutional neural network (CNN) is
trained with a relatively small amount of data and on a single, very simple
system, yet it is tested on much more complicated systems. This task requires
strong generalization capabilities, which are achieved by incorporating
physics-based information. This information is provided through a specific
pre-processing of the input data, which includes transformation into polar
coordinates, normalization, transformation into the logarithmic scale, and
filtering through a moving mean. The results demonstrate that such data
pre-processing enables the CNN to grasp the important features related to
transient time-series near a fold bifurcation, namely, the trend of the
oscillation amplitude, and disregard other characteristics that are not
particularly relevant, such as the vibration frequency. The developed CNN was
able to correctly classify transient trajectories near a fold for a
mass-on-moving-belt system, a van der Pol-Duffing oscillator with an attached
tuned mass damper, and a pitch-and-plunge wing profile. The results contribute
to the progress towards the development of similar CNNs effective in real-life
applications such as safety monitoring of dynamical systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can Optimization Trajectories Explain Multi-Task Transfer? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.14677v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.14677v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Mueller, Mark Dredze, Nicholas Andrews
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the widespread adoption of multi-task training in deep learning,
little is understood about how multi-task learning (MTL) affects
generalization. Prior work has conjectured that the negative effects of MTL are
due to optimization challenges that arise during training, and many
optimization methods have been proposed to improve multi-task performance.
However, recent work has shown that these methods fail to consistently improve
multi-task generalization. In this work, we seek to improve our understanding
of these failures by empirically studying how MTL impacts the optimization of
tasks, and whether this impact can explain the effects of MTL on
generalization. We show that MTL results in a generalization gap (a gap in
generalization at comparable training loss) between single-task and multi-task
trajectories early into training. However, we find that factors of the
optimization trajectory previously proposed to explain generalization gaps in
single-task settings cannot explain the generalization gaps between single-task
and multi-task models. Moreover, we show that the amount of gradient conflict
between tasks is correlated with negative effects to task optimization, but is
not predictive of generalization. Our work sheds light on the underlying causes
for failures in MTL and, importantly, raises questions about the role of
general purpose multi-task optimization algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages; Published in TMLR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FLRONet: Deep Operator Learning for High-Fidelity Fluid Flow Field
  Reconstruction from Sparse Sensor Measurements 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08009v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08009v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hiep Vo Dang, Joseph B. Choi, Phong C. H. Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing high-fidelity fluid flow fields from sparse sensor
measurements is vital for many science and engineering applications but remains
challenging because of dimensional disparities between state and observational
spaces. Due to such dimensional differences, the measurement operator becomes
ill-conditioned and non-invertible, making the reconstruction of flow fields
from sensor measurements extremely difficult. Although sparse optimization and
machine learning address the above problems to some extent, questions about
their generalization and efficiency remain, particularly regarding the
discretization dependence of these models. In this context, deep operator
learning offers a better solution as this approach models mappings between
infinite-dimensional functional spaces, enabling superior generalization and
discretization-independent reconstruction. We introduce FLRONet, a deep
operator learning framework that is trained to reconstruct fluid flow fields
from sparse sensor measurements. FLRONet employs a branch-trunk network
architecture to represent the inverse measurement operator that maps sensor
observations to the original flow field, a continuous function of both space
and time. Validation performed on the CFDBench dataset has demonstrated that
FLRONet consistently achieves high levels of reconstruction accuracy and
robustness, even in scenarios where sensor measurements are inaccurate or
missing. Furthermore, the operator learning approach endows FLRONet with the
capability to perform zero-shot super-resolution in both spatial and temporal
domains, offering a solution for rapid reconstruction of high-fidelity flow
fields.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Model's Interpretability and Reliability using Biomarkers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.12394v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.12394v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gautam Rajendrakumar Gare, Tom Fox, Beam Chansangavej, Amita Krishnan, Ricardo Luis Rodriguez, Bennett P deBoisblanc, Deva Kannan Ramanan, John Michael Galeotti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate and interpretable diagnostic models are crucial in the
safety-critical field of medicine. We investigate the interpretability of our
proposed biomarker-based lung ultrasound diagnostic pipeline to enhance
clinicians' diagnostic capabilities. The objective of this study is to assess
whether explanations from a decision tree classifier, utilizing biomarkers, can
improve users' ability to identify inaccurate model predictions compared to
conventional saliency maps. Our findings demonstrate that decision tree
explanations, based on clinically established biomarkers, can assist clinicians
in detecting false positives, thus improving the reliability of diagnostic
models in medicine.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at BIAS 2023 Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How Much Can We Forget about Data Contamination? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03249v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03249v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastian Bordt, Suraj Srinivas, Valentyn Boreiko, Ulrike von Luxburg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The leakage of benchmark data into the training data has emerged as a
significant challenge for evaluating the capabilities of large language models
(LLMs). In this work, we challenge the common assumption that small-scale
contamination renders benchmark evaluations invalid. First, we experimentally
quantify the magnitude of benchmark overfitting based on scaling along three
dimensions: The number of model parameters (up to 1.6B), the number of times an
example is seen (up to 144), and the number of training tokens (up to 40B). If
model and data follow the Chinchilla scaling laws, minor contamination indeed
leads to overfitting. At the same time, even 144 times of contamination can be
forgotten if the training data is scaled beyond five times Chinchilla, a regime
characteristic of many modern LLMs. Continual pre-training of OLMo-7B
corroborates these results. Next, we study the impact of the weight decay
parameter on example forgetting, showing that empirical forgetting occurs
faster than the cumulative weight decay. This allows us to gauge the degree of
example forgetting in large-scale training runs, indicating that many LLMs,
including Lllama 3 405B, have forgotten the data seen at the beginning of
training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Zero-Shot Medical Phrase Grounding with Off-the-shelf Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.12920v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.12920v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Konstantinos Vilouras, Pedro Sanchez, Alison Q. O'Neil, Sotirios A. Tsaftaris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Localizing the exact pathological regions in a given medical scan is an
important imaging problem that traditionally requires a large amount of
bounding box ground truth annotations to be accurately solved. However, there
exist alternative, potentially weaker, forms of supervision, such as
accompanying free-text reports, which are readily available. The task of
performing localization with textual guidance is commonly referred to as phrase
grounding. In this work, we use a publicly available Foundation Model, namely
the Latent Diffusion Model, to perform this challenging task. This choice is
supported by the fact that the Latent Diffusion Model, despite being generative
in nature, contains cross-attention mechanisms that implicitly align visual and
textual features, thus leading to intermediate representations that are
suitable for the task at hand. In addition, we aim to perform this task in a
zero-shot manner, i.e., without any training on the target task, meaning that
the model's weights remain frozen. To this end, we devise strategies to select
features and also refine them via post-processing without extra learnable
parameters. We compare our proposed method with state-of-the-art approaches
which explicitly enforce image-text alignment in a joint embedding space via
contrastive learning. Results on a popular chest X-ray benchmark indicate that
our method is competitive with SOTA on different types of pathology, and even
outperforms them on average in terms of two metrics (mean IoU and AUC-ROC).
Source code will be released upon acceptance at https://github.com/vios-s.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 3 figures, IEEE J-BHI Special Issue on Foundation Models in
  Medical Imaging</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Energy-based physics-informed neural network for frictionless contact
  problems under large deformation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03671v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03671v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinshuai Bai, Zhongya Lin, Yizheng Wang, Jiancong Wen, Yinghua Liu, Timon Rabczuk, YuanTong Gu, Xi-Qiao Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Numerical methods for contact mechanics are of great importance in
engineering applications, enabling the prediction and analysis of complex
surface interactions under various conditions. In this work, we propose an
energy-based physics-informed neural network (PINNs) framework for solving
frictionless contact problems under large deformation. Inspired by microscopic
Lennard-Jones potential, a surface contact energy is used to describe the
contact phenomena. To ensure the robustness of the proposed PINN framework,
relaxation, gradual loading and output scaling techniques are introduced. In
the numerical examples, the well-known Hertz contact benchmark problem is
conducted, demonstrating the effectiveness and robustness of the proposed PINNs
framework. Moreover, challenging contact problems with the consideration of
geometrical and material nonlinearities are tested. It has been shown that the
proposed PINNs framework provides a reliable and powerful tool for nonlinear
contact mechanics. More importantly, the proposed PINNs framework exhibits
competitive computational efficiency to the commercial FEM software when
dealing with those complex contact problems. The codes used in this manuscript
are available at https://github.com/JinshuaiBai/energy_PINN_Contact.(The code
will be available after acceptance)
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Hype-Adjusted Probability Measure for NLP Stock Return Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07587v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07587v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Cao, Helyette Geman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article introduces a Hype-Adjusted Probability Measure in the context of
a new Natural Language Processing (NLP) approach for stock return and
volatility forecasting. A novel sentiment score equation is proposed to
represent the impact of intraday news on forecasting next-period stock return
and volatility for selected U.S. semiconductor tickers, a very vibrant industry
sector. This work improves the forecast accuracy by addressing news bias,
memory, and weight, and incorporating shifts in sentiment direction. More
importantly, it extends the use of the remarkable tool of change of Probability
Measure developed in the finance of Asset Pricing to NLP forecasting by
constructing a Hype-Adjusted Probability Measure, obtained from a
redistribution of the weights in the probability space, meant to correct for
excessive or insufficient news.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bayesian Despeckling of Structured Sources 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.11860v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.11860v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Zafari, Shirin Jalali
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speckle noise is a fundamental challenge in coherent imaging systems,
significantly degrading image quality. Over the past decades, numerous
despeckling algorithms have been developed for applications such as Synthetic
Aperture Radar (SAR) and digital holography. In this paper, we aim to establish
a theoretically grounded approach to despeckling. We propose a method
applicable to general structured stationary stochastic sources. We demonstrate
the effectiveness of the proposed method on piecewise constant sources.
Additionally, we theoretically derive a lower bound on the despeckling
performance for such sources. The proposed depseckler applied to the 1-Markov
structured sources achieves better reconstruction performance with no strong
simplification of the ground truth signal model or speckle noise.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Probabilistic Verification of Neural Networks using Branch and Bound 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17556v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17556v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Boetius, Stefan Leue, Tobias Sutter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Probabilistic verification of neural networks is concerned with formally
analysing the output distribution of a neural network under a probability
distribution of the inputs. Examples of probabilistic verification include
verifying the demographic parity fairness notion or quantifying the safety of a
neural network. We present a new algorithm for the probabilistic verification
of neural networks based on an algorithm for computing and iteratively refining
lower and upper bounds on probabilities over the outputs of a neural network.
By applying state-of-the-art bound propagation and branch and bound techniques
from non-probabilistic neural network verification, our algorithm significantly
outpaces existing probabilistic verification algorithms, reducing solving times
for various benchmarks from the literature from tens of minutes to tens of
seconds. Furthermore, our algorithm compares favourably even to dedicated
algorithms for restricted subsets of probabilistic verification. We complement
our empirical evaluation with a theoretical analysis, proving that our
algorithm is sound and, under mildly restrictive conditions, also complete when
using a suitable set of heuristics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code available at https://github.com/sen-uni-kn/probspecs; 19 pages,
  3 figures, 30 pages references and appendix, including 7 more figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative Adversarial Reduced Order Modelling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.15881v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.15881v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dario Coscia, Nicola Demo, Gianluigi Rozza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we present GAROM, a new approach for reduced order modelling
(ROM) based on generative adversarial networks (GANs). GANs have the potential
to learn data distribution and generate more realistic data. While widely
applied in many areas of deep learning, little research is done on their
application for ROM, i.e. approximating a high-fidelity model with a simpler
one. In this work, we combine the GAN and ROM framework, by introducing a
data-driven generative adversarial model able to learn solutions to parametric
differential equations. The latter is achieved by modelling the discriminator
network as an autoencoder, extracting relevant features of the input, and
applying a conditioning mechanism to the generator and discriminator networks
specifying the differential equation parameters. We show how to apply our
methodology for inference, provide experimental evidence of the model
generalisation, and perform a convergence study of the method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLMs & XAI for Water Sustainability: Seasonal Water Quality Prediction
  with LIME Explainable AI and a RAG-based Chatbot for Insights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10898v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10898v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Biplov Paneru, Bishwash Paneru
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring safe water supplies requires effective water quality monitoring,
especially in developing countries like Nepal, where contamination risks are
high. This paper introduces a hybrid deep learning model to predict Nepal's
seasonal water quality using a small dataset with multiple water quality
parameters. Models such as CatBoost, XGBoost, Extra Trees, and LightGBM, along
with a neural network combining CNN and RNN layers, are used to capture
temporal and spatial patterns in the data. The model demonstrated notable
accuracy improvements, aiding proactive water quality control. CatBoost,
XGBoost, and Extra Trees Regressor predicted Water Quality Index (WQI) values
with an average RMSE of 1.2 and an R2 score of 0.99. Additionally, classifiers
achieved 99 percent accuracy, cross-validated across models. LIME analysis
highlighted the importance of indicators like EC and DO levels in XGBoost
classification decisions. The neural network model achieved 92 percent
classification accuracy and an R2 score of 0.97, with an RMSE of 2.87 in
regression analysis. Furthermore, a multifunctional application was developed
to predict WQI values using both regression and classification methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Models Reflect the Ideology of their Creators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.18417v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.18417v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maarten Buyl, Alexander Rogiers, Sander Noels, Guillaume Bied, Iris Dominguez-Catena, Edith Heiter, Iman Johary, Alexandru-Cristian Mara, Raphaël Romero, Jefrey Lijffijt, Tijl De Bie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are trained on vast amounts of data to generate
natural language, enabling them to perform tasks like text summarization and
question answering. These models have become popular in artificial intelligence
(AI) assistants like ChatGPT and already play an influential role in how humans
access information. However, the behavior of LLMs varies depending on their
design, training, and use.
  In this paper, we prompt a diverse panel of popular LLMs to describe a large
number of prominent personalities with political relevance, in all six official
languages of the United Nations. By identifying and analyzing moral assessments
reflected in their responses, we find normative differences between LLMs from
different geopolitical regions, as well as between the responses of the same
LLM when prompted in different languages. Among only models in the United
States, we find that popularly hypothesized disparities in political views are
reflected in significant normative differences related to progressive values.
Among Chinese models, we characterize a division between internationally- and
domestically-focused models.
  Our results show that the ideological stance of an LLM appears to reflect the
worldview of its creators. This poses the risk of political instrumentalization
and raises concerns around technological and regulatory efforts with the stated
aim of making LLMs ideologically 'unbiased'.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Universal Rates of Empirical Risk Minimization <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.02810v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.02810v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Steve Hanneke, Mingyue Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The well-known empirical risk minimization (ERM) principle is the basis of
many widely used machine learning algorithms, and plays an essential role in
the classical PAC theory. A common description of a learning algorithm's
performance is its so-called "learning curve", that is, the decay of the
expected error as a function of the input sample size. As the PAC model fails
to explain the behavior of learning curves, recent research has explored an
alternative universal learning model and has ultimately revealed a distinction
between optimal universal and uniform learning rates (Bousquet et al., 2021).
However, a basic understanding of such differences with a particular focus on
the ERM principle has yet to be developed.
  In this paper, we consider the problem of universal learning by ERM in the
realizable case and study the possible universal rates. Our main result is a
fundamental tetrachotomy: there are only four possible universal learning rates
by ERM, namely, the learning curves of any concept class learnable by ERM decay
either at $e^{-n}$, $1/n$, $\log(n)/n$, or arbitrarily slow rates. Moreover, we
provide a complete characterization of which concept classes fall into each of
these categories, via new complexity structures. We also develop new
combinatorial dimensions which supply sharp asymptotically-valid constant
factors for these rates, whenever possible.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted to the 38th Conference on Neural
  Information Processing Systems (NeurIPS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Clipped SGD Algorithms for Performative Prediction: Tight Bounds for
  Clipping Bias and Remedies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.10995v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.10995v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiang Li, Michal Yemini, Hoi-To Wai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies the convergence of clipped stochastic gradient descent
(SGD) algorithms with decision-dependent data distribution. Our setting is
motivated by privacy preserving optimization algorithms that interact with
performative data where the prediction models can influence future outcomes.
This challenging setting involves the non-smooth clipping operator and
non-gradient dynamics due to distribution shifts. We make two contributions in
pursuit for a performative stable solution using clipped SGD algorithms. First,
we characterize the clipping bias with projected clipped SGD (PCSGD) algorithm
which is caused by the clipping operator that prevents PCSGD from reaching a
stable solution. When the loss function is strongly convex, we quantify the
lower and upper bounds for this clipping bias and demonstrate a bias
amplification phenomenon with the sensitivity of data distribution. When the
loss function is non-convex, we bound the magnitude of stationarity bias.
Second, we propose remedies to mitigate the bias either by utilizing an optimal
step size design for PCSGD, or to apply the recent DiceSGD algorithm [Zhang et
al., 2024]. Our analysis is also extended to show that the latter algorithm is
free from clipping bias in the performative setting. Numerical experiments
verify our findings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Effective Learning with Node Perturbation in Multi-Layer Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.00965v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.00965v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sander Dalm, Marcel van Gerven, Nasir Ahmad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Backpropagation (BP) remains the dominant and most successful method for
training parameters of deep neural network models. However, BP relies on two
computationally distinct phases, does not provide a satisfactory explanation of
biological learning, and can be challenging to apply for training of networks
with discontinuities or noisy node dynamics. By comparison, node perturbation
(NP) proposes learning by the injection of noise into network activations, and
subsequent measurement of the induced loss change. NP relies on two forward
(inference) passes, does not make use of network derivatives, and has been
proposed as a model for learning in biological systems. However, standard NP is
highly data inefficient and unstable due to its unguided noise-based search
process. In this work, we investigate different formulations of NP and relate
it to the concept of directional derivatives as well as combining it with a
decorrelating mechanism for layer-wise inputs. We find that a closer alignment
with directional derivatives together with input decorrelation at every layer
strongly enhances performance of NP learning with large improvements in
parameter convergence and much higher performance on the test data, approaching
that of BP. Furthermore, our novel formulation allows for application to noisy
systems in which the noise process itself is inaccessible.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revisiting Prefix-tuning: Statistical Benefits of Reparameterization
  among <span class="highlight-title">Prompt</span>s <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02200v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02200v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minh Le, Chau Nguyen, Huy Nguyen, Quyen Tran, Trung Le, Nhat Ho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt-based techniques, such as prompt-tuning and prefix-tuning, have gained
prominence for their efficiency in fine-tuning large pre-trained models.
Despite their widespread adoption, the theoretical foundations of these methods
remain limited. For instance, in prefix-tuning, we observe that a key factor in
achieving performance parity with full fine-tuning lies in the
reparameterization strategy. However, the theoretical principles underpinning
the effectiveness of this approach have yet to be thoroughly examined. Our
study demonstrates that reparameterization is not merely an engineering trick
but is grounded in deep theoretical foundations. Specifically, we show that the
reparameterization strategy implicitly encodes a shared structure between
prefix key and value vectors. Building on recent insights into the connection
between prefix-tuning and mixture of experts models, we further illustrate that
this shared structure significantly improves sample efficiency in parameter
estimation compared to non-shared alternatives. The effectiveness of
prefix-tuning across diverse tasks is empirically confirmed to be enhanced by
the shared structure, through extensive experiments in both visual and language
domains. Additionally, we uncover similar structural benefits in prompt-tuning,
offering new perspectives on its success. Our findings provide theoretical and
empirical contributions, advancing the understanding of prompt-based methods
and their underlying mechanisms. Our code is publicly available at
https://github.com/Minhchuyentoancbn/ReparamPrefix
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2025. 42 pages, 8 tables, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Zeroth-Order Adaptive Neuron Alignment Based Pruning without Re-Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07066v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07066v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elia Cunegatti, Leonardo Lucio Custode, Giovanni Iacca
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Network pruning focuses on computational techniques that aim to reduce a
given model's computational cost by removing a subset of its parameters while
having minimal impact on performance. Throughout the last decade, the most
widely used pruning paradigm has been pruning and re-training, which nowadays
is inconvenient due to the vast amount of pre-trained models, which are in any
case too expensive to re-train. In this paper, we exploit functional
information from dense pre-trained models, i.e., their activations, to obtain
sparse models that maximize the activations' alignment w.r.t. their
corresponding dense models. Hence, we propose \textsc{NeuroAL}, a \emph{top-up}
algorithm that can be used on top of any given pruning algorithm for LLMs,
which modifies the block-wise and row-wise sparsity exploiting information from
both the dense model and its sparse version to maximize the \emph{neuron
alignment} among activations. Differently from existing methods, our approach
adaptively selects the best hyperparameters for the block-wise and row-wise
sparsity ratios w.r.t. the model and the desired sparsity, and requires
\emph{no re-training}. We test our method over 276 cases combining four LLM
families, three sparsity ratios, and ten language tasks (three language
modeling and seven zero-shot datasets), showing how it consistently outperforms
the latest state-of-the-art methods in terms of performance-runtime trade-off.
The code is available at
\href{https://github.com/eliacunegatti/NeuroAL}{https://github.com/eliacunegatti/NeuroAL}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Overcoming Uncertain Incompleteness for Robust Multimodal Sequential
  Diagnosis Prediction via Curriculum Data Erasing Guided Knowledge
  Distillation <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.19540v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.19540v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heejoon Koo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present NECHO v2, a novel framework designed to enhance the
predictive accuracy of multimodal sequential patient diagnoses under uncertain
missing visit sequences, a common challenge in real clinical settings. Firstly,
we modify NECHO, designed in a diagnosis code-centric fashion, to handle
uncertain modality representation dominance under the imperfect data. Secondly,
we develop a systematic knowledge distillation by employing the modified NECHO
as both teacher and student. It encompasses a modality-wise contrastive and
hierarchical distillation, transformer representation random distillation,
along with other distillations to align representations between teacher and
student tightly and effectively. We also propose curriculum learning guided
random data erasing within sequences during both training and distillation of
the teacher to lightly simulate scenario with missing visit information,
thereby fostering effective knowledge transfer. As a result, NECHO v2 verifies
itself by showing robust superiority in multimodal sequential diagnosis
prediction under both balanced and imbalanced incomplete settings on multimodal
healthcare data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2025 (2025 IEEE International Conference on
  Acoustics, Speech, and Signal Processing)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ROK Defense M&S in the Age of Hyperscale AI: Concepts, Challenges, and
  Future Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00367v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00367v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youngjoon Lee, Taehyun Park, Yeongjoon Kang, Jonghoe Kim, Joonhyuk Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Integrating hyperscale AI into national defense M&S(Modeling and Simulation),
under the expanding IoMDT(Internet of Military Defense Things) framework, is
crucial for boosting strategic and operational readiness. We examine how
IoMDT-driven hyperscale AI can provide high accuracy, speed, and the ability to
simulate complex, interconnected battlefield scenarios in defense M&S.
Countries like the United States and China are leading the adoption of these
technologies, with varying levels of success. However, realizing the full
potential of hyperscale AI requires overcoming challenges such as closed
networks, sparse or long-tail data, complex decision-making processes, and a
shortage of experts. Future directions highlight the need to adopt domestic
foundation models, expand GPU/NPU investments, leverage large tech services,
and employ open source solutions. These efforts will enhance national security,
maintain a competitive edge, and spur broader technological and economic
growth. With this blueprint, the Republic of Korea can strengthen its defense
posture and stay ahead of emerging threats in modern warfare.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE Internet of Things Magazine</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Remaining Useful Life Prediction for Batteries Utilizing an Explainable
  AI Approach with a Predictive Application for Decision-Making 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17931v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17931v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Biplov Paneru, Bipul Thapa, Durga Prasad Mainali, Bishwash Paneru, Krishna Bikram Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurately estimating the Remaining Useful Life (RUL) of a battery is
essential for determining its lifespan and recharge requirements. In this work,
we develop machine learning-based models to predict and classify battery RUL.
We introduce a two-level ensemble learning (TLE) framework and a CNN+MLP hybrid
model for RUL prediction, comparing their performance against traditional,
deep, and hybrid machine learning models. Our analysis evaluates various models
for both prediction and classification while incorporating interpretability
through SHAP. The proposed TLE model consistently outperforms baseline models
in RMSE, MAE, and R squared error, demonstrating its superior predictive
capabilities. Additionally, the XGBoost classifier achieves an impressive 99%
classification accuracy, validated through cross-validation techniques. The
models effectively predict relay-based charging triggers, enabling automated
and energy-efficient charging processes. This automation reduces energy
consumption and enhances battery performance by optimizing charging cycles.
SHAP interpretability analysis highlights the cycle index and charging
parameters as the most critical factors influencing RUL. To improve
accessibility, we developed a Tkinter-based GUI that allows users to input new
data and predict RUL in real time. This practical solution supports sustainable
battery management by enabling data-driven decisions about battery usage and
maintenance, contributing to energy-efficient and innovative battery life
prediction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ©Plug-in Authorization for Human Content Copyright Protection
  in Text-to-Image Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.11962v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.11962v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Zhou, Huishuai Zhang, Jiang Bian, Weiming Zhang, Nenghai Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the contentious issue of copyright infringement in
images generated by text-to-image models, sparking debates among AI developers,
content creators, and legal entities. State-of-the-art models create
high-quality content without crediting original creators, causing concern in
the artistic community. To mitigate this, we propose the \copyright Plug-in
Authorization framework, introducing three operations: addition, extraction,
and combination. Addition involves training a \copyright plug-in for specific
copyright, facilitating proper credit attribution. Extraction allows creators
to reclaim copyright from infringing models, and combination enables users to
merge different \copyright plug-ins. These operations act as permits,
incentivizing fair use and providing flexibility in authorization. We present
innovative approaches,"Reverse LoRA" for extraction and "EasyMerge" for
seamless combination. Experiments in artist-style replication and cartoon IP
recreation demonstrate \copyright plug-ins' effectiveness, offering a valuable
solution for human copyright protection in the age of generative AIs. The code
is available at https://github.com/zc1023/-Plug-in-Authorization.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Prospective Learning: Learning for a Dynamic Future <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00109v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00109v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashwin De Silva, Rahul Ramesh, Rubing Yang, Siyu Yu, Joshua T Vogelstein, Pratik Chaudhari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real-world applications, the distribution of the data, and our goals,
evolve over time. The prevailing theoretical framework for studying machine
learning, namely probably approximately correct (PAC) learning, largely ignores
time. As a consequence, existing strategies to address the dynamic nature of
data and goals exhibit poor real-world performance. This paper develops a
theoretical framework called "Prospective Learning" that is tailored for
situations when the optimal hypothesis changes over time. In PAC learning,
empirical risk minimization (ERM) is known to be consistent. We develop a
learner called Prospective ERM, which returns a sequence of predictors that
make predictions on future data. We prove that the risk of prospective ERM
converges to the Bayes risk under certain assumptions on the stochastic process
generating the data. Prospective ERM, roughly speaking, incorporates time as an
input in addition to the data. We show that standard ERM as done in PAC
learning, without incorporating time, can result in failure to learn when
distributions are dynamic. Numerical experiments illustrate that prospective
ERM can learn synthetic and visual recognition problems constructed from MNIST
and CIFAR-10. Code at https://github.com/neurodata/prolearn.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Real-Time Decision-Making for Digital Twin in Additive Manufacturing
  with Model Predictive Control using Time-Series Deep Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07601v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07601v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi-Ping Chen, Vispi Karkaria, Ying-Kuan Tsai, Faith Rolark, Daniel Quispe, Robert X. Gao, Jian Cao, Wei Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Digital Twin-a virtual replica of a physical system enabling real-time
monitoring, model updating, prediction, and decision-making-combined with
recent advances in machine learning (ML), offers new opportunities for
proactive control strategies in autonomous manufacturing. However, achieving
real-time decision-making with Digital Twins requires efficient optimization
driven by accurate predictions of highly nonlinear manufacturing systems. This
paper presents a simultaneous multi-step Model Predictive Control (MPC)
framework for real-time decision-making, using a multi-variate deep neural
network (DNN), named Time-Series Dense Encoder (TiDE), as the surrogate model.
Different from the models in conventional MPC which only provide one-step ahead
prediction, TiDE is capable of predicting future states within the prediction
horizon in one shot (multi-step), significantly accelerating MPC. Using
Directed Energy Deposition additive manufacturing as a case study, we
demonstrate the effectiveness of the proposed MPC in achieving melt pool
temperature tracking to ensure part quality, while reducing porosity defects by
regulating laser power to maintain melt pool depth constraints. In this work,
we first show that TiDE is capable of accurately predicting melt pool
temperature and depth. Second, we demonstrate that the proposed MPC achieves
precise temperature tracking while satisfying melt pool depth constraints
within a targeted dilution range (10%-30%), reducing potential porosity
defects. Compared to the PID controller, MPC results in smoother and less
fluctuating laser power profiles with competitive or superior melt pool
temperature control performance. This demonstrates MPC's proactive control
capabilities, leveraging time-series prediction and real-time optimization,
positioning it as a powerful tool for future Digital Twin applications and
real-time process optimization in manufacturing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Retrieve, Merge, Predict: Augmenting Tables with Data Lakes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.06282v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.06282v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Riccardo Cappuzzo, Aimee Coelho, Felix Lefebvre, Paolo Papotti, Gael Varoquaux
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine-learning from a disparate set of tables, a data lake, requires
assembling features by merging and aggregating tables. Data discovery can
extend autoML to data tables by automating these steps. We present an in-depth
analysis of such automated table augmentation for machine learning tasks,
analyzing different methods for the three main steps: retrieving joinable
tables, merging information, and predicting with the resultant table. We use
two data lakes: Open Data US, a well-referenced real data lake, and a novel
semi-synthetic dataset, YADL (Yet Another Data Lake), which we developed as a
tool for benchmarking this data discovery task. Systematic exploration on both
lakes outlines 1) the importance of accurately retrieving join candidates, 2)
the efficiency of simple merging methods, and 3) the resilience of tree-based
learners to noisy conditions. Our experimental environment is easily
reproducible and based on open data, to foster more research on feature
engineering, autoML, and learning in data lakes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages + references, 6 figures in main body. 15 pages + 11 figures
  in appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Network Verification is a Programming Language Challenge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05867v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05867v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucas C. Cordeiro, Matthew L. Daggitt, Julien Girard-Satabin, Omri Isac, Taylor T. Johnson, Guy Katz, Ekaterina Komendantskaya, Augustin Lemesle, Edoardo Manino, Artjoms Šinkarovs, Haoze Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural network verification is a new and rapidly developing field of
research. So far, the main priority has been establishing efficient
verification algorithms and tools, while proper support from the programming
language perspective has been considered secondary or unimportant. Yet, there
is mounting evidence that insights from the programming language community may
make a difference in the future development of this domain. In this paper, we
formulate neural network verification challenges as programming language
challenges and suggest possible future solutions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ESOP 2025, European Symposium on Programming Languages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Meta-Representation Hypothesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.02481v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.02481v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengpeng Xie, Jiahang Cao, Qiang Zhang, Jianxiong Zhang, Changwei Wang, Renjing Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans rely on high-level understandings of things, i.e.,
meta-representations, to engage in abstract reasoning. In complex cognitive
tasks, these meta-representations help individuals abstract general rules from
experience. However, constructing such meta-representations from
high-dimensional observations remains a longstanding challenge for
reinforcement learning (RL) agents. For instance, a well-trained agent often
fails to generalize to even minor variations of the same task, such as changes
in background color, while humans can easily handle. In this paper, we
theoretically investigate how meta-representations contribute to the
generalization ability of RL agents, demonstrating that learning
meta-representations from high-dimensional observations enhance an agent's
ability to generalize across varied environments. We further hypothesize that
deep mutual learning (DML) among agents can help them learn the
meta-representations that capture the underlying essence of the task. Empirical
results provide strong support for both our theory and hypothesis. Overall,
this work provides a new perspective on the generalization of deep
reinforcement learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LSEAttention is All You Need for Time Series Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23749v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23749v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dizhen Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based architectures have achieved remarkable success in natural
language processing and computer vision. However, their performance in
multivariate long-term forecasting often falls short compared to simpler linear
baselines. Previous research has identified the traditional attention mechanism
as a key factor limiting their effectiveness in this domain. To bridge this
gap, we introduce LATST, a novel approach designed to mitigate entropy collapse
and training instability common challenges in Transformer-based time series
forecasting. We rigorously evaluate LATST across multiple real-world
multivariate time series datasets, demonstrating its ability to outperform
existing state-of-the-art Transformer models. Notably, LATST manages to achieve
competitive performance with fewer parameters than some linear models on
certain datasets, highlighting its efficiency and effectiveness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages with referencing, 1 figure, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Uncertainty quantification in automated valuation models with spatially
  weighted conformal prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.06531v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.06531v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anders Hjort, Gudmund Horn Hermansen, Johan Pensar, Jonathan P. Williams
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Non-parametric machine learning models, such as random forests and gradient
boosted trees, are frequently used to estimate house prices due to their
predictive accuracy, but a main drawback of such methods is their limited
ability to quantify prediction uncertainty. Conformal prediction (CP) is a
model-agnostic framework for constructing confidence sets around predictions of
machine learning models with minimal assumptions. However, due to the spatial
dependencies observed in house prices, direct application of CP leads to
confidence sets that are not calibrated everywhere, i.e., the confidence sets
will be too large in certain geographical regions and too small in others. We
survey various approaches to adjust the CP confidence set to account for this
and demonstrate their performance on a data set from the housing market in
Oslo, Norway. Our findings indicate that calibrating the confidence sets on a
spatially weighted version of the non-conformity scores makes the coverage more
consistently calibrated across geographical regions. We also perform a
simulation study on synthetically generated sale prices to empirically explore
the performance of CP on housing market data under idealized conditions with
known data-generating mechanisms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Algorithms for Regularized Nonnegative Scale-invariant
  Low-rank Approximation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18517v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18517v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeremy E. Cohen, Valentin Leplat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Regularized nonnegative low-rank approximations, such as sparse Nonnegative
Matrix Factorization or sparse Nonnegative Tucker Decomposition, form an
important branch of dimensionality reduction models known for their enhanced
interpretability. From a practical perspective, however, selecting appropriate
regularizers and regularization coefficients, as well as designing efficient
algorithms, remains challenging due to the multifactor nature of these models
and the limited theoretical guidance available. This paper addresses these
challenges by studying a more general model, the Homogeneous Regularized
Scale-Invariant model. We prove that the scale-invariance inherent to low-rank
approximation models induces an implicit regularization effect that balances
solutions. This insight provides a deeper understanding of the role of
regularization functions in low-rank approximation models, informs the
selection of regularization hyperparameters, and enables the design of
balancing strategies to accelerate the empirical convergence of optimization
algorithms.
  Additionally, we propose a generic Majorization-Minimization (MM) algorithm
capable of handling $\ell_p^p$-regularized nonnegative low-rank approximations
with non-Euclidean loss functions, with convergence guarantees. Our
contributions are demonstrated on sparse Nonnegative Matrix Factorization,
ridge-regularized Nonnegative Canonical Polyadic Decomposition, and sparse
Nonnegative Tucker Decomposition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Version after acceptance in SIAM Journal on Mathematics of Data
  Science (SIMODS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CHIRPs: Change-Induced Regret Proxy metrics for Lifelong Reinforcement
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.03577v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.03577v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John Birkbeck, Adam Sobey, Federico Cerutti, Katherine Heseltine Hurley Flynn, Timothy J. Norman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) agents are costly to train and fragile to
environmental changes. They often perform poorly when there are many changing
tasks, prohibiting their widespread deployment in the real world. Many Lifelong
RL agent designs have been proposed to mitigate issues such as catastrophic
forgetting or demonstrate positive characteristics like forward transfer when
change occurs. However, no prior work has established whether the impact on
agent performance can be predicted from the change itself. Understanding this
relationship will help agents proactively mitigate a change's impact for
improved learning performance. We propose Change-Induced Regret Proxy (CHIRP)
metrics to link change to agent performance drops and use two environments to
demonstrate a CHIRP's utility in lifelong learning. A simple CHIRP-based agent
achieved $48\%$ higher performance than the next best method in one benchmark
and attained the best success rates in 8 of 10 tasks in a second benchmark
which proved difficult for existing lifelong RL agents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Learning With Sine-Activated Low-rank Matrices <span class="chip">ICLR
  2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.19243v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.19243v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiping Ji, Hemanth Saratchandran, Cameron Gordon, Zeyu Zhang, Simon Lucey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-rank decomposition has emerged as a vital tool for enhancing parameter
efficiency in neural network architectures, gaining traction across diverse
applications in machine learning. These techniques significantly lower the
number of parameters, striking a balance between compactness and performance.
However, a common challenge has been the compromise between parameter
efficiency and the accuracy of the model, where reduced parameters often lead
to diminished accuracy compared to their full-rank counterparts. In this work,
we propose a novel theoretical framework that integrates a sinusoidal function
within the low-rank decomposition process. This approach not only preserves the
benefits of the parameter efficiency characteristic of low-rank methods but
also increases the decomposition's rank, thereby enhancing model performance.
Our method proves to be a plug in enhancement for existing low-rank models, as
evidenced by its successful application in Vision Transformers (ViT), Large
Language Models (LLMs), Neural Radiance Fields (NeRF) and 3D shape modelling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors contributed equally. Paper accepted at ICLR
  2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MusicLIME: Explainable Multimodal Music Understanding <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10496v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10496v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Theodoros Sotirou, Vassilis Lyberatos, Orfeas Menis Mastromichalakis, Giorgos Stamou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal models are critical for music understanding tasks, as they capture
the complex interplay between audio and lyrics. However, as these models become
more prevalent, the need for explainability grows-understanding how these
systems make decisions is vital for ensuring fairness, reducing bias, and
fostering trust. In this paper, we introduce MusicLIME, a model-agnostic
feature importance explanation method designed for multimodal music models.
Unlike traditional unimodal methods, which analyze each modality separately
without considering the interaction between them, often leading to incomplete
or misleading explanations, MusicLIME reveals how audio and lyrical features
interact and contribute to predictions, providing a holistic view of the
model's decision-making. Additionally, we enhance local explanations by
aggregating them into global explanations, giving users a broader perspective
of model behavior. Through this work, we contribute to improving the
interpretability of multimodal music models, empowering users to make informed
choices, and fostering more equitable, fair, and transparent music
understanding systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>GitHub repository: https://github.com/IamTheo2000/MusicLIME. To be
  presented at ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ScaDyG:A New Paradigm for Large-scale Dynamic Graph Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16002v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16002v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Wu, Xunkai Li, Rong-Hua Li, Kangfei Zhao, Guoren Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dynamic graphs (DGs), which capture time-evolving relationships between graph
entities, have widespread real-world applications. To efficiently encode DGs
for downstream tasks, most dynamic graph neural networks follow the traditional
message-passing mechanism and extend it with time-based techniques. Despite
their effectiveness, the growth of historical interactions introduces
significant scalability issues, particularly in industry scenarios. To address
this limitation, we propose ScaDyG, with the core idea of designing a
time-aware scalable learning paradigm as follows: 1) Time-aware Topology
Reformulation: ScaDyG first segments historical interactions into time steps
(intra and inter) based on dynamic modeling, enabling weight-free and
time-aware graph propagation within pre-processing. 2) Dynamic Temporal
Encoding: To further achieve fine-grained graph propagation within time steps,
ScaDyG integrates temporal encoding through a combination of exponential
functions in a scalable manner. 3) Hypernetwork-driven Message Aggregation:
After obtaining the propagated features (i.e., messages), ScaDyG utilizes
hypernetwork to analyze historical dependencies, implementing node-wise
representation by an adaptive temporal fusion. Extensive experiments on 12
datasets demonstrate that ScaDyG performs comparably well or even outperforms
other SOTA methods in both node and link-level downstream tasks, with fewer
learnable parameters and higher efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unsupervised Learning in Echo State Networks for Input Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.11409v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.11409v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taiki Yamada, Yuichi Katori, Kantaro Fujiwara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conventional echo state networks (ESNs) require supervised learning to train
the readout layer, using the desired outputs as training data. In this study,
we focus on input reconstruction (IR), which refers to training the readout
layer to reproduce the input time series in its output. We reformulate the
learning algorithm of the ESN readout layer to perform IR using unsupervised
learning (UL). By conducting theoretical analysis and numerical experiments, we
demonstrate that IR in ESNs can be effectively implemented under realistic
conditions without explicitly using the desired outputs as training data; in
this way, UL is enabled. Furthermore, we demonstrate that applications relying
on IR, such as dynamical system replication and noise filtering, can be
reformulated within the UL framework. Our findings establish a theoretically
sound and universally applicable IR formulation, along with its related tasks
in ESNs. This work paves the way for novel predictions and highlights
unresolved theoretical challenges in ESNs, particularly in the context of
time-series processing methods and computational models of the brain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 7 figures, regular paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Boosting Weak Positives for Text Based Person Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17586v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17586v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akshay Modi, Ashhar Aziz, Nilanjana Chatterjee, A V Subramanyam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large vision-language models have revolutionized cross-modal object
retrieval, but text-based person search (TBPS) remains a challenging task due
to limited data and fine-grained nature of the task. Existing methods primarily
focus on aligning image-text pairs into a common representation space, often
disregarding the fact that real world positive image-text pairs share a varied
degree of similarity in between them. This leads models to prioritize easy
pairs, and in some recent approaches, challenging samples are discarded as
noise during training. In this work, we introduce a boosting technique that
dynamically identifies and emphasizes these challenging samples during
training. Our approach is motivated from classical boosting technique and
dynamically updates the weights of the weak positives, wherein, the rank-1
match does not share the identity of the query. The weight allows these
misranked pairs to contribute more towards the loss and the network has to pay
more attention towards such samples. Our method achieves improved performance
across four pedestrian datasets, demonstrating the effectiveness of our
proposed module.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MINN: Learning the dynamics of differential-algebraic equations and
  application to battery modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.14422v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.14422v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yicun Huang, Changfu Zou, Yang Li, Torsten Wik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The concept of integrating physics-based and data-driven approaches has
become popular for modeling sustainable energy systems. However, the existing
literature mainly focuses on the data-driven surrogates generated to replace
physics-based models. These models often trade accuracy for speed but lack the
generalizability, adaptability, and interpretability inherent in physics-based
models, which are often indispensable in modeling real-world dynamic systems
for optimization and control purposes. We propose a novel machine learning
architecture, termed model-integrated neural networks (MINN), that can learn
the physics-based dynamics of general autonomous or non-autonomous systems
consisting of partial differential-algebraic equations (PDAEs). The obtained
architecture systematically solves an unsettled research problem in
control-oriented modeling, i.e., how to obtain optimally simplified models that
are physically insightful, numerically accurate, and computationally tractable
simultaneously. We apply the proposed neural network architecture to model the
electrochemical dynamics of lithium-ion batteries and show that MINN is
extremely data-efficient to train while being sufficiently generalizable to
previously unseen input data, owing to its underlying physical invariants. The
MINN battery model has an accuracy comparable to the first principle-based
model in predicting both the system outputs and any locally distributed
electrochemical behaviors but achieves two orders of magnitude reduction in the
solution time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Swin fMRI <span class="highlight-title">Transformer</span> Predicts Early Neurodevelopmental Outcomes from
  Neonatal fMRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07783v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07783v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patrick Styll, Dowon Kim, Jiook Cha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Brain development in the first few months of human life is a critical phase
characterized by rapid structural growth and functional organization.
Accurately predicting developmental outcomes during this time is crucial for
identifying delays and enabling timely interventions. This study introduces the
SwiFT (Swin 4D fMRI Transformer) model, designed to predict Bayley-III
composite scores using neonatal fMRI from the Developing Human Connectome
Project (dHCP). To enhance predictive accuracy, we apply dimensionality
reduction via group independent component analysis (ICA) and pretrain SwiFT on
large adult fMRI datasets to address the challenges of limited neonatal data.
Our analysis shows that SwiFT significantly outperforms baseline models in
predicting cognitive, motor, and language outcomes, leveraging both
single-label and multi-label prediction strategies. The model's attention-based
architecture processes spatiotemporal data end-to-end, delivering superior
predictive performance. Additionally, we use Integrated Gradients with
Smoothgrad sQuare (IG-SQ) to interpret predictions, identifying neural spatial
representations linked to early cognitive and behavioral development. These
findings underscore the potential of Transformer models to advance
neurodevelopmental research and clinical practice.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>fMRI Transformer, Developing Human Connectome Project, Bayley Scales
  of Infant Development, Personalized Therapy, XAI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Computing the gradients with respect to all parameters of a quantum
  neural network using a single circuit 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.08167v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.08167v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guang Ping He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Finding gradients is a crucial step in training machine learning models. For
quantum neural networks, computing gradients using the parameter-shift rule
requires calculating the cost function twice for each adjustable parameter in
the network. When the total number of parameters is large, the quantum circuit
must be repeatedly adjusted and executed, leading to significant computational
overhead. Here we propose an approach to compute all gradients using a single
circuit only, significantly reducing both the circuit depth and the number of
classical registers required. We experimentally validate our approach on both
quantum simulators and IBM's real quantum hardware, demonstrating that our
method significantly reduces circuit compilation time compared to the
conventional approach, resulting in a substantial speedup in total runtime.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Fixed an incomplete link</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Necessary and Sufficient Conditions for Optimal Decision Trees using
  Dynamic Programming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.19706v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.19706v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jacobus G. M. van der Linden, Mathijs M. de Weerdt, Emir Demirović
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Global optimization of decision trees has shown to be promising in terms of
accuracy, size, and consequently human comprehensibility. However, many of the
methods used rely on general-purpose solvers for which scalability remains an
issue. Dynamic programming methods have been shown to scale much better because
they exploit the tree structure by solving subtrees as independent subproblems.
However, this only works when an objective can be optimized separately for
subtrees. We explore this relationship in detail and show the necessary and
sufficient conditions for such separability and generalize previous dynamic
programming approaches into a framework that can optimize any combination of
separable objectives and constraints. Experiments on five application domains
show the general applicability of this framework, while outperforming the
scalability of general-purpose solvers by a large margin.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Control LLM: Controlled Evolution for Intelligence Retention in LLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10979v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10979v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haichao Wei, Yunxiang Ren, Zhoutong Fu, Aman Lunia, Yi-Lin Chen, Alice Leung, Ya Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) demand significant computational resources,
making it essential to enhance their capabilities without retraining from
scratch. A key challenge in this domain is \textit{catastrophic forgetting}
(CF), which hampers performance during Continuous Pre-training (CPT) and
Continuous Supervised Fine-Tuning (CSFT). We propose \textbf{Control LLM}, a
novel approach that leverages parallel pre-trained and expanded transformer
blocks, aligning their hidden-states through interpolation strategies This
method effectively preserves performance on existing tasks while seamlessly
integrating new knowledge.
  Extensive experiments demonstrate the effectiveness of Control LLM in both
CPT and CSFT. On Llama3.1-8B-Instruct, it achieves significant improvements in
mathematical reasoning ($+14.4\%$ on Math-Hard) and coding performance ($+10\%$
on MBPP-PLUS). On Llama3.1-8B, it enhances multilingual capabilities ($+10.6\%$
on C-Eval, $+6.8\%$ on CMMLU, and $+30.2\%$ on CMMLU-0shot-CoT). It surpasses
existing methods and achieves SOTA among open-source models tuned from the same
base model, using substantially less data and compute. Crucially, these gains
are realized while preserving strong original capabilities, with minimal
degradation ($<4.3\% \text{on MMLU}$) compared to $>35\%$ in open-source Math
and Coding models. This approach has been successfully deployed in LinkedIn's
GenAI-powered job seeker and Ads unit products.
  To support further research, we release the training and evaluation code
(https://github.com/linkedin/ControlLLM) along with models trained on public
datasets (https://huggingface.co/ControlLLM) to the community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Compute Optimal Inference and Provable Amortisation Gap in Sparse
  Autoencoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.13117v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.13117v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charles O'Neill, Alim Gumran, David Klindt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A recent line of work has shown promise in using sparse autoencoders (SAEs)
to uncover interpretable features in neural network representations. However,
the simple linear-nonlinear encoding mechanism in SAEs limits their ability to
perform accurate sparse inference. Using compressed sensing theory, we prove
that an SAE encoder is inherently insufficient for accurate sparse inference,
even in solvable cases. We then decouple encoding and decoding processes to
empirically explore conditions where more sophisticated sparse inference
methods outperform traditional SAE encoders. Our results reveal substantial
performance gains with minimal compute increases in correct inference of sparse
codes. We demonstrate this generalises to SAEs applied to large language
models, where more expressive encoders achieve greater interpretability. This
work opens new avenues for understanding neural network representations and
analysing large language model activations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Predictions in Neural ODEs: Identification and Interventions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.12430v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.12430v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hananeh Aliee, Fabian J. Theis, Niki Kilbertus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spurred by tremendous success in pattern matching and prediction tasks,
researchers increasingly resort to machine learning to aid original scientific
discovery. Given large amounts of observational data about a system, can we
uncover the rules that govern its evolution? Solving this task holds the great
promise of fully understanding the causal interactions and being able to make
reliable predictions about the system's behavior under interventions. We take a
step towards answering this question for time-series data generated from
systems of ordinary differential equations (ODEs). While the governing ODEs
might not be identifiable from data alone, we show that combining simple
regularization schemes with flexible neural ODEs can robustly recover the
dynamics and causal structures from time-series data. Our results on a variety
of (non)-linear first and second order systems as well as real data validate
our method. We conclude by showing that we can also make accurate predictions
under interventions on variables or the system itself.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revisiting LocalSGD and SCAFFOLD: Improved Rates and Missing Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.04443v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.04443v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruichen Luo, Sebastian U Stich, Samuel Horváth, Martin Takáč
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LocalSGD and SCAFFOLD are widely used methods in distributed stochastic
optimization, with numerous applications in machine learning, large-scale data
processing, and federated learning. However, rigorously establishing their
theoretical advantages over simpler methods, such as minibatch SGD (MbSGD), has
proven challenging, as existing analyses often rely on strong assumptions,
unrealistic premises, or overly restrictive scenarios.
  In this work, we revisit the convergence properties of LocalSGD and SCAFFOLD
under a variety of existing or weaker conditions, including gradient
similarity, Hessian similarity, weak convexity, and Lipschitz continuity of the
Hessian. Our analysis shows that (i) LocalSGD achieves faster convergence
compared to MbSGD for weakly convex functions without requiring stronger
gradient similarity assumptions; (ii) LocalSGD benefits significantly from
higher-order similarity and smoothness; and (iii) SCAFFOLD demonstrates faster
convergence than MbSGD for a broader class of non-quadratic functions. These
theoretical insights provide a clearer understanding of the conditions under
which LocalSGD and SCAFFOLD outperform MbSGD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bayesian Flow Is All You Need to Sample Out-of-Distribution Chemical
  Spaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.11439v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.11439v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nianze Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating novel molecules with higher properties than the training space,
namely the out-of-distribution generation, is important for ${de~novo}$ drug
design. However, it is not easy for distribution learning-based models, for
example diffusion models, to solve this challenge as these methods are designed
to fit the distribution of training data as close as possible. In this paper,
we show that Bayesian flow network is capable of effortlessly generating high
quality out-of-distribution samples that meet several scenarios. We introduce a
semi-autoregressive training/sampling method that helps to enhance the model
performance and surpass the state-of-the-art models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 10 figures, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Methods for Non-stationary Online Learning <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.08911v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.08911v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Zhao, Yan-Feng Xie, Lijun Zhang, Zhi-Hua Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Non-stationary online learning has drawn much attention in recent years. In
particular, dynamic regret and adaptive regret are proposed as two principled
performance measures for online convex optimization in non-stationary
environments. To optimize them, a two-layer online ensemble is usually deployed
due to the inherent uncertainty of the non-stationarity, in which a group of
base-learners are maintained and a meta-algorithm is employed to track the best
one on the fly. However, the two-layer structure raises the concern about the
computational complexity -- those methods typically maintain $\mathcal{O}(\log
T)$ base-learners simultaneously for a $T$-round online game and thus perform
multiple projections onto the feasible domain per round, which becomes the
computational bottleneck when the domain is complicated. In this paper, we
first present efficient methods for optimizing dynamic regret and adaptive
regret, which reduce the number of projections per round from $\mathcal{O}(\log
T)$ to $1$. The obtained algorithms require only one gradient query and one
function evaluation at each round. Our technique hinges on the reduction
mechanism developed in parameter-free online learning and requires non-trivial
twists on non-stationary online methods. Furthermore, we study an even
strengthened measure, namely the ``interval dynamic regret'', and reduce the
number of projections per round from $\mathcal{O}(\log^2 T)$ to $1$ to minimize
it. Our reduction demonstrates great generalizability and can be applied to two
important applications: online stochastic control and online principal
component analysis, resulting in methods that are both efficient and optimal.
Finally, empirical studies verify our theoretical findings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preliminary conference version appeared at NeurIPS 2022; this
  extended version improves the paper presentation, further investigates the
  interval dynamic regret, and adds two applications (online non-stochastic
  control and online PCA)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Safety challenges of AI in medicine in the era of large language models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18968v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18968v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoye Wang, Nicole Xi Zhang, Hongyu He, Trang Nguyen, Kun-Hsing Yu, Hao Deng, Cynthia Brandt, Danielle S. Bitterman, Ling Pan, Ching-Yu Cheng, James Zou, Dianbo Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in artificial intelligence (AI), particularly in large
language models (LLMs), have unlocked significant potential to enhance the
quality and efficiency of medical care. By introducing a novel way to interact
with AI and data through natural language, LLMs offer new opportunities for
medical practitioners, patients, and researchers. However, as AI and LLMs
become more powerful and especially achieve superhuman performance in some
medical tasks, public concerns over their safety have intensified. These
concerns about AI safety have emerged as the most significant obstacles to the
adoption of AI in medicine. In response, this review examines emerging risks in
AI utilization during the LLM era. First, we explore LLM-specific safety
challenges from functional and communication perspectives, addressing issues
across data collection, model training, and real-world application. We then
consider inherent safety problems shared by all AI systems, along with
additional complications introduced by LLMs. Last, we discussed how safety
issues of using AI in clinical practice and healthcare system operation would
undermine trust among patient, clinicians and the public, and how to build
confidence in these systems. By emphasizing the development of safe AI, we
believe these technologies can be more rapidly and reliably integrated into
everyday medical practice to benefit both patients and clinicians.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond the Veil of Similarity: Quantifying Semantic Continuity in
  Explainable AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12950v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12950v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Huang, Emanuele Mezzi, Osman Mutlu, Miltiadis Kofinas, Vidya Prasad, Shadnan Azwad Khan, Elena Ranguelova, Niki van Stein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel metric for measuring semantic continuity in Explainable
AI methods and machine learning models. We posit that for models to be truly
interpretable and trustworthy, similar inputs should yield similar
explanations, reflecting a consistent semantic understanding. By leveraging XAI
techniques, we assess semantic continuity in the task of image recognition. We
conduct experiments to observe how incremental changes in input affect the
explanations provided by different XAI methods. Through this approach, we aim
to evaluate the models' capability to generalize and abstract semantic concepts
accurately and to evaluate different XAI methods in correctly capturing the
model behaviour. This paper contributes to the broader discourse on AI
interpretability by proposing a quantitative measure for semantic continuity
for XAI methods, offering insights into the models' and explainers' internal
reasoning processes, and promoting more reliable and transparent AI systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, accepted at the world conference of explainable AI, 2024,
  Malta</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bounded Rationality Equilibrium Learning in Mean Field Games <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07099v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07099v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yannick Eich, Christian Fabian, Kai Cui, Heinz Koeppl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mean field games (MFGs) tractably model behavior in large agent populations.
The literature on learning MFG equilibria typically focuses on finding Nash
equilibria (NE), which assume perfectly rational agents and are hence
implausible in many realistic situations. To overcome these limitations, we
incorporate bounded rationality into MFGs by leveraging the well-known concept
of quantal response equilibria (QRE). Two novel types of MFG QRE enable the
modeling of large agent populations where individuals only noisily estimate the
true objective. We also introduce a second source of bounded rationality to
MFGs by restricting agents' planning horizon. The resulting novel receding
horizon (RH) MFGs are combined with QRE and existing approaches to model
different aspects of bounded rationality in MFGs. We formally define MFG QRE
and RH MFGs and compare them to existing equilibrium concepts such as
entropy-regularized NE. Subsequently, we design generalized fixed point
iteration and fictitious play algorithms to learn QRE and RH equilibria. After
a theoretical analysis, we give different examples to evaluate the capabilities
of our learning algorithms and outline practical differences between the
equilibrium concepts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Hyperedge Prediction with Context-Aware <span class="highlight-title">Self-Supervised</span>
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.05798v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.05798v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunyong Ko, Hanghang Tong, Sang-Wook Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hypergraphs can naturally model group-wise relations (e.g., a group of users
who co-purchase an item) as hyperedges. Hyperedge prediction is to predict
future or unobserved hyperedges, which is a fundamental task in many real-world
applications (e.g., group recommendation). Despite the recent breakthrough of
hyperedge prediction methods, the following challenges have been rarely
studied: (C1) How to aggregate the nodes in each hyperedge candidate for
accurate hyperedge prediction? and (C2) How to mitigate the inherent data
sparsity problem in hyperedge prediction? To tackle both challenges together,
in this paper, we propose a novel hyperedge prediction framework (CASH) that
employs (1) context-aware node aggregation to precisely capture complex
relations among nodes in each hyperedge for (C1) and (2) self-supervised
contrastive learning in the context of hyperedge prediction to enhance
hypergraph representations for (C2). Furthermore, as for (C2), we propose a
hyperedge-aware augmentation method to fully exploit the latent semantics
behind the original hypergraph and consider both node-level and group-level
contrasts (i.e., dual contrasts) for better node and hyperedge representations.
Extensive experiments on six real-world hypergraphs reveal that CASH
consistently outperforms all competing methods in terms of the accuracy in
hyperedge prediction and each of the proposed strategies is effective in
improving the model accuracy of CASH. For the detailed information of CASH, we
provide the code and datasets at: https://github.com/yy-ko/cash.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 7 figures, 4 tables, accepted in IEEE TKDE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Transfer Learning in $\ell_1$ Regularized Regression: Hyperparameter
  Selection Strategy based on Sharp Asymptotic Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17704v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17704v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Koki Okajima, Tomoyuki Obuchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transfer learning techniques aim to leverage information from multiple
related datasets to enhance prediction quality against a target dataset. Such
methods have been adopted in the context of high-dimensional sparse regression,
and some Lasso-based algorithms have been invented: Trans-Lasso and Pretraining
Lasso are such examples. These algorithms require the statistician to select
hyperparameters that control the extent and type of information transfer from
related datasets. However, selection strategies for these hyperparameters, as
well as the impact of these choices on the algorithm's performance, have been
largely unexplored. To address this, we conduct a thorough, precise study of
the algorithm in a high-dimensional setting via an asymptotic analysis using
the replica method. Our approach reveals a surprisingly simple behavior of the
algorithm: Ignoring one of the two types of information transferred to the
fine-tuning stage has little effect on generalization performance, implying
that efforts for hyperparameter selection can be significantly reduced. Our
theoretical findings are also empirically supported by applications on
real-world and semi-artificial datasets using the IMDb and MNIST datasets,
respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HPSCAN: Human Perception-Based Scattered Data Clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.14185v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.14185v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastian Hartwig, Christian van Onzenoodt, Dominik Engel, Pedro Hermosilla, Timo Ropinski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cluster separation is a task typically tackled by widely used clustering
techniques, such as k-means or DBSCAN. However, these algorithms are based on
non-perceptual metrics, and our experiments demonstrate that their output does
not reflect human cluster perception. To bridge the gap between human cluster
perception and machine-computed clusters, we propose HPSCAN, a learning
strategy that operates directly on scattered data. To learn perceptual cluster
separation on such data, we crowdsourced the labeling of 7,320 bivariate
(scatterplot) datasets to 384 human participants. We train our HPSCAN model on
these human-annotated data. Instead of rendering these data as scatterplot
images, we used their x and y point coordinates as input to a modified
PointNet++ architecture, enabling direct inference on point clouds. In this
work, we provide details on how we collected our dataset, report statistics of
the resulting annotations, and investigate the perceptual agreement of cluster
separation for real-world data. We also report the training and evaluation
protocol for HPSCAN and introduce a novel metric, that measures the accuracy
between a clustering technique and a group of human annotators. We explore
predicting point-wise human agreement to detect ambiguities. Finally, we
compare our approach to ten established clustering techniques and demonstrate
that HPSCAN is capable of generalizing to unseen and out-of-scope data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Currently, this manuscript is under revision at CGF</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LMFusion: Adapting <span class="highlight-title">Pretrain</span>ed Language Models for Multimodal Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15188v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15188v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weijia Shi, Xiaochuang Han, Chunting Zhou, Weixin Liang, Xi Victoria Lin, Luke Zettlemoyer, Lili Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present LMFusion, a framework for empowering pretrained text-only large
language models (LLMs) with multimodal generative capabilities, enabling them
to understand and generate both text and images in arbitrary sequences.
LMFusion leverages existing Llama-3's weights for processing texts
autoregressively while introducing additional and parallel transformer modules
for processing images with diffusion. During training, the data from each
modality is routed to its dedicated modules: modality-specific feedforward
layers, query-key-value projections, and normalization layers process each
modality independently, while the shared self-attention layers allow
interactions across text and image features. By freezing the text-specific
modules and only training the image-specific modules, LMFusion preserves the
language capabilities of text-only LLMs while developing strong visual
understanding and generation abilities. Compared to methods that pretrain
multimodal generative models from scratch, our experiments demonstrate that,
LMFusion improves image understanding by 20% and image generation by 3.6% using
only 50% of the FLOPs while maintaining Llama-3's language capabilities. We
also demonstrate that this framework can adapt existing vision-language models
with multimodal generation ability. Overall, this framework not only leverages
existing computational investments in text-only LLMs but also enables the
parallel development of language and vision capabilities, presenting a
promising direction for efficient multimodal model development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Name change: LlamaFusion to LMFusion</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SpectralKD: A Unified Framework for Interpreting and Distilling Vision
  <span class="highlight-title">Transformer</span>s via Spectral Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19055v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19055v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huiyuan Tian, Bonan Xu, Shijian Li, Gang Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge Distillation (KD) has achieved widespread success in compressing
large Vision Transformers (ViTs), but a unified theoretical framework for both
ViTs and KD is still lacking. In this paper, we propose SpectralKD, a novel
unified analytical framework that offers deeper insights into ViTs and
optimizes KD via spectral analysis. Our model-wise analysis reveals that CaiT
concentrates information in their first and last few layers, informing optimal
layer selection for KD. Surprisingly, our layer-wise analysis discovers that
Swin Transformer and CaiT exhibit similar spectral encoding patterns despite
their architectural differences, leading to feature map alignment guideline.
Building on these insights, we propose a simple yet effective spectral
alignment method for KD. Benefiting from the deeper understanding by above
analysis results, even such a simple strategy achieves state-of-the-art
performance on ImageNet-1K without introducing any trainable parameters,
improving DeiT-Tiny by $+5.2\%$ and Swin-Tiny by $+1.4\%$ in top-1 accuracy.
Furthermore, our post-training analysis reveals that distilled students can
reproduce spectral patterns similar to their teachers, opening a new area we
term ``distillation dynamics". Code and experimental logs are available in
https://github.com/thy960112/SpectralKD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Guidance for Local Training in Heterogeneous Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06490v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06490v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianqing Zhang, Yang Liu, Yang Hua, Jian Cao, Qiang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model heterogeneity poses a significant challenge in Heterogeneous Federated
Learning (HtFL). In scenarios with diverse model architectures, directly
aggregating model parameters is impractical, leading HtFL methods to
incorporate an extra objective alongside the original local objective on each
client to facilitate collaboration. However, this often results in a mismatch
between the extra and local objectives. To resolve this, we propose Federated
Learning-to-Guide (FedL2G), a method that adaptively learns to guide local
training in a federated manner, ensuring the added objective aligns with each
client's original goal. With theoretical guarantees, FedL2G utilizes only
first-order derivatives w.r.t. model parameters, achieving a non-convex
convergence rate of O(1/T). We conduct extensive experiments across two data
heterogeneity and six model heterogeneity settings, using 14 heterogeneous
model architectures (e.g., CNNs and ViTs). The results show that FedL2G
significantly outperforms seven state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fitting Multiple Machine Learning Models with Performance Based
  Clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06572v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06572v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehmet Efe Lorasdagi, Ahmet Berker Koc, Ali Taha Koc, Suleyman Serdar Kozat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional machine learning approaches assume that data comes from a single
generating mechanism, which may not hold for most real life data. In these
cases, the single mechanism assumption can result in suboptimal performance. We
introduce a clustering framework that eliminates this assumption by grouping
the data according to the relations between the features and the target values
and we obtain multiple separate models to learn different parts of the data. We
further extend our framework to applications having streaming data where we
produce outcomes using an ensemble of models. For this, the ensemble weights
are updated based on the incoming data batches. We demonstrate the performance
of our approach over the widely-studied real life datasets, showing significant
improvements over the traditional single-model approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stronger Than You Think: Benchmarking Weak Supervision on Realistic
  Tasks <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07727v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07727v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyi Zhang, Linrong Cai, Jeffrey Li, Nicholas Roberts, Neel Guha, Jinoh Lee, Frederic Sala
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Weak supervision (WS) is a popular approach for label-efficient learning,
leveraging diverse sources of noisy but inexpensive weak labels to
automatically annotate training data. Despite its wide usage, WS and its
practical value are challenging to benchmark due to the many knobs in its
setup, including: data sources, labeling functions (LFs), aggregation
techniques (called label models), and end model pipelines. Existing evaluation
suites tend to be limited, focusing on particular components or specialized use
cases. Moreover, they often involve simplistic benchmark tasks or de-facto LF
sets that are suboptimally written, producing insights that may not generalize
to real-world settings. We address these limitations by introducing a new
benchmark, BOXWRENCH, designed to more accurately reflect real-world usages of
WS. This benchmark features tasks with (1) higher class cardinality and
imbalance, (2) notable domain expertise requirements, and (3) opportunities to
re-use LFs across parallel multilingual corpora. For all tasks, LFs are written
using a careful procedure aimed at mimicking real-world settings. In contrast
to existing WS benchmarks, we show that supervised learning requires
substantial amounts (1000+) of labeled examples to match WS in many settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 Datasets and Benchmarks Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LemmaHead: RAG Assisted Proof Generation Using Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15797v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15797v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianbo Yang, Mingqi Yang, Hongyi Zhao, Tianshuo Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing the logic necessary to solve mathematical problems or write
mathematical proofs is one of the more difficult objectives for large language
models (LLMS). Currently, the most popular methods in literature consists of
fine-tuning the model on written mathematical content such as academic
publications and textbooks, so that the model can learn to emulate the style of
mathematical writing. In this project, we explore the effectiveness of using
retrieval augmented generation (RAG) to address gaps in the mathematical
reasoning of LLMs. We develop LemmaHead, a RAG knowledge base that supplements
queries to the model with relevant mathematical context, with particular focus
on context from published textbooks. To measure our model's performance in
mathematical reasoning, our testing paradigm focuses on the task of automated
theorem proving via generating proofs to a given mathematical claim in the Lean
formal language.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Privacy Benefits of Redaction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17762v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17762v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vaibhav Gusain, Douglas Leith
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel redaction methodology that can be used to sanitize natural
text data. Our new technique provides better privacy benefits than other state
of the art techniques while maintaining lower redaction levels.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distillation of Discrete Diffusion through Dimensional Correlations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08709v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08709v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Satoshi Hayakawa, Yuhta Takida, Masaaki Imaizumi, Hiromi Wakaki, Yuki Mitsufuji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have demonstrated exceptional performances in various fields
of generative modeling, but suffer from slow sampling speed due to their
iterative nature. While this issue is being addressed in continuous domains,
discrete diffusion models face unique challenges, particularly in capturing
dependencies between elements (e.g., pixel relationships in image, sequential
dependencies in language) mainly due to the computational cost of processing
high-dimensional joint distributions. In this paper, (i) we propose "mixture"
models for discrete diffusion that are capable of treating dimensional
correlations while remaining scalable, and (ii) we provide a set of loss
functions for distilling the iterations of existing models. Two primary
theoretical insights underpin our approach: First, conventional models with
element-wise independence can well approximate the data distribution, but
essentially require many sampling steps. Second, our loss functions enable the
mixture models to distill such many-step conventional models into just a few
steps by learning the dimensional correlations. Our experimental results show
the effectiveness of the proposed method in distilling pretrained discrete
diffusion models across image and language domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>39 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Active Learning For Contextual Linear Optimization: A Margin-Based
  Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.06584v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.06584v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mo Liu, Paul Grigas, Heyuan Liu, Zuo-Jun Max Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We develop the first active learning method for contextual linear
optimization. Specifically, we introduce a label acquisition algorithm that
sequentially decides whether to request the ``labels'' of feature samples from
an unlabeled data stream, where the labels correspond to the coefficients of
the objective in the linear optimization. Our method is the first to be
directly informed by the decision loss induced by the predicted coefficients,
referred to as the Smart Predict-then-Optimize (SPO) loss. Motivated by the
structure of the SPO loss, our algorithm adopts a margin-based criterion
utilizing the concept of distance to degeneracy. In particular, we design an
efficient active learning algorithm with theoretical excess risk (i.e.,
generalization) guarantees. We derive upper bounds on the label complexity,
defined as the number of samples whose labels are acquired to achieve a desired
small level of SPO risk. These bounds show that our algorithm has a much
smaller label complexity than the naive supervised learning approach that
labels all samples, particularly when the SPO loss is minimized directly on the
collected data. To address the discontinuity and nonconvexity of the SPO loss,
we derive label complexity bounds under tractable surrogate loss functions.
Under natural margin conditions, these bounds also outperform naive supervised
learning. Using the SPO+ loss, a specialized surrogate of the SPO loss, we
establish even tighter bounds under separability conditions. Finally, we
present numerical evidence showing the practical value of our algorithms in
settings such as personalized pricing and the shortest path problem.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Complete Chess Games Enable LLM Become A Chess Master <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17186v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17186v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinqi Zhang, Xintian Han, Haolong Li, Kedi Chen, Shaohui Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLM) have shown remarkable abilities in text
generation, question answering, language translation, reasoning and many other
tasks. It continues to advance rapidly and is becoming increasingly influential
in various fields, from technology and business to education and entertainment.
Despite LLM's success in multiple areas, its ability to play abstract games,
such as chess, is underexplored. Chess-playing requires the language models to
output legal and reasonable moves from textual inputs. Here, we propose the
Large language model ChessLLM to play full chess games. We transform the game
into a textual format with the best move represented in the Forsyth-Edwards
Notation. We show that by simply supervised fine-tuning, our model has achieved
a professional-level Elo rating of 1788 in matches against the standard
Elo-rated Stockfish when permitted to sample 10 times. We further show that
data quality is important. Long-round data supervision enjoys a 350 Elo rating
improvement over short-round data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">5</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Inkspire: Supporting Design Exploration with Generative AI through
  Analogical Sketching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18588v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18588v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Chuan-En Lin, Hyeonsu B. Kang, Nikolas Martelaro, Aniket Kittur, Yan-Ying Chen, Matthew K. Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With recent advancements in the capabilities of Text-to-Image (T2I) AI
models, product designers have begun experimenting with them in their work.
However, T2I models struggle to interpret abstract language and the current
user experience of T2I tools can induce design fixation rather than a more
iterative, exploratory process. To address these challenges, we developed
Inkspire, a sketch-driven tool that supports designers in prototyping product
design concepts with analogical inspirations and a complete
sketch-to-design-to-sketch feedback loop. To inform the design of Inkspire, we
conducted an exchange session with designers and distilled design goals for
improving T2I interactions. In a within-subjects study comparing Inkspire to
ControlNet, we found that Inkspire supported designers with more inspiration
and exploration of design ideas, and improved aspects of the co-creative
process by allowing designers to effectively grasp the current state of the AI
to guide it towards novel design intentions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CHI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AGAV-Rater: Adapting Large Multimodal Model for AI-Generated
  Audio-Visual Quality Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18314v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18314v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuqin Cao, Xiongkuo Min, Yixuan Gao, Wei Sun, Guangtao Zhai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many video-to-audio (VTA) methods have been proposed for dubbing silent
AI-generated videos. An efficient quality assessment method for AI-generated
audio-visual content (AGAV) is crucial for ensuring audio-visual quality.
Existing audio-visual quality assessment methods struggle with unique
distortions in AGAVs, such as unrealistic and inconsistent elements. To address
this, we introduce AGAVQA, the first large-scale AGAV quality assessment
dataset, comprising 3,382 AGAVs from 16 VTA methods. AGAVQA includes two
subsets: AGAVQA-MOS, which provides multi-dimensional scores for audio quality,
content consistency, and overall quality, and AGAVQA-Pair, designed for optimal
AGAV pair selection. We further propose AGAV-Rater, a LMM-based model that can
score AGAVs, as well as audio and music generated from text, across multiple
dimensions, and selects the best AGAV generated by VTA methods to present to
the user. AGAV-Rater achieves state-of-the-art performance on AGAVQA,
Text-to-Audio, and Text-to-Music datasets. Subjective tests also confirm that
AGAV-Rater enhances VTA performance and user experience. The project page is
available at https://agav-rater.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Audiovisual Speech Processing via MUTUD: Multimodal Training
  and Unimodal Deployment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18157v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18157v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joanna Hong, Sanjeel Parekh, Honglie Chen, Jacob Donley, Ke Tan, Buye Xu, Anurag Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building reliable speech systems often requires combining multiple
modalities, like audio and visual cues. While such multimodal solutions
frequently lead to improvements in performance and may even be critical in
certain cases, they come with several constraints such as increased sensory
requirements, computational cost, and modality synchronization, to mention a
few. These challenges constrain the direct uses of these multimodal solutions
in real-world applications. In this work, we develop approaches where the
learning happens with all available modalities but the deployment or inference
is done with just one or reduced modalities. To do so, we propose a Multimodal
Training and Unimodal Deployment (MUTUD) framework which includes a Temporally
Aligned Modality feature Estimation (TAME) module that can estimate information
from missing modality using modalities present during inference. This
innovative approach facilitates the integration of information across different
modalities, enhancing the overall inference process by leveraging the strengths
of each modality to compensate for the absence of certain modalities during
inference. We apply MUTUD to various audiovisual speech tasks and show that it
can reduce the performance gap between the multimodal and corresponding
unimodal models to a considerable extent. MUTUD can achieve this while reducing
the model size and compute compared to multimodal models, in some cases by
almost 80%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DreamArtist++: Controllable One-Shot Text-to-Image Generation via
  Positive-Negative Adapter 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.11337v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.11337v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyi Dong, Pengxu Wei, Liang Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-arts text-to-image generation models such as Imagen and Stable
Diffusion Model have succeed remarkable progresses in synthesizing
high-quality, feature-rich images with high resolution guided by human text
prompts. Since certain characteristics of image content \emph{e.g.}, very
specific object entities or styles, are very hard to be accurately described by
text, some example-based image generation approaches have been proposed,
\emph{i.e.} generating new concepts based on absorbing the salient features of
a few input references. Despite of acknowledged successes, these methods have
struggled on accurately capturing the reference examples' characteristics while
keeping diverse and high-quality image generation, particularly in the one-shot
scenario (\emph{i.e.} given only one reference). To tackle this problem, we
propose a simple yet effective framework, namely DreamArtist, which adopts a
novel positive-negative prompt-tuning learning strategy on the pre-trained
diffusion model, and it has shown to well handle the trade-off between the
accurate controllability and fidelity of image generation with only one
reference example. Specifically, our proposed framework incorporates both
positive and negative embeddings or adapters and optimizes them in a joint
manner. The positive part aggressively captures the salient characteristics of
the reference image to drive diversified generation and the negative part
rectifies inadequacies from the positive part. We have conducted extensive
experiments and evaluated the proposed method from image similarity (fidelity)
and diversity, generation controllability, and style cloning. And our
DreamArtist has achieved a superior generation performance over existing
methods. Besides, our additional evaluation on extended tasks, including
concept compositions and prompt-guided image editing, demonstrates its
effectiveness for more applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EmoDubber: Towards High Quality and Emotion Controllable Movie Dubbing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08988v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08988v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gaoxiang Cong, Jiadong Pan, Liang Li, Yuankai Qi, Yuxin Peng, Anton van den Hengel, Jian Yang, Qingming Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given a piece of text, a video clip, and a reference audio, the movie dubbing
task aims to generate speech that aligns with the video while cloning the
desired voice. The existing methods have two primary deficiencies: (1) They
struggle to simultaneously hold audio-visual sync and achieve clear
pronunciation; (2) They lack the capacity to express user-defined emotions. To
address these problems, we propose EmoDubber, an emotion-controllable dubbing
architecture that allows users to specify emotion type and emotional intensity
while satisfying high-quality lip sync and pronunciation. Specifically, we
first design Lip-related Prosody Aligning (LPA), which focuses on learning the
inherent consistency between lip motion and prosody variation by duration level
contrastive learning to incorporate reasonable alignment. Then, we design
Pronunciation Enhancing (PE) strategy to fuse the video-level phoneme sequences
by efficient conformer to improve speech intelligibility. Next, the speaker
identity adapting module aims to decode acoustics prior and inject the speaker
style embedding. After that, the proposed Flow-based User Emotion Controlling
(FUEC) is used to synthesize waveform by flow matching prediction network
conditioned on acoustics prior. In this process, the FUEC determines the
gradient direction and guidance scale based on the user's emotion instructions
by the positive and negative guidance mechanism, which focuses on amplifying
the desired emotion while suppressing others. Extensive experimental results on
three benchmark datasets demonstrate favorable performance compared to several
state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-29T00:00:00Z">2025-01-29</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">65</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From tools to thieves: Measuring and understanding public perceptions of
  AI through crowdsourced metaphors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18045v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18045v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Myra Cheng, Angela Y. Lee, Kristina Rapuano, Kate Niederhoffer, Alex Liebscher, Jeffrey Hancock
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How has the public responded to the increasing prevalence of artificial
intelligence (AI)-based technologies? We investigate public perceptions of AI
by collecting over 12,000 responses over 12 months from a nationally
representative U.S. sample. Participants provided open-ended metaphors
reflecting their mental models of AI, a methodology that overcomes the
limitations of traditional self-reported measures. Using a mixed-methods
approach combining quantitative clustering and qualitative coding, we identify
20 dominant metaphors shaping public understanding of AI. To analyze these
metaphors systematically, we present a scalable framework integrating language
modeling (LM)-based techniques to measure key dimensions of public perception:
anthropomorphism (attribution of human-like qualities), warmth, and competence.
We find that Americans generally view AI as warm and competent, and that over
the past year, perceptions of AI's human-likeness and warmth have significantly
increased ($+34\%, r = 0.80, p < 0.01; +41\%, r = 0.62, p < 0.05$).
Furthermore, these implicit perceptions, along with the identified dominant
metaphors, strongly predict trust in and willingness to adopt AI ($r^2 = 0.21,
0.18, p < 0.001$). We further explore how differences in metaphors and implicit
perceptions--such as the higher propensity of women, older individuals, and
people of color to anthropomorphize AI--shed light on demographic disparities
in trust and adoption. In addition to our dataset and framework for tracking
evolving public attitudes, we provide actionable insights on using metaphors
for inclusive and responsible AI development.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InnerThoughts: Disentangling Representations and Predictions in Large
  Language Models <span class="chip">AISTATS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17994v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17994v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Didier Chételat, Joseph Cotnareanu, Rylee Thompson, Yingxue Zhang, Mark Coates
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) contain substantial factual knowledge which is
commonly elicited by multiple-choice question-answering prompts. Internally,
such models process the prompt through multiple transformer layers, building
varying representations of the problem within its hidden states. Ultimately,
however, only the hidden state corresponding to the final layer and token
position are used to predict the answer label. In this work, we propose instead
to learn a small separate neural network predictor module on a collection of
training questions, that take the hidden states from all the layers at the last
temporal position as input and outputs predictions. In effect, such a framework
disentangles the representational abilities of LLMs from their predictive
abilities. On a collection of hard benchmarks, our method achieves considerable
improvements in performance, sometimes comparable to supervised fine-tuning
procedures, but at a fraction of the computational cost.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AISTATS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dialogue is Better Than Monologue: Instructing Medical LLMs via
  Strategical Conversations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17860v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17860v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijie Liu, Xinyu Zhao, Jie Peng, Zhuangdi Zhu, Qingyu Chen, Xia Hu, Tianlong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current medical AI systems often fail to replicate real-world clinical
reasoning, as they are predominantly trained and evaluated on static text and
question-answer tasks. These tuning methods and benchmarks overlook critical
aspects like evidence-based reasoning and handling distracting information. To
bridge this gap, we introduce a novel benchmark that simulates real-world
diagnostic scenarios, integrating noise and difficulty levels aligned with
USMLE standards. Moreover, we explore dialogue-based fine-tuning, which
transforms static datasets into conversational formats to better capture
iterative reasoning processes. Experiments show that dialogue-tuned models
outperform traditional methods, with improvements of $9.64\%$ in multi-round
reasoning scenarios and $6.18\%$ in accuracy in a noisy environment. Our
findings highlight dialogue tuning as a promising approach for advancing
clinically aligned and robust medical AI systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Your Model Ranking on Chatbot Arena by Vote Rigging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17858v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17858v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Min, Tianyu Pang, Chao Du, Qian Liu, Minhao Cheng, Min Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chatbot Arena is a popular platform for evaluating LLMs by pairwise battles,
where users vote for their preferred response from two randomly sampled
anonymous models. While Chatbot Arena is widely regarded as a reliable LLM
ranking leaderboard, we show that crowdsourced voting can be rigged to improve
(or decrease) the ranking of a target model $m_{t}$. We first introduce a
straightforward target-only rigging strategy that focuses on new battles
involving $m_{t}$, identifying it via watermarking or a binary classifier, and
exclusively voting for $m_{t}$ wins. However, this strategy is practically
inefficient because there are over $190$ models on Chatbot Arena and on average
only about $1\%$ of new battles will involve $m_{t}$. To overcome this, we
propose omnipresent rigging strategies, exploiting the Elo rating mechanism of
Chatbot Arena that any new vote on a battle can influence the ranking of the
target model $m_{t}$, even if $m_{t}$ is not directly involved in the battle.
We conduct experiments on around $1.7$ million historical votes from the
Chatbot Arena Notebook, showing that omnipresent rigging strategies can improve
model rankings by rigging only hundreds of new votes. While we have evaluated
several defense mechanisms, our findings highlight the importance of continued
efforts to prevent vote rigging. Our code is available at
https://github.com/sail-sg/Rigging-ChatbotArena.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Beyond the Surface: How Far Can Continual <span class="highlight-title">Pre-Train</span>ing with
  LoRA Enhance LLMs' Domain-Specific Insight Learning? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17840v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17840v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pouya Pezeshkpour, Estevam Hruschka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated remarkable performance on
various tasks, yet their ability to extract and internalize deeper insights
from domain-specific datasets remains underexplored. In this study, we
investigate how continual pre-training can enhance LLMs' capacity for insight
learning across three distinct forms: declarative, statistical, and
probabilistic insights. Focusing on two critical domains: medicine and finance,
we employ LoRA to train LLMs on two existing datasets. To evaluate each insight
type, we create benchmarks to measure how well continual pre-training helps
models go beyond surface-level knowledge. We also assess the impact of document
modification on capturing insights. The results show that, while continual
pre-training on original documents has a marginal effect, modifying documents
to retain only essential information significantly enhances the
insight-learning capabilities of LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comprehensive <span class="highlight-title">Survey</span> on Legal Summarization: Challenges and Future
  Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17830v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17830v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mousumi Akter, Erion Çano, Erik Weber, Dennis Dobler, Ivan Habernal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article provides a systematic up-to-date survey of automatic
summarization techniques, datasets, models, and evaluation methods in the legal
domain. Through specific source selection criteria, we thoroughly review over
120 papers spanning the modern `transformer' era of natural language processing
(NLP), thus filling a gap in existing systematic surveys on the matter. We
present existing research along several axes and discuss trends, challenges,
and opportunities for future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Janus-Pro: Unified Multimodal Understanding and Generation with Data and
  Model Scaling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17811v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17811v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we introduce Janus-Pro, an advanced version of the previous
work Janus. Specifically, Janus-Pro incorporates (1) an optimized training
strategy, (2) expanded training data, and (3) scaling to larger model size.
With these improvements, Janus-Pro achieves significant advancements in both
multimodal understanding and text-to-image instruction-following capabilities,
while also enhancing the stability of text-to-image generation. We hope this
work will inspire further exploration in the field. Code and models are
publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Research paper. arXiv admin note: text overlap with arXiv:2410.13848</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BreezyVoice: Adapting TTS for Taiwanese Mandarin with Enhanced Polyphone
  Disambiguation -- Challenges and Insights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17790v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17790v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chan-Jan Hsu, Yi-Cheng Lin, Chia-Chun Lin, Wei-Chih Chen, Ho Lam Chung, Chen-An Li, Yi-Chang Chen, Chien-Yu Yu, Ming-Ji Lee, Chien-Cheng Chen, Ru-Heng Huang, Hung-yi Lee, Da-Shan Shiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present BreezyVoice, a Text-to-Speech (TTS) system specifically adapted
for Taiwanese Mandarin, highlighting phonetic control abilities to address the
unique challenges of polyphone disambiguation in the language. Building upon
CosyVoice, we incorporate a $S^{3}$ tokenizer, a large language model (LLM), an
optimal-transport conditional flow matching model (OT-CFM), and a grapheme to
phoneme prediction model, to generate realistic speech that closely mimics
human utterances. Our evaluation demonstrates BreezyVoice's superior
performance in both general and code-switching contexts, highlighting its
robustness and effectiveness in generating high-fidelity speech. Additionally,
we address the challenges of generalizability in modeling long-tail speakers
and polyphone disambiguation. Our approach significantly enhances performance
and offers valuable insights into the workings of neural codec TTS systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reasoning Over the Glyphs: Evaluation of LLM's Decipherment of Rare
  Scripts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17785v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17785v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu-Fei Shih, Zheng-Lin Lin, Shu-Kai Hsieh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We explore the capabilities of LVLMs and LLMs in deciphering rare scripts not
encoded in Unicode. We introduce a novel approach to construct a multimodal
dataset of linguistic puzzles involving such scripts, utilizing a tokenization
method for language glyphs. Our methods include the Picture Method for LVLMs
and the Description Method for LLMs, enabling these models to tackle these
challenges. We conduct experiments using prominent models, GPT-4o, Gemini, and
Claude 3.5 Sonnet, on linguistic puzzles. Our findings reveal the strengths and
limitations of current AI methods in linguistic decipherment, highlighting the
impact of Unicode encoding on model performance and the challenges of modeling
visual language tokens through descriptions. Our study advances understanding
of AI's potential in linguistic decipherment and underscores the need for
further research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 2SSP: A Two-Stage Framework for Structured Pruning of LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17771v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17771v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabrizio Sandri, Elia Cunegatti, Giovanni Iacca
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel Two-Stage framework for Structured Pruning (2SSP) for
pruning Large Language Models (LLMs), which combines two different strategies
of pruning, namely Width and Depth Pruning. The first stage (Width Pruning)
removes entire neurons, hence their corresponding rows and columns, aiming to
preserve the connectivity among the pruned structures in the intermediate state
of the Feed-Forward Networks in each Transformer block. This is done based on
an importance score measuring the impact of each neuron over the output
magnitude. The second stage (Depth Pruning), instead, removes entire Attention
submodules. This is done by applying an iterative process that removes the
Attention submodules with the minimum impact on a given metric of interest (in
our case, perplexity). We also propose a novel mechanism to balance the
sparsity rate of the two stages w.r.t. to the desired global sparsity. We test
2SSP on four LLM families and three sparsity rates (25\%, 37.5\%, and 50\%),
measuring the resulting perplexity over three language modeling datasets as
well as the performance over six downstream tasks. Our method consistently
outperforms five state-of-the-art competitors over three language modeling and
six downstream tasks, with an up to two-order-of-magnitude gain in terms of
pruning time. The code is available at available at
\url{https://github.com/FabrizioSandri/2SSP}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hybrid Graphs for Table-and-Text based Question Answering using LLMs <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17767v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17767v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ankush Agarwal, Ganesh S, Chaitanya Devaguptapu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Answering questions that require reasoning and aggregation across both
structured (tables) and unstructured (raw text) data sources presents
significant challenges. Current methods rely on fine-tuning and high-quality,
human-curated data, which is difficult to obtain. Recent advances in Large
Language Models (LLMs) have shown promising results for multi-hop question
answering (QA) over single-source text data in a zero-shot setting, yet
exploration into multi-source Table-Text QA remains limited. In this paper, we
present a novel Hybrid Graph-based approach for Table-Text QA that leverages
LLMs without fine-tuning. Our method constructs a unified Hybrid Graph from
textual and tabular data, pruning information based on the input question to
provide the LLM with relevant context concisely. We evaluate our approach on
the challenging Hybrid-QA and OTT-QA datasets using state-of-the-art LLMs,
including GPT-3.5, GPT-4, and LLaMA-3. Our method achieves the best zero-shot
performance on both datasets, improving Exact Match scores by up to 10% on
Hybrid-QA and 5.4% on OTT-QA. Moreover, our approach reduces token usage by up
to 53% compared to the original context.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NAACL 2025 Main Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VICCA: Visual Interpretation and Comprehension of Chest X-ray Anomalies
  in Generated Report Without Human Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17726v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17726v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sayeh Gholipour Picha, Dawood Al Chanti, Alice Caplier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As artificial intelligence (AI) becomes increasingly central to healthcare,
the demand for explainable and trustworthy models is paramount. Current report
generation systems for chest X-rays (CXR) often lack mechanisms for validating
outputs without expert oversight, raising concerns about reliability and
interpretability. To address these challenges, we propose a novel multimodal
framework designed to enhance the semantic alignment and localization accuracy
of AI-generated medical reports. Our framework integrates two key modules: a
Phrase Grounding Model, which identifies and localizes pathologies in CXR
images based on textual prompts, and a Text-to-Image Diffusion Module, which
generates synthetic CXR images from prompts while preserving anatomical
fidelity. By comparing features between the original and generated images, we
introduce a dual-scoring system: one score quantifies localization accuracy,
while the other evaluates semantic consistency. This approach significantly
outperforms existing methods, achieving state-of-the-art results in pathology
localization and text-to-image alignment. The integration of phrase grounding
with diffusion models, coupled with the dual-scoring evaluation system,
provides a robust mechanism for validating report quality, paving the way for
more trustworthy and transparent AI in medical imaging.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Using Code Generation to Solve Open Instances of Combinatorial Design
  Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17725v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17725v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher D. Rosin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Handbook of Combinatorial Designs catalogs many types of combinatorial
designs, together with lists of open instances for which existence has not yet
been determined. We develop a constructive protocol CPro1, which uses Large
Language Models (LLMs) to generate code that constructs combinatorial designs
and resolves some of these open instances. The protocol starts from a
definition of a particular type of design, and a verifier that reliably
confirms whether a proposed design is valid. The LLM selects strategies and
implements them in code, and scaffolding provides automated hyperparameter
tuning and execution feedback using the verifier. Most generated code fails,
but by generating many candidates, the protocol automates exploration of a
variety of standard methods (e.g. simulated annealing, genetic algorithms) and
experimentation with variations (e.g. cost functions) to find successful
approaches. Testing on 16 different types of designs, CPro1 constructs
solutions to open instances for 6 of them: Symmetric and Skew Weighing
Matrices, Equidistant Permutation Arrays, Packing Arrays, Balanced Ternary
Designs, and Florentine Rectangles.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RICoTA: Red-teaming of In-the-wild Conversation with Test Attempts <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17715v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17715v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eujeong Choi, Younghun Jeong, Soomin Kim, Won Ik Cho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  User interactions with conversational agents (CAs) evolve in the era of
heavily guardrailed large language models (LLMs). As users push beyond
programmed boundaries to explore and build relationships with these systems,
there is a growing concern regarding the potential for unauthorized access or
manipulation, commonly referred to as "jailbreaking." Moreover, with CAs that
possess highly human-like qualities, users show a tendency toward initiating
intimate sexual interactions or attempting to tame their chatbots. To capture
and reflect these in-the-wild interactions into chatbot designs, we propose
RICoTA, a Korean red teaming dataset that consists of 609 prompts challenging
LLMs with in-the-wild user-made dialogues capturing jailbreak attempts. We
utilize user-chatbot conversations that were self-posted on a Korean
Reddit-like community, containing specific testing and gaming intentions with a
social chatbot. With these prompts, we aim to evaluate LLMs' ability to
identify the type of conversation and users' testing purposes to derive chatbot
design implications for mitigating jailbreaking risks. Our dataset will be made
publicly available via GitHub.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>PACLIC 38</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DReSS: Data-driven Regularized Structured Streamlining for Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17905v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17905v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingkuan Feng, Jinyang Wu, Shuai Zhang, Pengpeng Shao, Ruihan Jin, Zhengqi Wen, Jianhua Tao, Feihu Che
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have achieved significant progress across
various domains, but their increasing scale results in high computational and
memory costs. Recent studies have revealed that LLMs exhibit sparsity,
providing the potential to reduce model size through pruning techniques.
However, existing pruning methods typically follow a prune-then-finetune
paradigm. Since the pruned components still contain valuable information, their
direct removal often leads to irreversible performance degradation, imposing a
substantial computational burden to recover performance during finetuning. In
this paper, we propose a novel paradigm that first applies regularization, then
prunes, and finally finetunes. Based on this paradigm, we introduce DReSS, a
simple and effective Data-driven Regularized Structured Streamlining method for
LLMs. By leveraging a small amount of data to regularize the components to be
pruned, DReSS explicitly transfers the important information to the remaining
parts of the model in advance. Compared to direct pruning, this can reduce the
information loss caused by parameter removal, thereby enhancing its language
modeling capabilities. Experimental results demonstrate that DReSS
significantly outperforms existing pruning methods even under extreme pruning
ratios, significantly reducing latency and increasing throughput.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Vision Language Models for Multimodal and Multilingual Stance
  Detection <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17654v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17654v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jake Vasilakes, Carolina Scarton, Zhixue Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Social media's global reach amplifies the spread of information, highlighting
the need for robust Natural Language Processing tasks like stance detection
across languages and modalities. Prior research predominantly focuses on
text-only inputs, leaving multimodal scenarios, such as those involving both
images and text, relatively underexplored. Meanwhile, the prevalence of
multimodal posts has increased significantly in recent years. Although
state-of-the-art Vision-Language Models (VLMs) show promise, their performance
on multimodal and multilingual stance detection tasks remains largely
unexamined. This paper evaluates state-of-the-art VLMs on a newly extended
dataset covering seven languages and multimodal inputs, investigating their use
of visual cues, language-specific performance, and cross-modality interactions.
Our results show that VLMs generally rely more on text than images for stance
detection and this trend persists across languages. Additionally, VLMs rely
significantly more on text contained within the images than other visual
content. Regarding multilinguality, the models studied tend to generate
consistent predictions across languages whether they are explicitly
multilingual or not, although there are outliers that are incongruous with
macro F1, language support, and model size.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to the International AAAI Conference on Web and Social
  Media (ICWSM) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tonguescape: Exploring Language Models Understanding of Vowel
  Articulation <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17643v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17643v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haruki Sakajo, Yusuke Sakai, Hidetaka Kamigaito, Taro Watanabe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vowels are primarily characterized by tongue position. Humans have discovered
these features of vowel articulation through their own experience and explicit
objective observation such as using MRI. With this knowledge and our
experience, we can explain and understand the relationship between tongue
positions and vowels, and this knowledge is helpful for language learners to
learn pronunciation. Since language models (LMs) are trained on a large amount
of data that includes linguistic and medical fields, our preliminary studies
indicate that an LM is able to explain the pronunciation mechanisms of vowels.
However, it is unclear whether multi-modal LMs, such as vision LMs, align
textual information with visual information. One question arises: do LMs
associate real tongue positions with vowel articulation? In this study, we
created video and image datasets from the existing real-time MRI dataset and
investigated whether LMs can understand vowel articulation based on tongue
positions using vision-based information. Our findings suggest that LMs exhibit
potential for understanding vowels and tongue positions when reference examples
are provided while they have difficulties without them. Our code for dataset
building is available on GitHub.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncertainty Quantification and Decomposition for LLM-based
  Recommendation <span class="chip">WWW 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17630v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17630v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wonbin Kweon, Sanghwan Jang, SeongKu Kang, Hwanjo Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the widespread adoption of large language models (LLMs) for
recommendation, we demonstrate that LLMs often exhibit uncertainty in their
recommendations. To ensure the trustworthy use of LLMs in generating
recommendations, we emphasize the importance of assessing the reliability of
recommendations generated by LLMs. We start by introducing a novel framework
for estimating the predictive uncertainty to quantitatively measure the
reliability of LLM-based recommendations. We further propose to decompose the
predictive uncertainty into recommendation uncertainty and prompt uncertainty,
enabling in-depth analyses of the primary source of uncertainty. Through
extensive experiments, we (1) demonstrate predictive uncertainty effectively
indicates the reliability of LLM-based recommendations, (2) investigate the
origins of uncertainty with decomposed uncertainty measures, and (3) propose
uncertainty-aware prompting for a lower predictive uncertainty and enhanced
recommendation. Our source code and model weights are available at
https://github.com/WonbinKweon/UNC_LLM_REC_WWW2025
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WWW 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Structured Context Recomposition for Large Language Models Using
  Probabilistic Layer Realignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17617v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17617v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Teel, Jocasta Cumberbatch, Raphael Benington, Quentin Baskerville
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extended sequence generation often leads to degradation in contextual
consistency due to the inability of conventional self-attention mechanisms to
effectively retain long-range dependencies. Existing approaches, including
memory compression and retrieval-augmented conditioning, introduce
computational trade-offs that either increase inference latency or impose
additional storage overhead. Structured Context Recomposition (SCR) introduces
a probabilistic layer realignment strategy that dynamically adjusts learned
representations within transformer layers, ensuring that semantically relevant
embeddings persist throughout extended transformations. The proposed method
enhances coherence retention through a recursive weighting function that
redistributes representational emphasis based on inferred contextual relevance
rather than relying on fixed token-level attention scores. Empirical results
indicate that probabilistic realignment mitigates abrupt topic shifts and
logical inconsistencies, particularly in scenarios where sequences exceed
standard attention window constraints. Sequence-level entropy analysis further
reveals that SCR moderates representational variability without introducing
excessive output regularization, allowing models to sustain generative
diversity while preserving contextual alignment. Attention head deviation
measurements confirm that hierarchical reweighting contributes to smoother
token dependency transitions across transformer layers, reinforcing the
stability of multi-turn interactions and document-level reasoning.
Computational resource assessments show that while SCR incurs a moderate
increase in processing time, memory overhead remains within feasible limits,
making it suitable for practical deployment in autoregressive generative
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cross-lingual Embedding Clustering for Hierarchical Softmax in
  Low-Resource Multilingual Speech Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17615v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17615v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengdong Yang, Qianying Liu, Sheng Li, Fei Cheng, Chenhui Chu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel approach centered on the decoding stage of Automatic
Speech Recognition (ASR) that enhances multilingual performance, especially for
low-resource languages. It utilizes a cross-lingual embedding clustering method
to construct a hierarchical Softmax (H-Softmax) decoder, which enables similar
tokens across different languages to share similar decoder representations. It
addresses the limitations of the previous Huffman-based H-Softmax method, which
relied on shallow features in token similarity assessments. Through experiments
on a downsampled dataset of 15 languages, we demonstrate the effectiveness of
our approach in improving low-resource multilingual ASR accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semantic Consistency Regularization with Large Language Models for
  Semi-supervised Sentiment Analysis <span class="chip">ICONIP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17598v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17598v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kunrong Li, Xinyu Liu, Zhen Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate sentiment analysis of texts is crucial for a variety of
applications, such as understanding customer feedback, monitoring market
trends, and detecting public sentiment. However, manually annotating large
sentiment corpora for supervised learning is labor-intensive and
time-consuming. Therefore, it is essential and effective to develop a
semi-supervised method for the sentiment analysis task. Although some methods
have been proposed for semi-supervised text classification, they rely on the
intrinsic information within the unlabeled data and the learning capability of
the NLP model, which lack generalization ability to the sentiment analysis
scenario and may prone to overfit. Inspired by the ability of pretrained Large
Language Models (LLMs) in following instructions and generating coherent text,
we propose a Semantic Consistency Regularization with Large Language Models
(SCR) framework for semi-supervised sentiment analysis. We introduce two
prompting strategies to semantically enhance unlabeled text using LLMs. The
first is Entity-based Enhancement (SCR-EE), which involves extracting entities
and numerical information, and querying the LLM to reconstruct the textual
information. The second is Concept-based Enhancement (SCR-CE), which directly
queries the LLM with the original sentence for semantic reconstruction.
Subsequently, the LLM-augmented data is utilized for a consistency loss with
confidence thresholding, which preserves high-quality agreement samples to
provide additional supervision signals during training. Furthermore, to fully
utilize the uncertain unlabeled data samples, we propose a class re-assembling
strategy inspired by the class space shrinking theorem. Experiments show our
method achieves remarkable performance over prior semi-supervised methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICONIP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GLLM: Self-Corrective G-Code Generation using Large Language Models with
  User Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17584v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17584v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Abdelaal, Samuel Lokadjaja, Gilbert Engert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces GLLM, an innovative tool that leverages Large Language
Models (LLMs) to automatically generate G-code from natural language
instructions for Computer Numerical Control (CNC) machining. GLLM addresses the
challenges of manual G-code writing by bridging the gap between human-readable
task descriptions and machine-executable code. The system incorporates a
fine-tuned StarCoder-3B model, enhanced with domain-specific training data and
a Retrieval-Augmented Generation (RAG) mechanism. GLLM employs advanced
prompting strategies and a novel self-corrective code generation approach to
ensure both syntactic and semantic correctness of the generated G-code. The
architecture includes robust validation mechanisms, including syntax checks,
G-code-specific verifications, and functional correctness evaluations using
Hausdorff distance. By combining these techniques, GLLM aims to democratize CNC
programming, making it more accessible to users without extensive programming
experience while maintaining high accuracy and reliability in G-code
generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CSEval: Towards Automated, Multi-Dimensional, and Reference-Free
  Counterspeech Evaluation using Auto-Calibrated LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17581v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17581v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amey Hengle, Aswini Kumar, Anil Bandhakavi, Tanmoy Chakraborty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Counterspeech has been popular as an effective approach to counter online
hate speech, leading to increasing research interest in automated counterspeech
generation using language models. However, this field lacks standardised
evaluation protocols and robust automated evaluation metrics that align with
human judgement. Current automatic evaluation methods, primarily based on
similarity metrics, do not effectively capture the complex and independent
attributes of counterspeech quality, such as contextual relevance,
aggressiveness, or argumentative coherence. This has led to an increased
dependency on labor-intensive human evaluations to assess automated
counter-speech generation methods. To address these challenges, we introduce
CSEval, a novel dataset and framework for evaluating counterspeech quality
across four dimensions: contextual-relevance, aggressiveness,
argument-coherence, and suitableness. Furthermore, we propose Auto-Calibrated
COT for Counterspeech Evaluation (ACE), a prompt-based method with
auto-calibrated chain-of-thoughts (CoT) for scoring counterspeech using large
language models. Our experiments show that ACE outperforms traditional metrics
like ROUGE, METEOR, and BertScore in correlating with human judgement,
indicating a significant advancement in automated counterspeech evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 5 figures. arXiv admin note: text overlap with
  arXiv:2309.13308 by other authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A linguistically-motivated evaluation methodology for unraveling model's
  abilities in reading comprehension tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17569v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17569v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elie Antoine, Frédéric Béchet, Géraldine Damnati, Philippe Langlais
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce an evaluation methodology for reading comprehension tasks based
on the intuition that certain examples, by the virtue of their linguistic
complexity, consistently yield lower scores regardless of model size or
architecture. We capitalize on semantic frame annotation for characterizing
this complexity, and study seven complexity factors that may account for
model's difficulty. We first deploy this methodology on a carefully annotated
French reading comprehension benchmark showing that two of those complexity
factors are indeed good predictors of models' failure, while others are less
so. We further deploy our methodology on a well studied English benchmark by
using Chat-GPT as a proxy for semantic annotation. Our study reveals that
fine-grained linguisticallymotivated automatic evaluation of a reading
comprehension task is not only possible, but helps understand models' abilities
to handle specific linguistic characteristics of input examples. It also shows
that current state-of-the-art models fail with some for those characteristics
which suggests that adequately handling them requires more than merely
increasing model size.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Query-Aware Learnable Graph Pooling Tokens as <span class="highlight-title">Prompt</span> for Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17549v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17549v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wooyoung Kim, Byungyoon Park, Wooju Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph-structured data plays a vital role in numerous domains, such as social
networks, citation networks, commonsense reasoning graphs and knowledge graphs.
While graph neural networks have been employed for graph processing, recent
advancements have explored integrating large language models for graph-based
tasks. In this paper, we propose a novel approach named Learnable Graph Pooling
Token (LGPT), which addresses the limitations of the scalability issues in
node-level projection and information loss in graph-level projection. LGPT
enables flexible and efficient graph representation by introducing learnable
parameters that act as tokens in large language models, balancing fine-grained
and global graph information. Additionally, we investigate an Early Query
Fusion technique, which fuses query context before constructing the graph
representation, leading to more effective graph embeddings. Our method achieves
a 4.13\% performance improvement on the GraphQA benchmark without training the
large language model, demonstrating significant gains in handling complex
textual-attributed graph data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM Assistance for Pediatric Depression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17510v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17510v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mariia Ignashina, Paulina Bondaronek, Dan Santel, John Pestian, Julia Ive
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional depression screening methods, such as the PHQ-9, are particularly
challenging for children in pediatric primary care due to practical
limitations. AI has the potential to help, but the scarcity of annotated
datasets in mental health, combined with the computational costs of training,
highlights the need for efficient, zero-shot approaches. In this work, we
investigate the feasibility of state-of-the-art LLMs for depressive symptom
extraction in pediatric settings (ages 6-24). This approach aims to complement
traditional screening and minimize diagnostic errors.
  Our findings show that all LLMs are 60% more efficient than word match, with
Flan leading in precision (average F1: 0.65, precision: 0.78), excelling in the
extraction of more rare symptoms like "sleep problems" (F1: 0.92) and
"self-loathing" (F1: 0.8). Phi strikes a balance between precision (0.44) and
recall (0.60), performing well in categories like "Feeling depressed" (0.69)
and "Weight change" (0.78). Llama 3, with the highest recall (0.90),
overgeneralizes symptoms, making it less suitable for this type of analysis.
Challenges include the complexity of clinical notes and overgeneralization from
PHQ-9 scores. The main challenges faced by LLMs include navigating the complex
structure of clinical notes with content from different times in the patient
trajectory, as well as misinterpreting elevated PHQ-9 scores.
  We finally demonstrate the utility of symptom annotations provided by Flan as
features in an ML algorithm, which differentiates depression cases from
controls with high precision of 0.78, showing a major performance boost
compared to a baseline that does not use these features.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DINT <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17486v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17486v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yueyang Cang, Yuhang Liu, Xiaoteng Zhang, Erlu Zhao, Li Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  DIFF Transformer addresses the issue of irrelevant context interference by
introducing a differential attention mechanism that enhances the robustness of
local attention. However, it has two critical limitations: the lack of global
context modeling, which is essential for identifying globally significant
tokens, and numerical instability due to the absence of strict row
normalization in the attention matrix. To overcome these challenges, we propose
DINT Transformer, which extends DIFF Transformer by incorporating a
differential-integral mechanism. By computing global importance scores and
integrating them into the attention matrix, DINT Transformer improves its
ability to capture global dependencies. Moreover, the unified parameter design
enforces row-normalized attention matrices, improving numerical stability.
Experimental results demonstrate that DINT Transformer excels in accuracy and
robustness across various practical applications, such as long-context language
modeling and key information retrieval. These results position DINT Transformer
as a highly effective and promising architecture.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2410.05258 by other authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DFPE: A Diverse Fingerprint Ensemble for Enhancing LLM Performance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17479v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17479v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seffi Cohen, Niv Goldshlager, Nurit Cohen-Inger, Bracha Shapira, Lior Rokach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown remarkable capabilities across
various natural language processing tasks but often struggle to excel uniformly
in diverse or complex domains. We propose a novel ensemble method - Diverse
Fingerprint Ensemble (DFPE), which leverages the complementary strengths of
multiple LLMs to achieve more robust performance. Our approach involves: (1)
clustering models based on response "fingerprints" patterns, (2) applying a
quantile-based filtering mechanism to remove underperforming models at a
per-subject level, and (3) assigning adaptive weights to remaining models based
on their subject-wise validation accuracy. In experiments on the Massive
Multitask Language Understanding (MMLU) benchmark, DFPE outperforms the best
single model by 3% overall accuracy and 5% in discipline-level accuracy. This
method increases the robustness and generalization of LLMs and underscores how
model selection, diversity preservation, and performance-driven weighting can
effectively address challenging, multi-faceted language understanding tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models for Single-Step and Multi-Step Flight Trajectory
  Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17459v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17459v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaiwei Luo, Jiliu Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Flight trajectory prediction is a critical time series task in aviation.
While deep learning methods have shown significant promise, the application of
large language models (LLMs) to this domain remains underexplored. This study
pioneers the use of LLMs for flight trajectory prediction by reframing it as a
language modeling problem. Specifically, We extract features representing the
aircraft's position and status from ADS-B flight data to construct a
prompt-based dataset, where trajectory waypoints are converted into language
tokens. The dataset is then employed to fine-tune LLMs, enabling them to learn
complex spatiotemporal patterns for accurate predictions. Comprehensive
experiments demonstrate that LLMs achieve notable performance improvements in
both single-step and multi-step predictions compared to traditional methods,
with LLaMA-3.1 model achieving the highest overall accuracy. However, the high
inference latency of LLMs poses a challenge for real-time applications,
underscoring the need for further research in this promising direction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">review</span> on the novelty measurements of academic papers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17456v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17456v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Zhao, Chengzhi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Novelty evaluation is vital for the promotion and management of innovation.
With the advancement of information techniques and the open data movement, some
progress has been made in novelty measurements. Tracking and reviewing novelty
measures provides a data-driven way to assess contributions, progress, and
emerging directions in the science field. As academic papers serve as the
primary medium for the dissemination, validation, and discussion of scientific
knowledge, this review aims to offer a systematic analysis of novelty
measurements for scientific papers. We began by comparing the differences
between scientific novelty and four similar concepts, including originality,
scientific innovation, creativity, and scientific breakthrough. Next, we
reviewed the types of scientific novelty. Then, we classified existing novelty
measures according to data types and reviewed the measures for each type.
Subsequently, we surveyed the approaches employed in validating novelty
measures and examined the current tools and datasets associated with these
measures. Finally, we proposed several open issues for future studies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cross-Language Approach for Quranic QA 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17449v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17449v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Islam Oshallah, Mohamed Basem, Ali Hamdi, Ammar Mohammed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Question answering systems face critical limitations in languages with
limited resources and scarce data, making the development of robust models
especially challenging. The Quranic QA system holds significant importance as
it facilitates a deeper understanding of the Quran, a Holy text for over a
billion people worldwide. However, these systems face unique challenges,
including the linguistic disparity between questions written in Modern Standard
Arabic and answers found in Quranic verses written in Classical Arabic, and the
small size of existing datasets, which further restricts model performance. To
address these challenges, we adopt a cross-language approach by (1) Dataset
Augmentation: expanding and enriching the dataset through machine translation
to convert Arabic questions into English, paraphrasing questions to create
linguistic diversity, and retrieving answers from an English translation of the
Quran to align with multilingual training requirements; and (2) Language Model
Fine-Tuning: utilizing pre-trained models such as BERT-Medium, RoBERTa-Base,
DeBERTa-v3-Base, ELECTRA-Large, Flan-T5, Bloom, and Falcon to address the
specific requirements of Quranic QA. Experimental results demonstrate that this
cross-language approach significantly improves model performance, with
RoBERTa-Base achieving the highest MAP@10 (0.34) and MRR (0.52), while
DeBERTa-v3-Base excels in Recall@10 (0.50) and Precision@10 (0.24). These
findings underscore the effectiveness of cross-language strategies in
overcoming linguistic barriers and advancing Quranic QA systems
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Making Flowchart Images Machine Interpretable <span class="chip">ICDAR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17441v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17441v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shreya Shukla, Prajwal Gatti, Yogesh Kumar, Vikash Yadav, Anand Mishra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computer programming textbooks and software documentations often contain
flowcharts to illustrate the flow of an algorithm or procedure. Modern OCR
engines often tag these flowcharts as graphics and ignore them in further
processing. In this paper, we work towards making flowchart images
machine-interpretable by converting them to executable Python codes. To this
end, inspired by the recent success in natural language to code generation
literature, we present a novel transformer-based framework, namely FloCo-T5.
Our model is well-suited for this task,as it can effectively learn semantics,
structure, and patterns of programming languages, which it leverages to
generate syntactically correct code. We also used a task-specific pre-training
objective to pre-train FloCo-T5 using a large number of logic-preserving
augmented code samples. Further, to perform a rigorous study of this problem,
we introduce theFloCo dataset that contains 11,884 flowchart images and their
corresponding Python codes. Our experiments show promising results, and
FloCo-T5 clearly outperforms related competitive baselines on code generation
metrics. We make our dataset and implementation publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at: ICDAR 2023, Project Page:
  https://vl2g.github.io/projects/floco/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Virus: Harmful Fine-tuning Attack for Large Language Models Bypassing
  Guardrail Moderation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17433v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17433v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tiansheng Huang, Sihao Hu, Fatih Ilhan, Selim Furkan Tekin, Ling Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research shows that Large Language Models (LLMs) are vulnerable to
harmful fine-tuning attacks -- models lose their safety alignment ability after
fine-tuning on a few harmful samples. For risk mitigation, a guardrail is
typically used to filter out harmful samples before fine-tuning. By designing a
new red-teaming method, we in this paper show that purely relying on the
moderation guardrail for data filtration is not reliable. Our proposed attack
method, dubbed Virus, easily bypasses the guardrail moderation by slightly
modifying the harmful data. Experimental results show that the harmful data
optimized by Virus is not detectable by the guardrail with up to 100\% leakage
ratio, and can simultaneously achieve superior attack performance. Finally, the
key message we want to convey through this paper is that: \textbf{it is
reckless to consider guardrail moderation as a clutch at straws towards harmful
fine-tuning attack}, as it cannot solve the inherent safety issue of the
pre-trained LLMs. Our code is available at https://github.com/git-disl/Virus
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Actions Speak Louder than Words: Agent Decisions Reveal Implicit Biases
  in Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17420v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17420v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Li, Hirokazu Shirado, Sauvik Das
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While advances in fairness and alignment have helped mitigate overt biases
exhibited by large language models (LLMs) when explicitly prompted, we
hypothesize that these models may still exhibit implicit biases when simulating
human behavior. To test this hypothesis, we propose a technique to
systematically uncover such biases across a broad range of sociodemographic
categories by assessing decision-making disparities among agents with
LLM-generated, sociodemographically-informed personas. Using our technique, we
tested six LLMs across three sociodemographic groups and four decision-making
scenarios. Our results show that state-of-the-art LLMs exhibit significant
sociodemographic disparities in nearly all simulations, with more advanced
models exhibiting greater implicit biases despite reducing explicit biases.
Furthermore, when comparing our findings to real-world disparities reported in
empirical studies, we find that the biases we uncovered are directionally
aligned but markedly amplified. This directional alignment highlights the
utility of our technique in uncovering systematic biases in LLMs rather than
random variations; moreover, the presence and amplification of implicit biases
emphasizes the need for novel strategies to address these biases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ General Scene Adaptation for Vision-and-Language Navigation <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17403v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17403v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haodong Hong, Yanyuan Qiao, Sen Wang, Jiajun Liu, Qi Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-and-Language Navigation (VLN) tasks mainly evaluate agents based on
one-time execution of individual instructions across multiple environments,
aiming to develop agents capable of functioning in any environment in a
zero-shot manner. However, real-world navigation robots often operate in
persistent environments with relatively consistent physical layouts, visual
observations, and language styles from instructors. Such a gap in the task
setting presents an opportunity to improve VLN agents by incorporating
continuous adaptation to specific environments. To better reflect these
real-world conditions, we introduce GSA-VLN, a novel task requiring agents to
execute navigation instructions within a specific scene and simultaneously
adapt to it for improved performance over time. To evaluate the proposed task,
one has to address two challenges in existing VLN datasets: the lack of OOD
data, and the limited number and style diversity of instructions for each
scene. Therefore, we propose a new dataset, GSA-R2R, which significantly
expands the diversity and quantity of environments and instructions for the R2R
dataset to evaluate agent adaptability in both ID and OOD contexts.
Furthermore, we design a three-stage instruction orchestration pipeline that
leverages LLMs to refine speaker-generated instructions and apply role-playing
techniques to rephrase instructions into different speaking styles. This is
motivated by the observation that each individual user often has consistent
signatures or preferences in their instructions. We conducted extensive
experiments on GSA-R2R to thoroughly evaluate our dataset and benchmark various
methods. Based on our findings, we propose a novel method, GR-DUET, which
incorporates memory-based navigation graphs with an environment-specific
training strategy, achieving state-of-the-art results on all GSA-R2R splits.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MultiChallenge: A Realistic Multi-Turn Conversation Evaluation Benchmark
  Challenging to Frontier LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17399v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17399v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ved Sirdeshmukh, Kaustubh Deshpande, Johannes Mols, Lifeng Jin, Ed-Yeremai Cardona, Dean Lee, Jeremy Kritz, Willow Primack, Summer Yue, Chen Xing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present MultiChallenge, a pioneering benchmark evaluating large language
models (LLMs) on conducting multi-turn conversations with human users, a
crucial yet underexamined capability for their applications. MultiChallenge
identifies four categories of challenges in multi-turn conversations that are
not only common and realistic among current human-LLM interactions, but are
also challenging to all current frontier LLMs. All 4 challenges require
accurate instruction-following, context allocation, and in-context reasoning at
the same time. We also develop LLM as judge with instance-level rubrics to
facilitate an automatic evaluation method with fair agreement with experienced
human raters. Despite achieving near-perfect scores on existing multi-turn
evaluation benchmarks, all frontier models have less than 50% accuracy on
MultiChallenge, with the top-performing Claude 3.5 Sonnet (June 2024) achieving
just a 41.4% average accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging In-Context Learning and Retrieval-Augmented Generation for
  Automatic Question Generation in Educational Domains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17397v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17397v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Subhankar Maity, Aniket Deroy, Sudeshna Sarkar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Question generation in education is a time-consuming and cognitively
demanding task, as it requires creating questions that are both contextually
relevant and pedagogically sound. Current automated question generation methods
often generate questions that are out of context. In this work, we explore
advanced techniques for automated question generation in educational contexts,
focusing on In-Context Learning (ICL), Retrieval-Augmented Generation (RAG),
and a novel Hybrid Model that merges both methods. We implement GPT-4 for ICL
using few-shot examples and BART with a retrieval module for RAG. The Hybrid
Model combines RAG and ICL to address these issues and improve question
quality. Evaluation is conducted using automated metrics, followed by human
evaluation metrics. Our results show that both the ICL approach and the Hybrid
Model consistently outperform other methods, including baseline models, by
generating more contextually accurate and relevant questions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 16th Meeting of the Forum for Information Retrieval
  Evaluation as a Regular Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Free Token Reduction for Multi-Modal LLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17391v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17391v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihui Zhao, Yingxin Li, Yang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language Models (VLMs) have achieved remarkable success across a range
of multimodal tasks; however, their practical deployment is often constrained
by high computational costs and prolonged inference times. Since the vision
modality typically carries more information than the text modality, compressing
visual prompts offers a promising solution to alleviate these challenges.
Existing approaches predominantly focus on refining model architectures or
directly reducing the number of visual tokens. However, these methods often
compromise inference performance due to a lack of consideration for the unique
spatial and temporal characteristics of visual data. In this work, we propose a
token compression paradigm that operates on both spatial and temporal
dimensions. Our approach includes a learning-free, plug-and-play compression
pipeline that can be seamlessly integrated into most Multimodal Large Language
Model (MLLM) frameworks. By leveraging this method, we enhance the model
inference capability while simultaneously reducing its computational cost.
Experimental results on the Video-QA task demonstrate the effectiveness of the
proposed approach, showcasing significant improvements in efficiency without
sacrificing performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Context-Aware Semantic Recomposition Mechanism for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17386v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17386v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Richard Katrix, Quentin Carroway, Rowan Hawkesbury, Matthias Heathfield
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Context-aware processing mechanisms have increasingly become a critical area
of exploration for improving the semantic and contextual capabilities of
language generation models. The Context-Aware Semantic Recomposition Mechanism
(CASRM) was introduced as a novel framework designed to address limitations in
coherence, contextual adaptability, and error propagation in large-scale text
generation tasks. Through the integration of dynamically generated context
vectors and attention modulation layers, CASRM enhances the alignment between
token-level representations and broader contextual dependencies. Experimental
evaluations demonstrated significant improvements in semantic coherence across
multiple domains, including technical, conversational, and narrative text. The
ability to adapt to unseen domains and ambiguous inputs was evaluated using a
diverse set of test scenarios, highlighting the robustness of the proposed
mechanism. A detailed computational analysis revealed that while CASRM
introduces additional processing overhead, the gains in linguistic precision
and contextual relevance outweigh the marginal increase in complexity. The
framework also successfully mitigates error propagation in sequential tasks,
improving performance in dialogue continuation and multi-step text synthesis.
Additional investigations into token-level attention distribution emphasized
the dynamic focus shifts enabled through context-aware enhancements. The
findings suggest that CASRM offers a scalable and flexible solution for
integrating contextual intelligence into existing language model architectures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generating Synthetic Free-text Medical Records with Low
  Re-identification Risk using Masked Language Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09831v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09831v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Belkadi, Libo Ren, Nicolo Micheletti, Lifeng Han, Goran Nenadic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The vast amount of available medical records has the potential to improve
healthcare and biomedical research. However, privacy restrictions make these
data accessible for internal use only. Recent works have addressed this problem
by generating synthetic data using Causal Language Modeling. Unfortunately, by
taking this approach, it is often impossible to guarantee patient privacy while
offering the ability to control the diversity of generations without increasing
the cost of generating such data. In contrast, we present a system for
generating synthetic free-text medical records using Masked Language Modeling.
The system preserves critical medical information while introducing diversity
in the generations and minimising re-identification risk. The system's size is
about 120M parameters, minimising inference cost. The results demonstrate
high-quality synthetic data with a HIPAA-compliant PHI recall rate of 96% and a
re-identification risk of 3.5%. Moreover, downstream evaluations show that the
generated data can effectively train a model with performance comparable to
real data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Rewrote manuscript and moved content to appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Impact of Visual Information in Chinese Characters: Evaluating Large
  Models' Ability to Recognize and Utilize Radicals <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09013v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09013v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaofeng Wu, Karl Stratos, Wei Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The glyphic writing system of Chinese incorporates information-rich visual
features in each character, such as radicals that provide hints about meaning
or pronunciation. However, there has been no investigation into whether
contemporary Large Language Models (LLMs) and Vision-Language Models (VLMs) can
harness these sub-character features in Chinese through prompting. In this
study, we establish a benchmark to evaluate LLMs' and VLMs' understanding of
visual elements in Chinese characters, including radicals, composition
structures, strokes, and stroke counts. Our results reveal that models
surprisingly exhibit some, but still limited, knowledge of the visual
information, regardless of whether images of characters are provided. To incite
models' ability to use radicals, we further experiment with incorporating
radicals into the prompts for Chinese language processing (CLP) tasks. We
observe consistent improvement in Part-Of-Speech tagging when providing
additional information about radicals, suggesting the potential to enhance CLP
by integrating sub-character information.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Context is Key for Agent Security 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17070v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17070v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lillian Tsai, Eugene Bagdasarian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Judging the safety of an action, whether taken by a human or a system, must
take into account the context in which the action takes place. For example,
deleting an email from a user's mailbox may or may not be appropriate depending
on the email's content, the user's goals, or even available space. Systems
today that make these judgements -- providing security against harmful or
inappropriate actions -- rely on manually-crafted policies or user confirmation
for each relevant context. With the upcoming deployment of systems like
generalist agents, we argue that we must rethink security designs to adapt to
the scale of contexts and capabilities of these systems. As a first step, this
paper explores contextual security in the domain of agents and proposes
contextual security for agents (Conseca), a framework to generate just-in-time,
contextual, and human-verifiable security policies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AxBench: Steering LLMs? Even Simple Baselines Outperform Sparse
  Autoencoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17148v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17148v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengxuan Wu, Aryaman Arora, Atticus Geiger, Zheng Wang, Jing Huang, Dan Jurafsky, Christopher D. Manning, Christopher Potts
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-grained steering of language model outputs is essential for safety and
reliability. Prompting and finetuning are widely used to achieve these goals,
but interpretability researchers have proposed a variety of
representation-based techniques as well, including sparse autoencoders (SAEs),
linear artificial tomography, supervised steering vectors, linear probes, and
representation finetuning. At present, there is no benchmark for making direct
comparisons between these proposals. Therefore, we introduce AxBench, a
large-scale benchmark for steering and concept detection, and report
experiments on Gemma-2-2B and 9B. For steering, we find that prompting
outperforms all existing methods, followed by finetuning. For concept
detection, representation-based methods such as difference-in-means, perform
the best. On both evaluations, SAEs are not competitive. We introduce a novel
weakly-supervised representational method (Rank-1 Representation Finetuning;
ReFT-r1), which is competitive on both tasks while providing the
interpretability advantages that prompting lacks. Along with AxBench, we train
and publicly release SAE-scale feature dictionaries for ReFT-r1 and DiffMean.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Models and Code Security: A Systematic Literature <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15004v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15004v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enna Basic, Alberto Giaretta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have emerged as powerful tools for automating
various programming tasks, including security-related ones, such as detecting
and fixing vulnerabilities. Despite their promising capabilities, when required
to produce or modify pre-existing code, LLMs could introduce vulnerabilities
unbeknown to the programmer. When analyzing code, they could miss clear
vulnerabilities or signal nonexistent ones. In this Systematic Literature
Review (SLR), we aim to investigate both the security benefits and potential
drawbacks of using LLMs for a variety of code-related tasks. In particular,
first we focus on the types of vulnerabilities that could be introduced by
LLMs, when used for producing code. Second, we analyze the capabilities of LLMs
to detect and fix vulnerabilities, in any given code, and how the prompting
strategy of choice impacts their performance in these two tasks. Last, we
provide an in-depth analysis on how data poisoning attacks on LLMs can impact
performance in the aforementioned tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tulu 3: Pushing Frontiers in Open Language Model Post-Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.15124v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.15124v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James V. Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini, Noah A. Smith, Yizhong Wang, Pradeep Dasigi, Hannaneh Hajishirzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language model post-training is applied to refine behaviors and unlock new
skills across a wide range of recent language models, but open recipes for
applying these techniques lag behind proprietary ones. The underlying training
data and recipes for post-training are simultaneously the most important pieces
of the puzzle and the portion with the least transparency. To bridge this gap,
we introduce Tulu 3, a family of fully-open state-of-the-art post-trained
models, alongside its data, code, and training recipes, serving as a
comprehensive guide for modern post-training techniques. Tulu 3, which builds
on Llama 3.1 base models, achieves results surpassing the instruct versions of
Llama 3.1, Qwen 2.5, Mistral, and even closed models such as GPT-4o-mini and
Claude 3.5-Haiku. The training algorithms for our models include supervised
finetuning (SFT), Direct Preference Optimization (DPO), and a novel method we
call Reinforcement Learning with Verifiable Rewards (RLVR). With Tulu 3, we
introduce a multi-task evaluation scheme for post-training recipes with
development and unseen evaluations, standard benchmark implementations, and
substantial decontamination of existing open datasets on said benchmarks. We
conclude with analysis and discussion of training methods that did not reliably
improve performance.
  In addition to the Tulu 3 model weights and demo, we release the complete
recipe -- including datasets for diverse core skills, a robust toolkit for data
curation and evaluation, the training code and infrastructure, and, most
importantly, a detailed report for reproducing and further adapting the Tulu 3
approach to more domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Added Tulu 3 405B results and additional analyses</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Models Can Solve Real-World Planning Rigorously with
  Formal Verification Tools 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.11891v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.11891v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilun Hao, Yongchao Chen, Yang Zhang, Chuchu Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) struggle to directly generate correct plans for
complex multi-constraint planning problems, even with self-verification and
self-critique. For example, a U.S. domestic travel planning benchmark
TravelPlanner was proposed in Xie et al. (2024), where the best LLM OpenAI
o1-preview can only find viable travel plans with a 10% success rate given all
needed information. In this work, we tackle this by proposing an LLM-based
planning framework that formalizes and solves complex multi-constraint planning
problems as constrained satisfiability problems, which are further consumed by
sound and complete satisfiability solvers. We start with TravelPlanner as the
primary use case and show that our framework achieves a success rate of 93.9%
and is effective with diverse paraphrased prompts. More importantly, our
framework has strong zero-shot generalizability, successfully handling unseen
constraints in our newly created unseen international travel dataset and
generalizing well to new fundamentally different domains. Moreover, when user
input queries are infeasible, our framework can identify the unsatisfiable
core, provide failure reasons, and offers personalized modification
suggestions. We show that our framework can modify and solve for an average of
81.6% and 91.7% unsatisfiable queries from two datasets and prove with
ablations that all key components of our framework are effective and necessary.
Project page: https://sites.google.com/view/llm-rwplanning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>50 pages, 6 figures, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How Green are Neural Language Models? Analyzing Energy Consumption in
  Text Summarization Fine-tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15398v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15398v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tohida Rehman, Debarshi Kumar Sanyal, Samiran Chattopadhyay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence systems significantly impact the environment,
particularly in natural language processing (NLP) tasks. These tasks often
require extensive computational resources to train deep neural networks,
including large-scale language models containing billions of parameters. This
study analyzes the trade-offs between energy consumption and performance across
three neural language models: two pre-trained models (T5-base and BART-base),
and one large language model (LLaMA 3-8B). These models were fine-tuned for the
text summarization task, focusing on generating research paper highlights that
encapsulate the core themes of each paper. A wide range of evaluation metrics,
including ROUGE, METEOR, MoverScore, BERTScore, and SciBERTScore, were employed
to assess their performance. Furthermore, the carbon footprint associated with
fine-tuning each model was measured, offering a comprehensive assessment of
their environmental impact. This research underscores the importance of
incorporating environmental considerations into the design and implementation
of neural language models and calls for the advancement of energy-efficient AI
methodologies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Planning Anything with Rigor: General-Purpose Zero-Shot Planning with
  LLM-based Formalized Programming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12112v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12112v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilun Hao, Yang Zhang, Chuchu Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large language models (LLMs) have recently demonstrated strong
potential in solving planning problems, there is a trade-off between
flexibility and complexity. LLMs, as zero-shot planners themselves, are still
not capable of directly generating valid plans for complex planning problems
such as multi-constraint or long-horizon tasks. On the other hand, many
frameworks aiming to solve complex planning problems often rely on
task-specific preparatory efforts, such as task-specific in-context examples
and pre-defined critics/verifiers, which limits their cross-task generalization
capability. In this paper, we tackle these challenges by observing that the
core of many planning problems lies in optimization problems: searching for the
optimal solution (best plan) with goals subject to constraints (preconditions
and effects of decisions). With LLMs' commonsense, reasoning, and programming
capabilities, this opens up the possibilities of a universal LLM-based approach
to planning problems. Inspired by this observation, we propose LLMFP, a
general-purpose framework that leverages LLMs to capture key information from
planning problems and formally formulate and solve them as optimization
problems from scratch, with no task-specific examples needed. We apply LLMFP to
9 planning problems, ranging from multi-constraint decision making to
multi-step planning problems, and demonstrate that LLMFP achieves on average
83.7% and 86.8% optimal rate across 9 tasks for GPT-4o and Claude 3.5 Sonnet,
significantly outperforming the best baseline (direct planning with OpenAI
o1-preview) with 37.6% and 40.7% improvements. We also validate components of
LLMFP with ablation experiments and analyzed the underlying success and failure
reasons. Project page: https://sites.google.com/view/llmfp.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>57 pages, 25 figures, 15 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Lifelong Dialogue Agents via Timeline-based Memory Management <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10996v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10996v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Tzu-iunn Ong, Namyoung Kim, Minju Gwak, Hyungjoo Chae, Taeyoon Kwon, Yohan Jo, Seung-won Hwang, Dongha Lee, Jinyoung Yeo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To achieve lifelong human-agent interaction, dialogue agents need to
constantly memorize perceived information and properly retrieve it for response
generation (RG). While prior studies focus on getting rid of outdated memories
to improve retrieval quality, we argue that such memories provide rich,
important contextual cues for RG (e.g., changes in user behaviors) in long-term
conversations. We present THEANINE, a framework for LLM-based lifelong dialogue
agents. THEANINE discards memory removal and manages large-scale memories by
linking them based on their temporal and cause-effect relation. Enabled by this
linking structure, THEANINE augments RG with memory timelines - series of
memories representing the evolution or causality of relevant past events. Along
with THEANINE, we introduce TeaFarm, a counterfactual-driven evaluation scheme,
addressing the limitation of G-Eval and human efforts when assessing agent
performance in integrating past memories into RG. A supplementary video for
THEANINE and data for TeaFarm are at
https://huggingface.co/spaces/ResearcherScholar/Theanine.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Continuously Learning New Words in Automatic Speech Recognition <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.04482v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.04482v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian Huber, Alexander Waibel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent advances, Automatic Speech Recognition (ASR) systems are still
far from perfect. Typical errors include acronyms, named entities, and
domain-specific special words for which little or no labeled data is available.
To address the problem of recognizing these words, we propose a self-supervised
continual learning approach: Given the audio of a lecture talk with the
corresponding slides, we bias the model towards decoding new words from the
slides by using a memory-enhanced ASR model from the literature. Then, we
perform inference on the talk, collecting utterances that contain detected new
words into an adaptation data set. Continual learning is then performed by
training adaptation weights added to the model on this data set. The whole
procedure is iterated for many talks. We show that with this approach, we
obtain increasing performance on the new words when they occur more frequently
(more than 80% recall) while preserving the general performance of the model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AI-Assisted Human Evaluation of Machine Translation <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12419v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12419v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vilém Zouhar, Tom Kocmi, Mrinmaya Sachan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Annually, research teams spend large amounts of money to evaluate the quality
of machine translation systems (WMT, inter alia). This is expensive because it
requires a lot of expert human labor. In the recently adopted annotation
protocol, Error Span Annotation (ESA), annotators mark erroneous parts of the
translation and then assign a final score. A lot of the annotator time is spent
on scanning the translation for possible errors. In our work, we help the
annotators by pre-filling the error annotations with recall-oriented automatic
quality estimation. With this AI assistance, we obtain annotations at the same
quality level while cutting down the time per span annotation by half
(71s/error span $\rightarrow$ 31s/error span). The biggest advantage of the
ESA$^\mathrm{AI}$ protocol is an accurate priming of annotators (pre-filled
error spans) before they assign the final score. This alleviates a potential
automation bias, which we confirm to be low. In our experiments, we find that
the annotation budget can be further reduced by almost 25% with filtering of
examples that the AI deems to be likely to be correct.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can LLMs Obfuscate Code? A Systematic Analysis of Large Language Models
  into Assembly Code Obfuscation <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.16135v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.16135v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seyedreza Mohseni, Seyedali Mohammadi, Deepa Tilwani, Yash Saxena, Gerald Ketu Ndawula, Sriram Vema, Edward Raff, Manas Gaur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Malware authors often employ code obfuscations to make their malware harder
to detect. Existing tools for generating obfuscated code often require access
to the original source code (e.g., C++ or Java), and adding new obfuscations is
a non-trivial, labor-intensive process. In this study, we ask the following
question: Can Large Language Models (LLMs) potentially generate a new
obfuscated assembly code? If so, this poses a risk to anti-virus engines and
potentially increases the flexibility of attackers to create new obfuscation
patterns. We answer this in the affirmative by developing the MetamorphASM
benchmark comprising MetamorphASM Dataset (MAD) along with three code
obfuscation techniques: dead code, register substitution, and control flow
change. The MetamorphASM systematically evaluates the ability of LLMs to
generate and analyze obfuscated code using MAD, which contains 328,200
obfuscated assembly code samples. We release this dataset and analyze the
success rate of various LLMs (e.g., GPT-3.5/4, GPT-4o-mini, Starcoder,
CodeGemma, CodeLlama, CodeT5, and LLaMA 3.1) in generating obfuscated assembly
code. The evaluation was performed using established information-theoretic
metrics and manual human review to ensure correctness and provide the
foundation for researchers to study and develop remediations to this risk.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in AAAI 2025, Main Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Collapsed Language Models Promote Fairness <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04472v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04472v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingxuan Xu, Wuyang Chen, Linyi Li, Yao Zhao, Yunchao Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To mitigate societal biases implicitly encoded in recent successful
pretrained language models, a diverse array of approaches have been proposed to
encourage model fairness, focusing on prompting, data augmentation, regularized
fine-tuning, and more. Despite the development, it is nontrivial to reach a
principled understanding of fairness and an effective algorithm that can
consistently debias language models. In this work, by rigorous evaluations of
Neural Collapse -- a learning phenomenon happen in last-layer representations
and classifiers in deep networks -- on fairness-related words, we find that
debiased language models exhibit collapsed alignment between token
representations and word embeddings. More importantly, this observation
inspires us to design a principled fine-tuning method that can effectively
improve fairness in a wide range of debiasing methods, while still preserving
the performance of language models on standard natural language understanding
tasks. We attach our code at https://github.com/Xujxyang/Fairness-NC-main.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ medIKAL: Integrating Knowledge Graphs as Assistants of LLMs for Enhanced
  Clinical Diagnosis on EMRs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14326v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14326v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyi Jia, Junwen Duan, Yan Song, Jianxin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electronic Medical Records (EMRs), while integral to modern healthcare,
present challenges for clinical reasoning and diagnosis due to their complexity
and information redundancy. To address this, we proposed medIKAL (Integrating
Knowledge Graphs as Assistants of LLMs), a framework that combines Large
Language Models (LLMs) with knowledge graphs (KGs) to enhance diagnostic
capabilities. medIKAL assigns weighted importance to entities in medical
records based on their type, enabling precise localization of candidate
diseases within KGs. It innovatively employs a residual network-like approach,
allowing initial diagnosis by the LLM to be merged into KG search results.
Through a path-based reranking algorithm and a fill-in-the-blank style prompt
template, it further refined the diagnostic process. We validated medIKAL's
effectiveness through extensive experiments on a newly introduced open-sourced
Chinese EMR dataset, demonstrating its potential to improve clinical diagnosis
in real-world settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast Word Error Rate Estimation Using <span class="highlight-title">Self-Supervised</span> Representations
  for Speech and Text <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08225v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08225v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chanho Park, Chengsong Lu, Mingjie Chen, Thomas Hain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Word error rate (WER) estimation aims to evaluate the quality of an automatic
speech recognition (ASR) system's output without requiring ground-truth labels.
This task has gained increasing attention as advanced ASR systems are trained
on large amounts of data. In this context, the computational efficiency of a
WER estimator becomes essential in practice. However, previous works have not
prioritised this aspect. In this paper, a Fast estimator for WER (Fe-WER) is
introduced, utilizing average pooling over self-supervised learning
representations for speech and text. Our results demonstrate that Fe-WER
outperformed a baseline relatively by 14.10% in root mean square error and
1.22% in Pearson correlation coefficient on Ted-Lium3. Moreover, a comparative
analysis of the distributions of target WER and WER estimates was conducted,
including an examination of the average values per speaker. Lastly, the
inference speed was approximately 3.4 times faster in the real-time factor.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, accepted by ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EmoDynamiX: Emotional Support Dialogue Strategy Prediction by Modelling
  MiXed Emotions and Discourse Dynamics <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.08782v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.08782v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenwei Wan, Matthieu Labeau, Chloé Clavel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Designing emotionally intelligent conversational systems to provide comfort
and advice to people experiencing distress is a compelling area of research.
Recently, with advancements in large language models (LLMs), end-to-end
dialogue agents without explicit strategy prediction steps have become
prevalent. However, implicit strategy planning lacks transparency, and recent
studies show that LLMs' inherent preference bias towards certain
socio-emotional strategies hinders the delivery of high-quality emotional
support. To address this challenge, we propose decoupling strategy prediction
from language generation, and introduce a novel dialogue strategy prediction
framework, EmoDynamiX, which models the discourse dynamics between user
fine-grained emotions and system strategies using a heterogeneous graph for
better performance and transparency. Experimental results on two ESC datasets
show EmoDynamiX outperforms previous state-of-the-art methods with a
significant margin (better proficiency and lower preference bias). Our approach
also exhibits better transparency by allowing backtracing of decision making.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2025 main, camera ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating Telugu Proficiency in Large Language Models_ A Comparative
  Analysis of Chat<span class="highlight-title">GPT</span> and Gemini 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.19369v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.19369v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Katikela Sreeharsha Kishore, Rahimanuddin Shaik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing prominence of large language models (LLMs) necessitates the
exploration of their capabilities beyond English. This research investigates
the Telugu language proficiency of ChatGPT and Gemini, two leading LLMs.
Through a designed set of 20 questions encompassing greetings, grammar,
vocabulary, common phrases, task completion, and situational reasoning, the
study delves into their strengths and weaknesses in handling Telugu. The
analysis aims to identify the LLM that demonstrates a deeper understanding of
Telugu grammatical structures, possesses a broader vocabulary, and exhibits
superior performance in tasks like writing and reasoning. By comparing their
ability to comprehend and use everyday Telugu expressions, the research sheds
light on their suitability for real-world language interaction. Furthermore,
the evaluation of adaptability and reasoning capabilities provides insights
into how each LLM leverages Telugu to respond to dynamic situations. This
comparative analysis contributes to the ongoing discussion on multilingual
capabilities in AI and paves the way for future research in developing LLMs
that can seamlessly integrate with Telugu-speaking communities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Disparities in fundamental understandings about the article between
  the authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Text Generation in Joint NLG/NLU Learning Through Curriculum
  Learning, Semi-Supervised Training, and Advanced Optimization Techniques 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13498v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13498v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rahimanuddin Shaik, Katikela Sreeharsha Kishore
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text generation is the automated process of producing written or spoken
language using computational methods. It involves generating coherent and
contextually relevant text based on predefined rules or learned patterns.
However, challenges in text generation arise from maintaining coherence,
ensuring diversity and creativity, and avoiding biases or inappropriate
content. This research paper developed a novel approach to improve text
generation in the context of joint Natural Language Generation (NLG) and
Natural Language Understanding (NLU) learning. The data is prepared by
gathering and preprocessing annotated datasets, including cleaning,
tokenization, stemming, and stop-word removal. Feature extraction techniques
such as POS tagging, Bag of words, and Term Frequency-Inverse Document
Frequency (TF-IDF) are applied. Transformer-based encoders and decoders,
capturing long range dependencies and improving source-target sequence
modelling. Pre-trained language models like Optimized BERT are incorporated,
along with a Hybrid Redfox Artificial Hummingbird Algorithm (HRAHA).
Reinforcement learning with policy gradient techniques, semi-supervised
training, improved attention mechanisms, and differentiable approximations like
straight-through Gumbel SoftMax estimator are employed to fine-tune the models
and handle complex linguistic tasks effectively. The proposed model is
implemented using Python.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Disparities in fundamental understandings about the article between
  the authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Regularized Best-of-N Sampling with Minimum Bayes Risk Objective for
  Language Model Alignment <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.01054v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.01054v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuu Jinnai, Tetsuro Morimura, Kaito Ariu, Kenshi Abe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Best-of-N (BoN) sampling with a reward model has been shown to be an
effective strategy for aligning Large Language Models (LLMs) to human
preferences at the time of decoding. BoN sampling is susceptible to a problem
known as reward hacking when the accuracy of the reward model is not high
enough due to the quality or the quantity of the preference dataset. Because
the reward model is an imperfect proxy for the true objective, over-optimizing
its value can compromise its performance on the true objective. In this
research, we propose MBR-BoN, a variant of BoN that aims to mitigate reward
hacking at inference time by incorporating the Minimum Bayes Risk (MBR)
objective as a proximity regularization term. We show empirically and
analytically that the MBR objective quantifies the proximity of the response to
the reference policy, serving as a proximity regularizer. We evaluate MBR-BoN
on the AlpacaFarm and Anthropic's hh-rlhf datasets and show that it outperforms
both BoN sampling and MBR decoding. We also evaluate MBR-BoN to generate a
pairwise preference learning dataset for Direct Preference Optimization (DPO).
Empirical results show that models trained on a dataset generated with MBR-BoN
outperform those with vanilla BoN. Our code is available at
https://github.com/CyberAgentAILab/regularized-bon
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ COBias and Debias: Balancing Class Accuracies for Language Models in
  Inference Time via Nonlinear Integer Programming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.07623v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.07623v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruixi Lin, Yang You
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are good knowledge bases but struggle to perform
equally well for all classes in text classification tasks. This paper
investigates a fundamental inference-time problem in language models:
imbalanced class accuracies. We find what's underneath the issue is a tendency
to over-predict some classes while under-predicting some others. This class
accuracy imbalance is difficult to solve from the root via better pre-training
or fine-tuning strategies, but we show it can be effectively mitigated via
inference-time combinatorial optimization. To this end, we conceptualize and
quantify the over- and under-prediction issue as the Contextual Oddity Bias
(COBias), and propose the Debiasing as Nonlinear Integer Programming (DNIP)
model to correct in-context learned class probabilities based on minimizing
COBias and maximizing overall accuracy, without LLM parameter update.
Considering that the DNIP model implicitly contains non-differentiable
elements, we therefore use the simulated annealing algorithm to solve it.
Extensive evaluations on three LLMs across seven NLP classification tasks in
different prompting settings show that DNIP simultaneously achieves significant
COBias reduction (-27%) and accuracy improvement (+12%) over the conventional
ICL approach, suggesting that inference-time mitigation of class accuracy
imbalance is a promising direction to push forward LLM performances.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TAID: Temporally Adaptive Interpolated Distillation for Efficient
  Knowledge Transfer in Language Models <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16937v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16937v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Makoto Shing, Kou Misaki, Han Bao, Sho Yokoi, Takuya Akiba
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Causal language models have demonstrated remarkable capabilities, but their
size poses significant challenges for deployment in resource-constrained
environments. Knowledge distillation, a widely-used technique for transferring
knowledge from a large teacher model to a small student model, presents a
promising approach for model compression. A significant remaining issue lies in
the major differences between teacher and student models, namely the
substantial capacity gap, mode averaging, and mode collapse, which pose
barriers during distillation. To address these issues, we introduce
$\textit{Temporally Adaptive Interpolated Distillation (TAID)}$, a novel
knowledge distillation approach that dynamically interpolates student and
teacher distributions through an adaptive intermediate distribution, gradually
shifting from the student's initial distribution towards the teacher's
distribution. We provide a theoretical analysis demonstrating TAID's ability to
prevent mode collapse and empirically show its effectiveness in addressing the
capacity gap while balancing mode averaging and mode collapse. Our
comprehensive experiments demonstrate TAID's superior performance across
various model sizes and architectures in both instruction tuning and
pre-training scenarios. Furthermore, we showcase TAID's practical impact by
developing two state-of-the-art compact foundation models:
$\texttt{TAID-LLM-1.5B}$ for language tasks and $\texttt{TAID-VLM-2B}$ for
vision-language tasks. These results demonstrate TAID's effectiveness in
creating high-performing and efficient models, advancing the development of
more accessible AI technologies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at the 13th International Conference on Learning
  Representations (ICLR 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PhysBench: Benchmarking and Enhancing Vision-Language Models for
  Physical World Understanding <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16411v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16411v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Chow, Jiageng Mao, Boyi Li, Daniel Seita, Vitor Guizilini, Yue Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the physical world is a fundamental challenge in embodied AI,
critical for enabling agents to perform complex tasks and operate safely in
real-world environments. While Vision-Language Models (VLMs) have shown great
promise in reasoning and task planning for embodied agents, their ability to
comprehend physical phenomena remains extremely limited. To close this gap, we
introduce PhysBench, a comprehensive benchmark designed to evaluate VLMs'
physical world understanding capability across a diverse set of tasks.
PhysBench contains 10,002 entries of interleaved video-image-text data,
categorized into four major domains: physical object properties, physical
object relationships, physical scene understanding, and physics-based dynamics,
further divided into 19 subclasses and 8 distinct capability dimensions. Our
extensive experiments, conducted on 75 representative VLMs, reveal that while
these models excel in common-sense reasoning, they struggle with understanding
the physical world -- likely due to the absence of physical knowledge in their
training data and the lack of embedded physical priors. To tackle the
shortfall, we introduce PhysAgent, a novel framework that combines the
generalization strengths of VLMs with the specialized expertise of vision
models, significantly enhancing VLMs' physical understanding across a variety
of tasks, including an 18.4\% improvement on GPT-4o. Furthermore, our results
demonstrate that enhancing VLMs' physical world understanding capabilities can
help embodied agents such as MOKA. We believe that PhysBench and PhysAgent
offer valuable insights and contribute to bridging the gap between VLMs and
physical world understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025. Project page: https://physbench.github.io/ Dataset:
  https://huggingface.co/datasets/USC-GVL/PhysBench</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ O1-Pruner: Length-Harmonizing Fine-Tuning for O1-Like Reasoning Pruning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.12570v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.12570v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haotian Luo, Li Shen, Haiying He, Yibo Wang, Shiwei Liu, Wei Li, Naiqiang Tan, Xiaochun Cao, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, long-thought reasoning LLMs, such as OpenAI's O1, adopt extended
reasoning processes similar to how humans ponder over complex problems. This
reasoning paradigm significantly enhances the model's problem-solving abilities
and has achieved promising results. However, long-thought reasoning process
leads to a substantial increase in inference time. A pressing challenge is
reducing the inference overhead of long-thought LLMs while ensuring accuracy.
In this paper, we experimentally demonstrate that long-thought reasoning models
struggle to effectively allocate token budgets based on problem difficulty and
reasoning redundancies. To address this, we propose Length-Harmonizing
Fine-Tuning (O1-Pruner), aiming at minimizing reasoning overhead while
maintaining accuracy. This effective fine-tuning method first estimates the
LLM's baseline performance through pre-sampling and then uses RL-style
fine-tuning to encourage the model to generate shorter reasoning processes
under accuracy constraints. This allows the model to achieve efficient
reasoning with lower redundancy while maintaining accuracy. Experiments on
various mathematical reasoning benchmarks show that O1-Pruner not only
significantly reduces inference overhead but also achieves higher accuracy,
providing a novel and promising solution to this challenge. Our code is coming
soon at https://github.com/StarDewXXX/O1-Pruner
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ API Pack: A Massive Multi-Programming Language <span class="highlight-title">Dataset</span> for API Call
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.09615v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.09615v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Guo, Adriana Meza Soria, Wei Sun, Yikang Shen, Rameswar Panda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce API Pack, a massive multi-programming language dataset
containing over one million instruction-API calls for improving the API call
generation capabilities of large language models. Our evaluation highlights
three key findings: First, fine-tuning on API Pack enables open-source models
to outperform GPT-3.5 and GPT-4 in generating code for entirely new API calls.
We show this by fine-tuning CodeLlama-13B on 20,000 Python instances from API
Pack. Second, fine-tuning on a large dataset in one language, combined with
smaller datasets from others, improves API generation accuracy across multiple
languages. Third, we confirm the benefits of larger datasets for API
generalization, as increasing fine-tuning data to one million instances
enhances generalization to new APIs. To support further research, we
open-source the API Pack dataset, trained model, and code at
https://github.com/zguo0525/API-Pack.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Benchmark Evaluations, Applications, and Challenges of Large Vision
  Language Models: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.02189v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.02189v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zongxia Li, Xiyang Wu, Hongyang Du, Huy Nghiem, Guangyao Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Vision Language Models (VLMs) have emerged as a transformative
technology at the intersection of computer vision and natural language
processing, enabling machines to perceive and reason about the world through
both visual and textual modalities. For example, models such as CLIP, Claude,
and GPT-4V demonstrate strong reasoning and understanding abilities on visual
and textual data and beat classical single modality vision models on zero-shot
classification. Despite their rapid advancements in research and growing
popularity in applications, a comprehensive survey of existing studies on VLMs
is notably lacking, particularly for researchers aiming to leverage VLMs in
their specific domains. To this end, we provide a systematic overview of VLMs
in the following aspects: model information of the major VLMs developed over
the past five years (2019-2024); the main architectures and training methods of
these VLMs; summary and categorization of the popular benchmarks and evaluation
metrics of VLMs; the applications of VLMs including embodied agents, robotics,
and video generation; the challenges and issues faced by current VLMs such as
hallucination, fairness, and safety. Detailed collections including papers and
model repository links are listed in
https://github.com/zli12321/Awesome-VLM-Papers-And-Models.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">60</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative AI for Vision: A Comprehensive Study of Frameworks and
  Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18033v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18033v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fouad Bousetouane
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative AI is transforming image synthesis, enabling the creation of
high-quality, diverse, and photorealistic visuals across industries like
design, media, healthcare, and autonomous systems. Advances in techniques such
as image-to-image translation, text-to-image generation, domain transfer, and
multimodal alignment have broadened the scope of automated visual content
creation, supporting a wide spectrum of applications. These advancements are
driven by models like Generative Adversarial Networks (GANs), conditional
frameworks, and diffusion-based approaches such as Stable Diffusion. This work
presents a structured classification of image generation techniques based on
the nature of the input, organizing methods by input modalities like noisy
vectors, latent representations, and conditional inputs. We explore the
principles behind these models, highlight key frameworks including DALL-E,
ControlNet, and DeepSeek Janus-Pro, and address challenges such as
computational costs, data biases, and output alignment with user intent. By
offering this input-centric perspective, this study bridges technical depth
with practical insights, providing researchers and practitioners with a
comprehensive resource to harness generative AI for real-world applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>53 pages, 18 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Anatomy Might Be All You Need: Forecasting What to Do During Surgery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18011v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18011v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gary Sarwin, Alessandro Carretta, Victor Staartjes, Matteo Zoli, Diego Mazzatenta, Luca Regli, Carlo Serra, Ender Konukoglu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Surgical guidance can be delivered in various ways. In neurosurgery, spatial
guidance and orientation are predominantly achieved through neuronavigation
systems that reference pre-operative MRI scans. Recently, there has been
growing interest in providing live guidance by analyzing video feeds from tools
such as endoscopes. Existing approaches, including anatomy detection,
orientation feedback, phase recognition, and visual question-answering,
primarily focus on aiding surgeons in assessing the current surgical scene.
This work aims to provide guidance on a finer scale, aiming to provide guidance
by forecasting the trajectory of the surgical instrument, essentially
addressing the question of what to do next. To address this task, we propose a
model that not only leverages the historical locations of surgical instruments
but also integrates anatomical features. Importantly, our work does not rely on
explicit ground truth labels for instrument trajectories. Instead, the ground
truth is generated by a detection model trained to detect both anatomical
structures and instruments within surgical videos of a comprehensive dataset
containing pituitary surgery videos. By analyzing the interaction between
anatomy and instrument movements in these videos and forecasting future
instrument movements, we show that anatomical features are a valuable asset in
addressing this challenging task. To the best of our knowledge, this work is
the first attempt to address this task for manually operated surgeries.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pressure Field Reconstruction with SIREN: A Mesh-Free Approach for Image
  Velocimetry in Complex Noisy Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17987v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17987v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renato F. Miotto, William R. Wolf, Fernando Zigunov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work presents a novel approach for pressure field reconstruction from
image velocimetry data using SIREN (Sinusoidal Representation Network),
emphasizing its effectiveness as an implicit neural representation in noisy
environments and its mesh-free nature. While we briefly assess two recently
proposed methods - one-shot matrix-omnidirectional integration (OS-MODI) and
Green's function integral (GFI) - the primary focus is on the advantages of the
SIREN approach. The OS-MODI technique performs well in noise-free conditions
and with structured meshes but struggles when applied to unstructured meshes
with high aspect ratio. Similarly, the GFI method encounters difficulties due
to singularities inherent from the Newtonian kernel. In contrast, the proposed
SIREN approach is a mesh-free method that directly reconstructs the pressure
field, bypassing the need for an intrinsic grid connectivity and, hence,
avoiding the challenges associated with ill-conditioned cells and unstructured
meshes. This provides a distinct advantage over traditional mesh-based methods.
Moreover, it is shown that changes in the architecture of the SIREN can be used
to filter out inherent noise from velocimetry data. This work positions SIREN
as a robust and versatile solution for pressure reconstruction, particularly in
noisy environments characterized by the absence of mesh structure, opening new
avenues for innovative applications in this field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Feature Fusion for UAV Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17983v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17983v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xudong Wang, Chaomin Shen, Yaxin Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object detection in unmanned aerial vehicle (UAV) remote sensing images poses
significant challenges due to unstable image quality, small object sizes,
complex backgrounds, and environmental occlusions. Small objects, in
particular, occupy minimal portions of images, making their accurate detection
highly difficult. Existing multi-scale feature fusion methods address these
challenges to some extent by aggregating features across different resolutions.
However, these methods often fail to effectively balance classification and
localization performance for small objects, primarily due to insufficient
feature representation and imbalanced network information flow. In this paper,
we propose a novel feature fusion framework specifically designed for UAV
object detection tasks to enhance both localization accuracy and classification
performance. The proposed framework integrates hybrid upsampling and
downsampling modules, enabling feature maps from different network depths to be
flexibly adjusted to arbitrary resolutions. This design facilitates cross-layer
connections and multi-scale feature fusion, ensuring improved representation of
small objects. Our approach leverages hybrid downsampling to enhance
fine-grained feature representation, improving spatial localization of small
targets, even under complex conditions. Simultaneously, the upsampling module
aggregates global contextual information, optimizing feature consistency across
scales and enhancing classification robustness in cluttered scenes.
Experimental results on two public UAV datasets demonstrate the effectiveness
of the proposed framework. Integrated into the YOLO-V10 model, our method
achieves a 2\% improvement in average precision (AP) compared to the baseline
YOLO-V10 model, while maintaining the same number of parameters. These results
highlight the potential of our framework for accurate and efficient UAV object
detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VoD-3DGS: View-opacity-Dependent 3D Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17978v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17978v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nowak Mateusz, Jarosz Wojciech, Chin Peter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing a 3D scene from images is challenging due to the different
ways light interacts with surfaces depending on the viewer's position and the
surface's material. In classical computer graphics, materials can be classified
as diffuse or specular, interacting with light differently. The standard 3D
Gaussian Splatting model struggles to represent view-dependent content, since
it cannot differentiate an object within the scene from the light interacting
with its specular surfaces, which produce highlights or reflections. In this
paper, we propose to extend the 3D Gaussian Splatting model by introducing an
additional symmetric matrix to enhance the opacity representation of each 3D
Gaussian. This improvement allows certain Gaussians to be suppressed based on
the viewer's perspective, resulting in a more accurate representation of
view-dependent reflections and specular highlights without compromising the
scene's integrity. By allowing the opacity to be view dependent, our enhanced
model achieves state-of-the-art performance on Mip-Nerf, Tanks\&Temples, Deep
Blending, and Nerf-Synthetic datasets without a significant loss in rendering
speed, achieving >60FPS, and only incurring a minimal increase in memory used.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TransRAD: Retentive Vision <span class="highlight-title">Transformer</span> for Enhanced Radar Object
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17977v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17977v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Cheng, Siyang Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite significant advancements in environment perception capabilities for
autonomous driving and intelligent robotics, cameras and LiDARs remain
notoriously unreliable in low-light conditions and adverse weather, which
limits their effectiveness. Radar serves as a reliable and low-cost sensor that
can effectively complement these limitations. However, radar-based object
detection has been underexplored due to the inherent weaknesses of radar data,
such as low resolution, high noise, and lack of visual information. In this
paper, we present TransRAD, a novel 3D radar object detection model designed to
address these challenges by leveraging the Retentive Vision Transformer (RMT)
to more effectively learn features from information-dense radar
Range-Azimuth-Doppler (RAD) data. Our approach leverages the Retentive
Manhattan Self-Attention (MaSA) mechanism provided by RMT to incorporate
explicit spatial priors, thereby enabling more accurate alignment with the
spatial saliency characteristics of radar targets in RAD data and achieving
precise 3D radar detection across Range-Azimuth-Doppler dimensions.
Furthermore, we propose Location-Aware NMS to effectively mitigate the common
issue of duplicate bounding boxes in deep radar object detection. The
experimental results demonstrate that TransRAD outperforms state-of-the-art
methods in both 2D and 3D radar detection tasks, achieving higher accuracy,
faster inference speed, and reduced computational complexity. Code is available
at https://github.com/radar-lab/TransRAD
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Radar Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ U2A: Unified Unimodal Adaptation for Robust and Efficient Multimodal
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17823v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17823v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Kaykobad Reza, Niki Nezakati, Ameya Patil, Mashhour Solh, M. Salman Asif
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal learning often relies on designing new models and complex training
strategies to achieve optimal performance. We present Unified Unimodal
Adaptation (U2A), which jointly fine-tunes pretrained unimodal encoders using
low-rank adaptation (LoRA) for various multimodal tasks. Our method
significantly reduces the number of learnable parameters and eliminates the
need for complex training strategies, such as alternating training, gradient
modifications, or unimodal fine-tuning. To address missing modalities during
both training and testing, we introduce Mask Tokens (MT), which generate
missing modality features from available modalities using a single token per
modality. This simplifies the process, removing the need for specialized
feature estimation or prompt-tuning methods. Our evaluation demonstrates that
U2A matches or outperforms state-of-the-art methods in both complete and
missing modality settings, showcasing strong performance and robustness across
various modalities, tasks, and datasets. We also analyze and report the
effectiveness of Mask Tokens in different missing modality scenarios. Overall,
our method provides a robust, flexible, and efficient solution for multimodal
learning, with minimal computational overhead.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 Pages, 6 Figures, 6 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aggregation Schemes for Single-Vector WSI Representation Learning in
  Digital Pathology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17822v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17822v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sobhan Hemati, Ghazal Alabtah, Saghir Alfasly, H. R. Tizhoosh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A crucial step to efficiently integrate Whole Slide Images (WSIs) in
computational pathology is assigning a single high-quality feature vector,
i.e., one embedding, to each WSI. With the existence of many pre-trained deep
neural networks and the emergence of foundation models, extracting embeddings
for sub-images (i.e., tiles or patches) is straightforward. However, for WSIs,
given their high resolution and gigapixel nature, inputting them into existing
GPUs as a single image is not feasible. As a result, WSIs are usually split
into many patches. Feeding each patch to a pre-trained model, each WSI can then
be represented by a set of patches, hence, a set of embeddings. Hence, in such
a setup, WSI representation learning reduces to set representation learning
where for each WSI we have access to a set of patch embeddings. To obtain a
single embedding from a set of patch embeddings for each WSI, multiple
set-based learning schemes have been proposed in the literature. In this paper,
we evaluate the WSI search performance of multiple recently developed
aggregation techniques (mainly set representation learning techniques)
including simple average or max pooling operations, Deep Sets, Memory networks,
Focal attention, Gaussian Mixture Model (GMM) Fisher Vector, and deep sparse
and binary Fisher Vector on four different primary sites including bladder,
breast, kidney, and Colon from TCGA. Further, we benchmark the search
performance of these methods against the median of minimum distances of patch
embeddings, a non-aggregating approach used for WSI retrieval.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SSF: Sparse Long-Range Scene Flow for Autonomous Driving <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17821v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17821v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ajinkya Khoche, Qingwen Zhang, Laura Pereira Sanchez, Aron Asefaw, Sina Sharif Mansouri, Patric Jensfelt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scene flow enables an understanding of the motion characteristics of the
environment in the 3D world. It gains particular significance in the
long-range, where object-based perception methods might fail due to sparse
observations far away. Although significant advancements have been made in
scene flow pipelines to handle large-scale point clouds, a gap remains in
scalability with respect to long-range. We attribute this limitation to the
common design choice of using dense feature grids, which scale quadratically
with range. In this paper, we propose Sparse Scene Flow (SSF), a general
pipeline for long-range scene flow, adopting a sparse convolution based
backbone for feature extraction. This approach introduces a new challenge: a
mismatch in size and ordering of sparse feature maps between time-sequential
point scans. To address this, we propose a sparse feature fusion scheme, that
augments the feature maps with virtual voxels at missing locations.
Additionally, we propose a range-wise metric that implicitly gives greater
importance to faraway points. Our method, SSF, achieves state-of-the-art
results on the Argoverse2 dataset, demonstrating strong performance in
long-range scene flow estimation. Our code will be released at
https://github.com/KTH-RPL/SSF.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 3 figures, accepted to International Conference on Robotics
  and Automation (ICRA) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ P-TAME: Explain Any Image Classifier with Trained Perturbations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17813v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17813v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mariano V. Ntrougkas, Vasileios Mezaris, Ioannis Patras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The adoption of Deep Neural Networks (DNNs) in critical fields where
predictions need to be accompanied by justifications is hindered by their
inherent black-box nature. In this paper, we introduce P-TAME
(Perturbation-based Trainable Attention Mechanism for Explanations), a
model-agnostic method for explaining DNN-based image classifiers. P-TAME
employs an auxiliary image classifier to extract features from the input image,
bypassing the need to tailor the explanation method to the internal
architecture of the backbone classifier being explained. Unlike traditional
perturbation-based methods, which have high computational requirements, P-TAME
offers an efficient alternative by generating high-resolution explanations in a
single forward pass during inference. We apply P-TAME to explain the decisions
of VGG-16, ResNet-50, and ViT-B-16, three distinct and widely used image
classifiers. Quantitative and qualitative results show that our method matches
or outperforms previous explainability methods, including model-specific
approaches. Code and trained models will be released upon acceptance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted for publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Janus-Pro: Unified Multimodal Understanding and Generation with Data and
  Model Scaling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17811v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17811v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we introduce Janus-Pro, an advanced version of the previous
work Janus. Specifically, Janus-Pro incorporates (1) an optimized training
strategy, (2) expanded training data, and (3) scaling to larger model size.
With these improvements, Janus-Pro achieves significant advancements in both
multimodal understanding and text-to-image instruction-following capabilities,
while also enhancing the stability of text-to-image generation. We hope this
work will inspire further exploration in the field. Code and models are
publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Research paper. arXiv admin note: text overlap with arXiv:2410.13848</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CrowdSplat: Exploring Gaussian Splatting For Crowd Rendering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17792v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17792v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaohan Sun, Yinghan Xu, John Dingliana, Carol O'Sullivan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present CrowdSplat, a novel approach that leverages 3D Gaussian Splatting
for real-time, high-quality crowd rendering. Our method utilizes 3D Gaussian
functions to represent animated human characters in diverse poses and outfits,
which are extracted from monocular videos. We integrate Level of Detail (LoD)
rendering to optimize computational efficiency and quality. The CrowdSplat
framework consists of two stages: (1) avatar reconstruction and (2) crowd
synthesis. The framework is also optimized for GPU memory usage to enhance
scalability. Quantitative and qualitative evaluations show that CrowdSplat
achieves good levels of rendering quality, memory efficiency, and computational
performance. Through these experiments, we demonstrate that CrowdSplat is a
viable solution for dynamic, realistic crowd simulation in real-time
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Glioma Multimodal MRI Analysis System for Tumor Layered Diagnosis via
  Multi-task Semi-supervised Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17758v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17758v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihao Liu, Zhihao Cui, Liming Li, Junjie You, Xinle Feng, Jianxin Wang, Xiangyu Wang, Qing Liu, Minghua Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gliomas are the most common primary tumors of the central nervous system.
Multimodal MRI is widely used for the preliminary screening of gliomas and
plays a crucial role in auxiliary diagnosis, therapeutic efficacy, and
prognostic evaluation. Currently, the computer-aided diagnostic studies of
gliomas using MRI have focused on independent analysis events such as tumor
segmentation, grading, and radiogenomic classification, without studying
inter-dependencies among these events. In this study, we propose a Glioma
Multimodal MRI Analysis System (GMMAS) that utilizes a deep learning network
for processing multiple events simultaneously, leveraging their
inter-dependencies through an uncertainty-based multi-task learning
architecture and synchronously outputting tumor region segmentation, glioma
histological subtype, IDH mutation genotype, and 1p/19q chromosome disorder
status. Compared with the reported single-task analysis models, GMMAS improves
the precision across tumor layered diagnostic tasks. Additionally, we have
employed a two-stage semi-supervised learning method, enhancing model
performance by fully exploiting both labeled and unlabeled MRI samples.
Further, by utilizing an adaptation module based on knowledge self-distillation
and contrastive learning for cross-modal feature extraction, GMMAS exhibited
robustness in situations of modality absence and revealed the differing
significance of each MRI modal. Finally, based on the analysis outputs of the
GMMAS, we created a visual and user-friendly platform for doctors and patients,
introducing GMMAS-GPT to generate personalized prognosis evaluations and
suggestions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VICCA: Visual Interpretation and Comprehension of Chest X-ray Anomalies
  in Generated Report Without Human Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17726v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17726v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sayeh Gholipour Picha, Dawood Al Chanti, Alice Caplier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As artificial intelligence (AI) becomes increasingly central to healthcare,
the demand for explainable and trustworthy models is paramount. Current report
generation systems for chest X-rays (CXR) often lack mechanisms for validating
outputs without expert oversight, raising concerns about reliability and
interpretability. To address these challenges, we propose a novel multimodal
framework designed to enhance the semantic alignment and localization accuracy
of AI-generated medical reports. Our framework integrates two key modules: a
Phrase Grounding Model, which identifies and localizes pathologies in CXR
images based on textual prompts, and a Text-to-Image Diffusion Module, which
generates synthetic CXR images from prompts while preserving anatomical
fidelity. By comparing features between the original and generated images, we
introduce a dual-scoring system: one score quantifies localization accuracy,
while the other evaluates semantic consistency. This approach significantly
outperforms existing methods, achieving state-of-the-art results in pathology
localization and text-to-image alignment. The integration of phrase grounding
with diffusion models, coupled with the dual-scoring evaluation system,
provides a robust mechanism for validating report quality, paving the way for
more trustworthy and transparent AI in medical imaging.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Semantic Facial Descriptors for Accurate Face Animation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17718v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17718v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Zhu, Yuanqi Chen, Xiaohang Liu, Thomas H. Li, Ge Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Face animation is a challenging task. Existing model-based methods (utilizing
3DMMs or landmarks) often result in a model-like reconstruction effect, which
doesn't effectively preserve identity. Conversely, model-free approaches face
challenges in attaining a decoupled and semantically rich feature space,
thereby making accurate motion transfer difficult to achieve. We introduce the
semantic facial descriptors in learnable disentangled vector space to address
the dilemma. The approach involves decoupling the facial space into identity
and motion subspaces while endowing each of them with semantics by learning
complete orthogonal basis vectors. We obtain basis vector coefficients by
employing an encoder on the source and driving faces, leading to effective
facial descriptors in the identity and motion subspaces. Ultimately, these
descriptors can be recombined as latent codes to animate faces. Our approach
successfully addresses the issue of model-based methods' limitations in
high-fidelity identity and the challenges faced by model-free methods in
accurate motion transfer. Extensive experiments are conducted on three
challenging benchmarks (i.e. VoxCeleb, HDTF, CelebV). Comprehensive
quantitative and qualitative results demonstrate that our model outperforms
SOTA methods with superior identity preservation and motion transfer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages,6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real Time Scheduling Framework for Multi Object Detection via Spiking
  Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18412v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18412v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Donghwa Kang, Woojin Shin, Cheol-Ho Hong, Minsuk Koo, Brent ByungHoon Kang, Jinkyu Lee, Hyeongboo Baek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given the energy constraints in autonomous mobile agents (AMAs), such as
unmanned vehicles, spiking neural networks (SNNs) are increasingly favored as a
more efficient alternative to traditional artificial neural networks. AMAs
employ multi-object detection (MOD) from multiple cameras to identify nearby
objects while ensuring two essential objectives, (R1) timing guarantee and (R2)
high accuracy for safety. In this paper, we propose RT-SNN, the first system
design, aiming at achieving R1 and R2 in SNN-based MOD systems on AMAs.
Leveraging the characteristic that SNNs gather feature data of input image
termed as membrane potential, through iterative computation over multiple
timesteps, RT-SNN provides multiple execution options with adjustable timesteps
and a novel method for reusing membrane potential to support R1. Then, it
captures how these execution strategies influence R2 by introducing a novel
notion of mean absolute error and membrane confidence. Further, RT-SNN develops
a new scheduling framework consisting of offline schedulability analysis for R1
and a run-time scheduling algorithm for R2 using the notion of membrane
confidence. We deployed RT-SNN to Spiking-YOLO, the SNN-based MOD model derived
from ANN-to-SNN conversion, and our experimental evaluation confirms its
effectiveness in meeting the R1 and R2 requirements while providing significant
energy efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PulmoFusion: Advancing Pulmonary Health with Efficient Multi-Modal
  Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17699v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17699v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed Sharshar, Yasser Attia, Mohammad Yaqub, Mohsen Guizani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional remote spirometry lacks the precision required for effective
pulmonary monitoring. We present a novel, non-invasive approach using
multimodal predictive models that integrate RGB or thermal video data with
patient metadata. Our method leverages energy-efficient Spiking Neural Networks
(SNNs) for the regression of Peak Expiratory Flow (PEF) and classification of
Forced Expiratory Volume (FEV1) and Forced Vital Capacity (FVC), using
lightweight CNNs to overcome SNN limitations in regression tasks. Multimodal
data integration is improved with a Multi-Head Attention Layer, and we employ
K-Fold validation and ensemble learning to boost robustness. Using thermal
data, our SNN models achieve 92% accuracy on a breathing-cycle basis and 99.5%
patient-wise. PEF regression models attain Relative RMSEs of 0.11 (thermal) and
0.26 (RGB), with an MAE of 4.52% for FEV1/FVC predictions, establishing
state-of-the-art performance. Code and dataset can be found on
https://github.com/ahmed-sharshar/RespiroDynamics.git
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Segmentation-Aware Generative Reinforcement Network (GRN) for Tissue
  Layer Segmentation in 3-D Ultrasound Images for Chronic Low-back Pain (cLBP)
  Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17690v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17690v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixue Zeng, Xiaoyan Zhao, Matthew Cartier, Tong Yu, Jing Wang, Xin Meng, Zhiyu Sheng, Maryam Satarpour, John M Cormack, Allison Bean, Ryan Nussbaum, Maya Maurer, Emily Landis-Walkenhorst, Dinesh Kumbhare, Kang Kim, Ajay Wasan, Jiantao Pu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel segmentation-aware joint training framework called
generative reinforcement network (GRN) that integrates segmentation loss
feedback to optimize both image generation and segmentation performance in a
single stage. An image enhancement technique called segmentation-guided
enhancement (SGE) is also developed, where the generator produces images
tailored specifically for the segmentation model. Two variants of GRN were also
developed, including GRN for sample-efficient learning (GRN-SEL) and GRN for
semi-supervised learning (GRN-SSL). GRN's performance was evaluated using a
dataset of 69 fully annotated 3D ultrasound scans from 29 subjects. The
annotations included six anatomical structures: dermis, superficial fat,
superficial fascial membrane (SFM), deep fat, deep fascial membrane (DFM), and
muscle. Our results show that GRN-SEL with SGE reduces labeling efforts by up
to 70% while achieving a 1.98% improvement in the Dice Similarity Coefficient
(DSC) compared to models trained on fully labeled datasets. GRN-SEL alone
reduces labeling efforts by 60%, GRN-SSL with SGE decreases labeling
requirements by 70%, and GRN-SSL alone by 60%, all while maintaining
performance comparable to fully supervised models. These findings suggest the
effectiveness of the GRN framework in optimizing segmentation performance with
significantly less labeled data, offering a scalable and efficient solution for
ultrasound image analysis and reducing the burdens associated with data
annotation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised Patch-GAN with Targeted Patch Ranking for Fine-Grained
  Novelty Detection in Medical Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17906v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17906v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingkun Chen, Guang Yang, Xiao Zhang, Jingchao Peng, Tianlu Zhang, Jianguo Zhang, Jungong Han, Vicente Grau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting novel anomalies in medical imaging is challenging due to the
limited availability of labeled data for rare abnormalities, which often
display high variability and subtlety. This challenge is further compounded
when small abnormal regions are embedded within larger normal areas, as
whole-image predictions frequently overlook these subtle deviations. To address
these issues, we propose an unsupervised Patch-GAN framework designed to detect
and localize anomalies by capturing both local detail and global structure. Our
framework first reconstructs masked images to learn fine-grained,
normal-specific features, allowing for enhanced sensitivity to minor deviations
from normality. By dividing these reconstructed images into patches and
assessing the authenticity of each patch, our approach identifies anomalies at
a more granular level, overcoming the limitations of whole-image evaluation.
Additionally, a patch-ranking mechanism prioritizes regions with higher
abnormal scores, reinforcing the alignment between local patch discrepancies
and the global image context. Experimental results on the ISIC 2016 skin lesion
and BraTS 2019 brain tumor datasets validate our framework's effectiveness,
achieving AUCs of 95.79% and 96.05%, respectively, and outperforming three
state-of-the-art baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FeatureGS: Eigenvalue-Feature Optimization in 3D Gaussian Splatting for
  Geometrically Accurate and Artifact-Reduced Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17655v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17655v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miriam Jäger, Markus Hillemann, Boris Jutzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Gaussian Splatting (3DGS) has emerged as a powerful approach for 3D scene
reconstruction using 3D Gaussians. However, neither the centers nor surfaces of
the Gaussians are accurately aligned to the object surface, complicating their
direct use in point cloud and mesh reconstruction. Additionally, 3DGS typically
produces floater artifacts, increasing the number of Gaussians and storage
requirements. To address these issues, we present FeatureGS, which incorporates
an additional geometric loss term based on an eigenvalue-derived 3D shape
feature into the optimization process of 3DGS. The goal is to improve geometric
accuracy and enhance properties of planar surfaces with reduced structural
entropy in local 3D neighborhoods.We present four alternative formulations for
the geometric loss term based on 'planarity' of Gaussians, as well as
'planarity', 'omnivariance', and 'eigenentropy' of Gaussian neighborhoods. We
provide quantitative and qualitative evaluations on 15 scenes of the DTU
benchmark dataset focusing on following key aspects: Geometric accuracy and
artifact-reduction, measured by the Chamfer distance, and memory efficiency,
evaluated by the total number of Gaussians. Additionally, rendering quality is
monitored by Peak Signal-to-Noise Ratio. FeatureGS achieves a 30 % improvement
in geometric accuracy, reduces the number of Gaussians by 90 %, and suppresses
floater artifacts, while maintaining comparable photometric rendering quality.
The geometric loss with 'planarity' from Gaussians provides the highest
geometric accuracy, while 'omnivariance' in Gaussian neighborhoods reduces
floater artifacts and number of Gaussians the most. This makes FeatureGS a
strong method for geometrically accurate, artifact-reduced and memory-efficient
3D scene reconstruction, enabling the direct use of Gaussian centers for
geometric representation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 9 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Redundancy Reduction for Open-Vocabulary Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17642v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17642v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lin Chen, Qi Yang, Kun Ding, Zhihao Li, Gang Shen, Fei Li, Qiyuan Cao, Shiming Xiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-vocabulary semantic segmentation (OVSS) is an open-world task that aims
to assign each pixel within an image to a specific class defined by arbitrary
text descriptions. Recent advancements in large-scale vision-language models
have demonstrated their open-vocabulary understanding capabilities,
significantly facilitating the development of OVSS. However, most existing
methods suffer from either suboptimal performance or long latency. This study
introduces ERR-Seg, a novel framework that effectively reduces redundancy to
balance accuracy and efficiency. ERR-Seg incorporates a training-free Channel
Reduction Module (CRM) that leverages prior knowledge from vision-language
models like CLIP to identify the most relevant classes while discarding others.
Moreover, it incorporates Efficient Semantic Context Fusion (ESCF) with
spatial-level and class-level sequence reduction strategies. CRM and ESCF
result in substantial memory and computational savings without compromising
accuracy. Additionally, recognizing the significance of hierarchical semantics
extracted from middle-layer features for closed-set semantic segmentation,
ERR-Seg introduces the Hierarchical Semantic Module (HSM) to exploit
hierarchical semantics in the context of OVSS. Compared to previous
state-of-the-art methods under the ADE20K-847 setting, ERR-Seg achieves
+$5.6\%$ mIoU improvement and reduces latency by $67.3\%$.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Federated Learning With Individualized Privacy Through Client Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17634v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17634v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucas Lange, Ole Borchardt, Erhard Rahm
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With growing concerns about user data collection, individualized privacy has
emerged as a promising solution to balance protection and utility by accounting
for diverse user privacy preferences. Instead of enforcing a uniform level of
anonymization for all users, this approach allows individuals to choose privacy
settings that align with their comfort levels. Building on this idea, we
propose an adapted method for enabling Individualized Differential Privacy
(IDP) in Federated Learning (FL) by handling clients according to their
personal privacy preferences. By extending the SAMPLE algorithm from
centralized settings to FL, we calculate client-specific sampling rates based
on their heterogeneous privacy budgets and integrate them into a modified
IDP-FedAvg algorithm. We test this method under realistic privacy distributions
and multiple datasets. The experimental results demonstrate that our approach
achieves clear improvements over uniform DP baselines, reducing the trade-off
between privacy and utility. Compared to the alternative SCALE method in
related work, which assigns differing noise scales to clients, our method
performs notably better. However, challenges remain for complex tasks with
non-i.i.d. data, primarily stemming from the constraints of the decentralized
setting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dual Invariance Self-training for Reliable Semi-supervised Surgical
  Phase Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17628v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17628v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sahar Nasirihaghighi, Negin Ghamsarian, Raphael Sznitman, Klaus Schoeffmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate surgical phase recognition is crucial for advancing
computer-assisted interventions, yet the scarcity of labeled data hinders
training reliable deep learning models. Semi-supervised learning (SSL),
particularly with pseudo-labeling, shows promise over fully supervised methods
but often lacks reliable pseudo-label assessment mechanisms. To address this
gap, we propose a novel SSL framework, Dual Invariance Self-Training (DIST),
that incorporates both Temporal and Transformation Invariance to enhance
surgical phase recognition. Our two-step self-training process dynamically
selects reliable pseudo-labels, ensuring robust pseudo-supervision. Our
approach mitigates the risk of noisy pseudo-labels, steering decision
boundaries toward true data distribution and improving generalization to unseen
data. Evaluations on Cataract and Cholec80 datasets show our method outperforms
state-of-the-art SSL approaches, consistently surpassing both supervised and
SSL baselines across various network architectures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Watch Your STEPP: Semantic Traversability Estimation using Pose
  Projected Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17594v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17594v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastian Ægidius, Dennis Hadjivelichkov, Jianhao Jiao, Jonathan Embley-Riches, Dimitrios Kanoulas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the traversability of terrain is essential for autonomous robot
navigation, particularly in unstructured environments such as natural
landscapes. Although traditional methods, such as occupancy mapping, provide a
basic framework, they often fail to account for the complex mobility
capabilities of some platforms such as legged robots. In this work, we propose
a method for estimating terrain traversability by learning from demonstrations
of human walking. Our approach leverages dense, pixel-wise feature embeddings
generated using the DINOv2 vision Transformer model, which are processed
through an encoder-decoder MLP architecture to analyze terrain segments. The
averaged feature vectors, extracted from the masked regions of interest, are
used to train the model in a reconstruction-based framework. By minimizing
reconstruction loss, the network distinguishes between familiar terrain with a
low reconstruction error and unfamiliar or hazardous terrain with a higher
reconstruction error. This approach facilitates the detection of anomalies,
allowing a legged robot to navigate more effectively through challenging
terrain. We run real-world experiments on the ANYmal legged robot both indoor
and outdoor to prove our proposed method. The code is open-source, while video
demonstrations can be found on our website: https://rpl-cs-ucl.github.io/STEPP
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Trustworthy image-to-image translation: evaluating uncertainty
  calibration in unpaired training scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17570v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17570v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ciaran Bench, Emir Ahmed, Spencer A. Thomas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mammographic screening is an effective method for detecting breast cancer,
facilitating early diagnosis. However, the current need to manually inspect
images places a heavy burden on healthcare systems, spurring a desire for
automated diagnostic protocols. Techniques based on deep neural networks have
been shown effective in some studies, but their tendency to overfit leaves
considerable risk for poor generalisation and misdiagnosis, preventing their
widespread adoption in clinical settings. Data augmentation schemes based on
unpaired neural style transfer models have been proposed that improve
generalisability by diversifying the representations of training image features
in the absence of paired training data (images of the same tissue in either
image style). But these models are similarly prone to various pathologies, and
evaluating their performance is challenging without ground truths/large
datasets (as is often the case in medical imaging). Here, we consider two
frameworks/architectures: a GAN-based cycleGAN, and the more recently developed
diffusion-based SynDiff. We evaluate their performance when trained on image
patches parsed from three open access mammography datasets and one non-medical
image dataset. We consider the use of uncertainty quantification to assess
model trustworthiness, and propose a scheme to evaluate calibration quality in
unpaired training scenarios. This ultimately helps facilitate the trustworthy
use of image-to-image translation models in domains where ground truths are not
typically available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Exceptional <span class="highlight-title">Dataset</span> For Rare Pancreatic Tumor Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17555v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17555v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenqi Li, Yingli Chen, Keyang Zhou, Xiaoxiao Hu, Zilu Zheng, Yue Yan, Xinpeng Zhang, Wei Tang, Zhenxing Qian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pancreatic NEuroendocrine Tumors (pNETs) are very rare endocrine neoplasms
that account for less than 5% of all pancreatic malignancies, with an incidence
of only 1-1.5 cases per 100,000. Early detection of pNETs is critical for
improving patient survival, but the rarity of pNETs makes segmenting them from
CT a very challenging problem. So far, there has not been a dataset
specifically for pNETs available to researchers. To address this issue, we
propose a pNETs dataset, a well-annotated Contrast-Enhanced Computed Tomography
(CECT) dataset focused exclusively on Pancreatic Neuroendocrine Tumors,
containing data from 469 patients. This is the first dataset solely dedicated
to pNETs, distinguishing it from previous collections. Additionally, we provide
the baseline detection networks with a new slice-wise weight loss function
designed for the UNet-based model, improving the overall pNET segmentation
performance. We hope that our dataset can enhance the understanding and
diagnosis of pNET Tumors within the medical community, facilitate the
development of more accurate diagnostic tools, and ultimately improve patient
outcomes and advance the field of oncology.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Action Recognition Using Temporal Shift Module and Ensemble Learning <span class="chip">ICPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17550v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17550v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anh-Kiet Duong, Petra Gomez-Krämer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the first-rank solution for the Multi-Modal Action
Recognition Challenge, part of the Multi-Modal Visual Pattern Recognition
Workshop at the \acl{ICPR} 2024. The competition aimed to recognize human
actions using a diverse dataset of 20 action classes, collected from
multi-modal sources. The proposed approach is built upon the \acl{TSM}, a
technique aimed at efficiently capturing temporal dynamics in video data,
incorporating multiple data input types. Our strategy included transfer
learning to leverage pre-trained models, followed by meticulous fine-tuning on
the challenge's specific dataset to optimize performance for the 20 action
classes. We carefully selected a backbone network to balance computational
efficiency and recognition accuracy and further refined the model using an
ensemble technique that integrates outputs from different modalities. This
ensemble approach proved crucial in boosting the overall performance. Our
solution achieved a perfect top-1 accuracy on the test set, demonstrating the
effectiveness of the proposed approach in recognizing human actions across 20
classes. Our code is available online https://github.com/ffyyytt/TSM-MMVPR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, MMVPR @ ICPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Training-Free Open-World Classification with 3D Generative
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17547v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17547v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinzhe Xia, Weiguang Zhao, Yuyao Yan, Guanyu Yang, Rui Zhang, Kaizhu Huang, Xi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D open-world classification is a challenging yet essential task in dynamic
and unstructured real-world scenarios, requiring both open-category and
open-pose recognition. To address these challenges, recent wisdom often takes
sophisticated 2D pre-trained models to provide enriched and stable
representations. However, these methods largely rely on how 3D objects can be
projected into 2D space, which is unfortunately not well solved, and thus
significantly limits their performance. Unlike these present efforts, in this
paper we make a pioneering exploration of 3D generative models for 3D
open-world classification. Drawing on abundant prior knowledge from 3D
generative models, we additionally craft a rotation-invariant feature
extractor. This innovative synergy endows our pipeline with the advantages of
being training-free, open-category, and pose-invariant, thus well suited to 3D
open-world classification. Extensive experiments on benchmark datasets
demonstrate the potential of generative models in 3D open-world classification,
achieving state-of-the-art performance on ModelNet10 and McGill with 32.0% and
8.7% overall accuracy improvement, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3DSES: an indoor Lidar point cloud segmentation <span class="highlight-title">dataset</span> with real and
  pseudo-labels from a 3D model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17534v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17534v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maxime Mérizette, Nicolas Audebert, Pierre Kervella, Jérôme Verdun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic segmentation of indoor point clouds has found various applications
in the creation of digital twins for robotics, navigation and building
information modeling (BIM). However, most existing datasets of labeled indoor
point clouds have been acquired by photogrammetry. In contrast, Terrestrial
Laser Scanning (TLS) can acquire dense sub-centimeter point clouds and has
become the standard for surveyors. We present 3DSES (3D Segmentation of ESGT
point clouds), a new dataset of indoor dense TLS colorized point clouds
covering 427 m 2 of an engineering school. 3DSES has a unique double annotation
format: semantic labels annotated at the point level alongside a full 3D CAD
model of the building. We introduce a model-to-cloud algorithm for automated
labeling of indoor point clouds using an existing 3D CAD model. 3DSES has 3
variants of various semantic and geometrical complexities. We show that our
model-to-cloud alignment can produce pseudo-labels on our point clouds with a
\&gt; 95% accuracy, allowing us to train deep models with significant time
savings compared to manual labeling. First baselines on 3DSES show the
difficulties encountered by existing models when segmenting objects relevant to
BIM, such as light and safety utilities. We show that segmentation accuracy can
be improved by leveraging pseudo-labels and Lidar intensity, an information
rarely considered in current datasets. Code and data will be open sourced.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Solving Inverse Problems using Diffusion with Fast Iterative Renoising 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17468v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17468v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matt C. Bendel, Saurav K. Shastri, Rizwan Ahmad, Philip Schniter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Imaging inverse problems can be solved in an unsupervised manner using
pre-trained diffusion models. In most cases, that involves approximating the
gradient of the measurement-conditional score function in the reverse process.
Since the approximations produced by existing methods are quite poor,
especially early in the reverse process, we propose a new approach that
re-estimates and renoises the image several times per diffusion step. Renoising
adds carefully shaped colored noise that ensures the pre-trained diffusion
model sees white-Gaussian error, in accordance with how it was trained. We
demonstrate the effectiveness of our "DDfire" method at 20, 100, and 1000
neural function evaluations on linear inverse problems and phase retrieval.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Making Flowchart Images Machine Interpretable <span class="chip">ICDAR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17441v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17441v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shreya Shukla, Prajwal Gatti, Yogesh Kumar, Vikash Yadav, Anand Mishra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computer programming textbooks and software documentations often contain
flowcharts to illustrate the flow of an algorithm or procedure. Modern OCR
engines often tag these flowcharts as graphics and ignore them in further
processing. In this paper, we work towards making flowchart images
machine-interpretable by converting them to executable Python codes. To this
end, inspired by the recent success in natural language to code generation
literature, we present a novel transformer-based framework, namely FloCo-T5.
Our model is well-suited for this task,as it can effectively learn semantics,
structure, and patterns of programming languages, which it leverages to
generate syntactically correct code. We also used a task-specific pre-training
objective to pre-train FloCo-T5 using a large number of logic-preserving
augmented code samples. Further, to perform a rigorous study of this problem,
we introduce theFloCo dataset that contains 11,884 flowchart images and their
corresponding Python codes. Our experiments show promising results, and
FloCo-T5 clearly outperforms related competitive baselines on code generation
metrics. We make our dataset and implementation publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at: ICDAR 2023, Project Page:
  https://vl2g.github.io/projects/floco/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SIGN: A Statistically-Informed Gaze Network for Gaze Time Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17422v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17422v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianping Ye, Michel Wedel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a first version of SIGN, a Statistically-Informed Gaze Network, to
predict aggregate gaze times on images. We develop a foundational statistical
model for which we derive a deep learning implementation involving CNNs and
Visual Transformers, which enables the prediction of overall gaze times. The
model enables us to derive from the aggregate gaze times the underlying gaze
pattern as a probability map over all regions in the image, where each region's
probability represents the likelihood of being gazed at across all possible
scan-paths. We test SIGN's performance on AdGaze3500, a dataset of images of
ads with aggregate gaze times, and on COCO-Search18, a dataset with
individual-level fixation patterns collected during search. We demonstrate that
SIGN (1) improves gaze duration prediction significantly over state-of-the-art
deep learning benchmarks on both datasets, and (2) can deliver plausible gaze
patterns that correspond to empirical fixation patterns in COCO-Search18. These
results suggest that the first version of SIGN holds promise for gaze-time
predictions and deserves further development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ General Scene Adaptation for Vision-and-Language Navigation <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17403v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17403v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haodong Hong, Yanyuan Qiao, Sen Wang, Jiajun Liu, Qi Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-and-Language Navigation (VLN) tasks mainly evaluate agents based on
one-time execution of individual instructions across multiple environments,
aiming to develop agents capable of functioning in any environment in a
zero-shot manner. However, real-world navigation robots often operate in
persistent environments with relatively consistent physical layouts, visual
observations, and language styles from instructors. Such a gap in the task
setting presents an opportunity to improve VLN agents by incorporating
continuous adaptation to specific environments. To better reflect these
real-world conditions, we introduce GSA-VLN, a novel task requiring agents to
execute navigation instructions within a specific scene and simultaneously
adapt to it for improved performance over time. To evaluate the proposed task,
one has to address two challenges in existing VLN datasets: the lack of OOD
data, and the limited number and style diversity of instructions for each
scene. Therefore, we propose a new dataset, GSA-R2R, which significantly
expands the diversity and quantity of environments and instructions for the R2R
dataset to evaluate agent adaptability in both ID and OOD contexts.
Furthermore, we design a three-stage instruction orchestration pipeline that
leverages LLMs to refine speaker-generated instructions and apply role-playing
techniques to rephrase instructions into different speaking styles. This is
motivated by the observation that each individual user often has consistent
signatures or preferences in their instructions. We conducted extensive
experiments on GSA-R2R to thoroughly evaluate our dataset and benchmark various
methods. Based on our findings, we propose a novel method, GR-DUET, which
incorporates memory-based navigation graphs with an environment-specific
training strategy, achieving state-of-the-art results on all GSA-R2R splits.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Free Token Reduction for Multi-Modal LLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17391v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17391v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihui Zhao, Yingxin Li, Yang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language Models (VLMs) have achieved remarkable success across a range
of multimodal tasks; however, their practical deployment is often constrained
by high computational costs and prolonged inference times. Since the vision
modality typically carries more information than the text modality, compressing
visual prompts offers a promising solution to alleviate these challenges.
Existing approaches predominantly focus on refining model architectures or
directly reducing the number of visual tokens. However, these methods often
compromise inference performance due to a lack of consideration for the unique
spatial and temporal characteristics of visual data. In this work, we propose a
token compression paradigm that operates on both spatial and temporal
dimensions. Our approach includes a learning-free, plug-and-play compression
pipeline that can be seamlessly integrated into most Multimodal Large Language
Model (MLLM) frameworks. By leveraging this method, we enhance the model
inference capability while simultaneously reducing its computational cost.
Experimental results on the Video-QA task demonstrate the effectiveness of the
proposed approach, showcasing significant improvements in efficiency without
sacrificing performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visualization of Organ Movements Using Automatic Region Segmentation of
  Swallowing CT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17897v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17897v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yukihiro Michiwaki, Takahiro Kikuchi, Takashi Ijiri, Yoko Inamoto, Hiroshi Moriya, Takumi Ogawa, Ryota Nakatani, Yuto Masaki, Yoshito Otake, Yoshinobu Sato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study presents the first report on the development of an artificial
intelligence (AI) for automatic region segmentation of four-dimensional
computer tomography (4D-CT) images during swallowing. The material consists of
4D-CT images taken during swallowing. Additionally, data for verifying the
practicality of the AI were obtained from 4D-CT images during mastication and
swallowing. The ground truth data for the region segmentation for the AI were
created from five 4D-CT datasets of swallowing. A 3D convolutional model of
nnU-Net was used for the AI. The learning and evaluation method for the AI was
leave-one-out cross-validation. The number of epochs for training the nnU-Net
was 100. The Dice coefficient was used as a metric to assess the AI's region
segmentation accuracy. Regions with a median Dice coefficient of 0.7 or higher
included the bolus, bones, tongue, and soft palate. Regions with a Dice
coefficient below 0.7 included the thyroid cartilage and epiglottis. Factors
that reduced the Dice coefficient included metal artifacts caused by dental
crowns in the bolus and the speed of movement for the thyroid cartilage and
epiglottis. In practical verification of the AI, no significant misrecognition
was observed for facial bones, jaw bones, or the tongue. However, regions such
as the hyoid bone, thyroid cartilage, and epiglottis were not fully delineated
during fast movement. It is expected that future research will improve the
accuracy of the AI's region segmentation, though the risk of misrecognition
will always exist. Therefore, the development of tools for efficiently
correcting the AI's segmentation results is necessary. AI-based visualization
is expected to contribute not only to the deepening of motion analysis of
organs during swallowing but also to improving the accuracy of swallowing CT by
clearly showing the current state of its precision.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NUDT4MSTAR: A Large <span class="highlight-title">Dataset</span> and Benchmark Towards Remote Sensing Object
  Recognition in the Wild 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13354v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13354v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongxiang Liu, Weijie Li, Li Liu, Jie Zhou, Xuying Xiong, Bowen Peng, Yafei Song, Wei Yang, Tianpeng Liu, Zhen Liu, Xiang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As an indispensable sensor for Remote sensing, Synthetic Aperture Radar (SAR)
has a unique capability for all-day imaging. Nevertheless, in a data-driven
era, the scarcity of large-scale datasets poses a significant bottleneck to
advancing SAR automatic target recognition (ATR) technology. This paper
introduces NUDT4MSTAR, a large-scale SAR dataset for remote sensing target
recognition in the wild, including 40 vehicle target types and various imaging
conditions across 5 realistic scenes. NUDT4MSTAR represents a significant leap
forward in dataset scale, containing over 190,000 images-tenfold the size of
its predecessors. We meticulously annotate each image with detailed target
information and imaging conditions. Besides, data in both processed magnitude
images and original complex formats are provided. Then, we construct a
comprehensive benchmark consisting of 7 experiments with 15 recognition methods
focusing on the stable and effective ATR issues. Besides, we conduct transfer
learning experiments utilizing various models training on NUDT4MSTAR and apply
them to three other target datasets, demonstrating its substantial potential
for the broader field of ground objects ATR. Finally, we discuss this dataset's
application value and ATR's significant challenges. To the best of our
knowledge, this work marks the first-ever endeavor to create a large-scale
dataset benchmark for fine-grained SAR recognition in the wild, featuring an
extensive collection of exhaustively annotated vehicle images. We expect that
the open source of NUDT4MSTAR will facilitate the development of SAR ATR and
attract a wider community of researchers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 14 figures; NUDT4MSTAR:
  https://github.com/waterdisappear/NUDT4MSTAR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How to Backdoor Consistency Models? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.19785v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.19785v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengen Wang, Murat Kantarcioglu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Consistency models are a new class of models that generate images by directly
mapping noise to data, allowing for one-step generation and significantly
accelerating the sampling process. However, their robustness against
adversarial attacks has not yet been thoroughly investigated. In this work, we
conduct the first study on the vulnerability of consistency models to backdoor
attacks. While previous research has explored backdoor attacks on diffusion
models, those studies have primarily focused on conventional diffusion models,
employing a customized backdoor training process and objective, whereas
consistency models have distinct training processes and objectives. Our
proposed framework demonstrates the vulnerability of consistency models to
backdoor attacks. During image generation, poisoned consistency models produce
images with a Fr\'echet Inception Distance (FID) comparable to that of a clean
model when sampling from Gaussian noise. However, once the trigger is
activated, they generate backdoor target images. We explore various trigger and
target configurations to evaluate the vulnerability of consistency models,
including the use of random noise as a trigger. This novel trigger is visually
inconspicuous, more challenging to detect, and aligns well with the sampling
process of consistency models. Across all configurations, our framework
successfully compromises the consistency models while maintaining high utility
and specificity. We also examine the stealthiness of our proposed attack, which
is attributed to the unique properties of consistency models and the elusive
nature of the Gaussian noise trigger.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SLIM: Sim-to-Real Legged Instructive Manipulation via Long-Horizon
  Visuomotor Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09905v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09905v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haichao Zhang, Haonan Yu, Le Zhao, Andrew Choi, Qinxun Bai, Break Yang, Wei Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a low-cost legged mobile manipulation system that solves
long-horizon real-world tasks, trained by reinforcement learning purely in
simulation. This system is made possible by 1) a hierarchical design of a
high-level policy for visual-mobile manipulation following task instructions,
and a low-level quadruped locomotion policy, 2) a teacher and student training
pipeline for the high level, which trains a teacher to tackle long-horizon
tasks using privileged task decomposition and target object information, and
further trains a student for visual-mobile manipulation via RL guided by the
teacher's behavior, and 3) a suite of techniques for minimizing the sim-to-real
gap.
  In contrast to many previous works that use high-end equipments, our system
demonstrates effective performance with more accessible hardware --
specifically, a Unitree Go1 quadruped, a WidowX-250S arm, and a single
wrist-mounted RGB camera -- despite the increased challenges of sim-to-real
transfer. Trained fully in simulation, a single policy autonomously solves
long-horizon tasks involving search, move to, grasp, transport, and drop into,
achieving nearly 80% real-world success. This performance is comparable to that
of expert human teleoperation on the same tasks while the robot is more
efficient, operating at about 1.5x the speed of the teleoperation. Finally, we
perform extensive ablations on key techniques for efficient RL training and
effective sim-to-real transfer, and demonstrate effective deployment across
diverse indoor and outdoor scenes under various lighting conditions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PIR: Photometric Inverse Rendering with Shading Cues Modeling and
  Surface Reflectance Regularization <span class="chip">3DV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.06828v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.06828v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingzhi Bao, Guanying Chen, Shuguang Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the problem of inverse rendering from photometric
images. Existing approaches for this problem suffer from the effects of
self-shadows, inter-reflections, and lack of constraints on the surface
reflectance, leading to inaccurate decomposition of reflectance and
illumination due to the ill-posed nature of inverse rendering. In this work, we
propose a new method for neural inverse rendering. Our method jointly optimizes
the light source position to account for the self-shadows in images, and
computes indirect illumination using a differentiable rendering layer and an
importance sampling strategy. To enhance surface reflectance decomposition, we
introduce a new regularization by distilling DINO features to foster accurate
and consistent material decomposition. Extensive experiments on synthetic and
real datasets demonstrate that our method outperforms the state-of-the-art
methods in reflectance decomposition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to 3DV 2025. Project page:
  https://jzbao03.site/projects/PIR/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stroke classification using Virtual Hybrid Edge Detection from in silico
  electrical impedance tomography data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14704v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14704v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juan Pablo Agnelli, Fernando S. Moura, Siiri Rautio, Melody Alsaker, Rashmi Murthy, Matti Lassas, Samuli Siltanen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electrical impedance tomography (EIT) is a non-invasive imaging method for
recovering the internal conductivity of a physical body from electric boundary
measurements. EIT combined with machine learning has shown promise for the
classification of strokes. However, most previous works have used raw EIT
voltage data as network inputs. We build upon a recent development which
suggested the use of special noise-robust Virtual Hybrid Edge Detection (VHED)
functions as network inputs, although that work used only highly simplified and
mathematically ideal models. In this work we strengthen the case for the use of
EIT, and VHED functions especially, for stroke classification. We design models
with high detail and mathematical realism to test the use of VHED functions as
inputs. Virtual patients are created using a physically detailed 2D head model
which includes features known to create challenges in real-world imaging
scenarios. Conductivity values are drawn from statistically realistic
distributions, and phantoms are afflicted with either hemorrhagic or ischemic
strokes of various shapes and sizes. Simulated noisy EIT electrode data,
generated using the realistic Complete Electrode Model (CEM) as opposed to the
mathematically ideal continuum model, is processed to obtain VHED functions. We
compare the use of VHED functions as inputs against the alternative paradigm of
using raw EIT voltages. Our results show that (i) stroke classification can be
performed with high accuracy using 2D EIT data from physically detailed and
mathematically realistic models, and (ii) in the presence of noise, VHED
functions outperform raw data as network inputs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Polyp-Gen: Realistic and Diverse Polyp Image Generation for Endoscopic
  <span class="highlight-title">Dataset</span> Expansion <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16679v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16679v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengyuan Liu, Zhen Chen, Qiushi Yang, Weihao Yu, Di Dong, Jiancong Hu, Yixuan Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated diagnostic systems (ADS) have shown significant potential in the
early detection of polyps during endoscopic examinations, thereby reducing the
incidence of colorectal cancer. However, due to high annotation costs and
strict privacy concerns, acquiring high-quality endoscopic images poses a
considerable challenge in the development of ADS. Despite recent advancements
in generating synthetic images for dataset expansion, existing endoscopic image
generation algorithms failed to accurately generate the details of polyp
boundary regions and typically required medical priors to specify plausible
locations and shapes of polyps, which limited the realism and diversity of the
generated images. To address these limitations, we present Polyp-Gen, the first
full-automatic diffusion-based endoscopic image generation framework.
Specifically, we devise a spatial-aware diffusion training scheme with a
lesion-guided loss to enhance the structural context of polyp boundary regions.
Moreover, to capture medical priors for the localization of potential polyp
areas, we introduce a hierarchical retrieval-based sampling strategy to match
similar fine-grained spatial features. In this way, our Polyp-Gen can generate
realistic and diverse endoscopic images for building reliable ADS. Extensive
experiments demonstrate the state-of-the-art generation quality, and the
synthetic images can improve the downstream polyp detection task. Additionally,
our Polyp-Gen has shown remarkable zero-shot generalizability on other
datasets. The source code is available at
https://github.com/CUHK-AIM-Group/Polyp-Gen.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICRA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Real-Time Video Generation with Pyramid Attention Broadcast <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.12588v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.12588v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuanlei Zhao, Xiaolong Jin, Kai Wang, Yang You
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Pyramid Attention Broadcast (PAB), a real-time, high quality and
training-free approach for DiT-based video generation. Our method is founded on
the observation that attention difference in the diffusion process exhibits a
U-shaped pattern, indicating significant redundancy. We mitigate this by
broadcasting attention outputs to subsequent steps in a pyramid style. It
applies different broadcast strategies to each attention based on their
variance for best efficiency. We further introduce broadcast sequence parallel
for more efficient distributed inference. PAB demonstrates up to 10.5x speedup
across three models compared to baselines, achieving real-time generation for
up to 720p videos. We anticipate that our simple yet effective method will
serve as a robust baseline and facilitate future research and application for
video generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GMT: Guided Mask <span class="highlight-title">Transformer</span> for Leaf Instance Segmentation <span class="chip">WACV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17109v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17109v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feng Chen, Sotirios A. Tsaftaris, Mario Valerio Giuffrida
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Leaf instance segmentation is a challenging multi-instance segmentation task,
aiming to separate and delineate each leaf in an image of a plant. Accurate
segmentation of each leaf is crucial for plant-related applications such as the
fine-grained monitoring of plant growth and crop yield estimation. This task is
challenging because of the high similarity (in shape and colour), great size
variation, and heavy occlusions among leaf instances. Furthermore, the
typically small size of annotated leaf datasets makes it more difficult to
learn the distinctive features needed for precise segmentation. We hypothesise
that the key to overcoming the these challenges lies in the specific spatial
patterns of leaf distribution. In this paper, we propose the Guided Mask
Transformer (GMT), which leverages and integrates leaf spatial distribution
priors into a Transformer-based segmentor. These spatial priors are embedded in
a set of guide functions that map leaves at different positions into a more
separable embedding space. Our GMT consistently outperforms the
state-of-the-art on three public plant datasets. Our code is available at
https://github.com/vios-s/gmt-leaf-ins-seg.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IEEE/CVF Winter Conference on Applications of Computer
  Vision (WACV) 2025 (Oral Presentation)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TUNeS: A Temporal U-Net with Self-Attention for Video-based Surgical
  Phase Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.09997v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.09997v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Isabel Funke, Dominik Rivoir, Stefanie Krell, Stefanie Speidel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Objective: To enable context-aware computer assistance in the operating room
of the future, cognitive systems need to understand automatically which
surgical phase is being performed by the medical team. The primary source of
information for surgical phase recognition is typically video, which presents
two challenges: extracting meaningful features from the video stream and
effectively modeling temporal information in the sequence of visual features.
Methods: For temporal modeling, attention mechanisms have gained popularity due
to their ability to capture long-range dependencies. In this paper, we explore
design choices for attention in existing temporal models for surgical phase
recognition and propose a novel approach that uses attention more effectively
and does not require hand-crafted constraints: TUNeS, an efficient and simple
temporal model that incorporates self-attention at the core of a convolutional
U-Net structure. In addition, we propose to train the feature extractor, a
standard CNN, together with an LSTM on preferably long video segments, i.e.,
with long temporal context. Results: In our experiments, almost all temporal
models performed better on top of feature extractors that were trained with
longer temporal context. On these contextualized features, TUNeS achieves
state-of-the-art results on the Cholec80 dataset. Conclusion: This study offers
new insights on how to use attention mechanisms to build accurate and efficient
temporal models for surgical phase recognition. Significance: Implementing
automatic surgical phase recognition is essential to automate the analysis and
optimization of surgical workflows and to enable context-aware computer
assistance during surgery, thus ultimately improving patient care.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in IEEE Transactions on Biomedical
  Engineering</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Network Fission Ensembles for Low-Cost Self-Ensembles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02301v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02301v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hojung Lee, Jong-Seok Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent ensemble learning methods for image classification have been shown to
improve classification accuracy with low extra cost. However, they still
require multiple trained models for ensemble inference, which eventually
becomes a significant burden when the model size increases. In this paper, we
propose a low-cost ensemble learning and inference, called Network Fission
Ensembles (NFE), by converting a conventional network itself into a multi-exit
structure. Starting from a given initial network, we first prune some of the
weights to reduce the training burden. We then group the remaining weights into
several sets and create multiple auxiliary paths using each set to construct
multi-exits. We call this process Network Fission. Through this, multiple
outputs can be obtained from a single network, which enables ensemble learning.
Since this process simply changes the existing network structure to multi-exits
without using additional networks, there is no extra computational burden for
ensemble learning and inference. Moreover, by learning from multiple losses of
all exits, the multi-exits improve performance via regularization, and high
performance can be achieved even with increased network sparsity. With our
simple yet effective method, we achieve significant improvement compared to
existing ensemble methods. The code is available at
https://github.com/hjdw2/NFE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Segmentation and Smoothing Affect Explanation Quality More Than the
  Choice of Perturbation-based XAI Method for Image Explanations <span class="chip">IJCNN 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.04116v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.04116v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gustav Grund Pihlgren, Kary Främling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Perturbation-based post-hoc image explanation methods are commonly used to
explain image prediction models. These methods perturb parts of the input to
measure how those parts affect the output. Since the methods only require the
input and output they can be applied to any model, making them a popular choice
to explain black-box models. While many different models exist and have been
compared with one another, it remains poorly understood which parameters of the
different methods are responsible for their varying performance.
  This work uses the Randomized Input Sampling for Explanations (RISE) method
as a baseline to evaluate many combinations of mask sampling, segmentation
techniques, smoothing, attribution calculation, and per-segment or per-pixel
attribution, using a proxy metric. The results show that attribution
calculation, which is frequently the focus of other works, has little impact on
the results. Conversely, segmentation and per-pixel attribution, rarely
examined parameters, have a significant impact.
  The implementation of and data gathered in this work are available online:
https://github.com/guspih/post-hoc-image-perturbation and
https://bit.ly/smooth-mask-perturbation
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This manuscript have been submitted to IJCNN 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Out-of-distribution detection using normalizing flows on the data
  manifold 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.13792v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.13792v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seyedeh Fatemeh Razavi, Mohammad Mahdi Mehmanchi, Reshad Hosseini, Mostafa Tavassolipour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Using the intuition that out-of-distribution data have lower likelihoods, a
common approach for out-of-distribution detection involves estimating the
underlying data distribution. Normalizing flows are likelihood-based generative
models providing a tractable density estimation via dimension-preserving
invertible transformations. Conventional normalizing flows are prone to fail in
out-of-distribution detection, because of the well-known curse of
dimensionality problem of the likelihood-based models. To solve the problem of
likelihood-based models, some works try to modify likelihood for example by
incorporating a data complexity measure. We observed that these modifications
are still insufficient. According to the manifold hypothesis, real-world data
often lie on a low-dimensional manifold. Therefore, we proceed by estimating
the density on a low-dimensional manifold and calculating a distance from the
manifold as a measure for out-of-distribution detection. We propose a powerful
criterion that combines this measure with the modified likelihood measure based
on data complexity. Extensive experimental results show that incorporating
manifold learning while accounting for the estimation of data complexity
improves the out-of-distribution detection ability of normalizing flows. This
improvement is achieved without modifying the model structure or using
auxiliary out-of-distribution data during training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ORB-SfMLearner: ORB-Guided <span class="highlight-title">Self-supervised</span> Visual Odometry with
  Selective Online Adaptation <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11692v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11692v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanlin Jin, Rui-Yang Ju, Haojun Liu, Yuzhong Zhong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep visual odometry, despite extensive research, still faces limitations in
accuracy and generalizability that prevent its broader application. To address
these challenges, we propose an Oriented FAST and Rotated BRIEF (ORB)-guided
visual odometry with selective online adaptation named ORB-SfMLearner. We
present a novel use of ORB features for learning-based ego-motion estimation,
leading to more robust and accurate results. We also introduce the
cross-attention mechanism to enhance the explainability of PoseNet and have
revealed that driving direction of the vehicle can be explained through the
attention weights. To improve generalizability, our selective online adaptation
allows the network to rapidly and selectively adjust to the optimal parameters
across different domains. Experimental results on KITTI and vKITTI datasets
show that our method outperforms previous state-of-the-art deep visual odometry
methods in terms of ego-motion accuracy and generalizability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICRA 2025; Project page:
  https://www.neiljin.site/projects/orbsfm/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Robust Prototype-Based Network with Interpretable RBF Classifier
  Foundations <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15499v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15499v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sascha Saralajew, Ashish Rana, Thomas Villmann, Ammar Shaker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prototype-based classification learning methods are known to be inherently
interpretable. However, this paradigm suffers from major limitations compared
to deep models, such as lower performance. This led to the development of the
so-called deep Prototype-Based Networks (PBNs), also known as prototypical
parts models. In this work, we analyze these models with respect to different
properties, including interpretability. In particular, we focus on the
Classification-by-Components (CBC) approach, which uses a probabilistic model
to ensure interpretability and can be used as a shallow or deep architecture.
We show that this model has several shortcomings, like creating contradicting
explanations. Based on these findings, we propose an extension of CBC that
solves these issues. Moreover, we prove that this extension has robustness
guarantees and derive a loss that optimizes robustness. Additionally, our
analysis shows that most (deep) PBNs are related to (deep) RBF classifiers,
which implies that our robustness guarantees generalize to shallow RBF
classifiers. The empirical evaluation demonstrates that our deep PBN yields
state-of-the-art classification accuracy on different benchmarks while
resolving the interpretability shortcomings of other approaches. Further, our
shallow PBN variant outperforms other shallow PBNs while being inherently
interpretable and exhibiting provable robustness guarantees.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at AAAI 2025. Includes the Appendix of the AAAI submission.
  In v2 the font size has been increased in some figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exposing Image Classifier Shortcuts with Counterfactual Frequency (CoF)
  Tables 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15661v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15661v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        James Hinns, David Martens
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of deep learning in image classification has brought unprecedented
accuracy but also highlighted a key issue: the use of 'shortcuts' by models.
Such shortcuts are easy-to-learn patterns from the training data that fail to
generalise to new data. Examples include the use of a copyright watermark to
recognise horses, snowy background to recognise huskies, or ink markings to
detect malignant skin lesions. The explainable AI (XAI) community has suggested
using instance-level explanations to detect shortcuts without external data,
but this requires the examination of many explanations to confirm the presence
of such shortcuts, making it a labour-intensive process. To address these
challenges, we introduce Counterfactual Frequency (CoF) tables, a novel
approach that aggregates instance-based explanations into global insights, and
exposes shortcuts. The aggregation implies the need for some semantic concepts
to be used in the explanations, which we solve by labelling the segments of an
image. We demonstrate the utility of CoF tables across several datasets,
revealing the shortcuts learned from them.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 18 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Synthesizing 3D Abstractions by Inverting Procedural Buildings with
  <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17044v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17044v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maximilian Dax, Jordi Berbel, Jan Stria, Leonidas Guibas, Urs Bergmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We generate abstractions of buildings, reflecting the essential aspects of
their geometry and structure, by learning to invert procedural models. We first
build a dataset of abstract procedural building models paired with simulated
point clouds and then learn the inverse mapping through a transformer. Given a
point cloud, the trained transformer then infers the corresponding abstracted
building in terms of a programmatic language description. This approach
leverages expressive procedural models developed for gaming and animation, and
thereby retains desirable properties such as efficient rendering of the
inferred abstractions and strong priors for regularity and symmetry. Our
approach achieves good reconstruction accuracy in terms of geometry and
structure, as well as structurally consistent inpainting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Transformer</span>-Based Auxiliary Loss for Face Recognition Across Age
  Variations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.02198v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.02198v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pritesh Prakash, Ashish Jacob Sam, S Umamaheswaran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aging presents a significant challenge in face recognition, as changes in
skin texture and tone can alter facial features over time, making it
particularly difficult to compare images of the same individual taken years
apart, such as in long-term identification scenarios. Transformer networks have
the strength to preserve sequential spatial relationships caused by aging
effect. This paper presents a technique for loss evaluation that uses a
transformer network as an additive loss in the face recognition domain. The
standard metric loss function typically takes the final embedding of the main
CNN backbone as its input. Here, we employ a transformer-metric loss, a
combined approach that integrates both transformer-loss and metric-loss. This
research intends to analyze the transformer behavior on the convolution output
when the CNN outcome is arranged in a sequential vector. These sequential
vectors have the potential to overcome the texture or regional structure
referred to as wrinkles or sagging skin affected by aging. The transformer
encoder takes input from the contextual vectors obtained from the final
convolution layer of the network. The learned features can be more
age-invariant, complementing the discriminative power of the standard metric
loss embedding. With this technique, we use transformer loss with various base
metric-loss functions to evaluate the effect of the combined loss functions. We
observe that such a configuration allows the network to achieve SoTA results in
LFW and age-variant datasets (CA-LFW and AgeDB). This research expands the role
of transformers in the machine vision domain and opens new possibilities for
exploring transformers as a loss function.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Face Recognition for Age-variant Datasets</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on Quality Metrics for Text-to-Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11821v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11821v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastian Hartwig, Dominik Engel, Leon Sick, Hannah Kniesel, Tristan Payer, Poonam Poonam, Michael Glöckler, Alex Bäuerle, Timo Ropinski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI-based text-to-image models do not only excel at generating realistic
images, they also give designers more and more fine-grained control over the
image content. Consequently, these approaches have gathered increased attention
within the computer graphics research community, which has been historically
devoted towards traditional rendering techniques, that offer precise control
over scene parameters (e.g., objects, materials, and lighting). While the
quality of conventionally rendered images is assessed through well established
image quality metrics, such as SSIM or PSNR, the unique challenges of
text-to-image generation require other, dedicated quality metrics. These
metrics must be able to not only measure overall image quality, but also how
well images reflect given text prompts, whereby the control of scene and
rendering parameters is interweaved. Within this survey, we provide a
comprehensive overview of such text-to-image quality metrics, and propose a
taxonomy to categorize these metrics. Our taxonomy is grounded in the
assumption, that there are two main quality criteria, namely compositional
quality and general quality, that contribute to the overall image quality.
Besides the metrics, this survey covers dedicated text-to-image benchmark
datasets, over which the metrics are frequently computed. Finally, we identify
limitations and open challenges in the field of text-to-image generation, and
derive guidelines for practitioners conducting text-to-image evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GFE-Mamba: Mamba-based AD Multi-modal Progression Assessment via
  Generative Feature Extraction from MCI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.15719v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.15719v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaojie Fang, Shenghao Zhu, Yifei Chen, Binfeng Zou, Fan Jia, Chang Liu, Xiang Feng, Linwei Qiu, Feiwei Qin, Jin Fan, Changbiao Chu, Changmiao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Alzheimer's Disease (AD) is a progressive, irreversible neurodegenerative
disorder that often originates from Mild Cognitive Impairment (MCI). This
progression results in significant memory loss and severely affects patients'
quality of life. Clinical trials have consistently shown that early and
targeted interventions for individuals with MCI may slow or even prevent the
advancement of AD. Research indicates that accurate medical classification
requires diverse multimodal data, including detailed assessment scales and
neuroimaging techniques like Magnetic Resonance Imaging (MRI) and Positron
Emission Tomography (PET). However, simultaneously collecting the
aforementioned three modalities for training presents substantial challenges.
To tackle these difficulties, we propose GFE-Mamba, a multimodal classifier
founded on Generative Feature Extractor. The intermediate features provided by
this Extractor can compensate for the shortcomings of PET and achieve profound
multimodal fusion in the classifier. The Mamba block, as the backbone of the
classifier, enables it to efficiently extract information from long-sequence
scale information. Pixel-level Bi-cross Attention supplements pixel-level
information from MRI and PET. We provide our rationale for developing this
cross-temporal progression prediction dataset and the pre-trained Extractor
weights. Our experimental findings reveal that the GFE-Mamba model effectively
predicts the progression from MCI to AD and surpasses several leading methods
in the field. Our source code is available at
https://github.com/Tinysqua/GFE-Mamba.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When Video Coding Meets Multimodal Large Language Models: A Unified
  Paradigm for Video Coding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.08093v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.08093v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pingping Zhang, Jinlong Li, Kecheng Chen, Meng Wang, Long Xu, Haoliang Li, Nicu Sebe, Sam Kwong, Shiqi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing codecs are designed to eliminate intrinsic redundancies to create a
compact representation for compression. However, strong external priors from
Multimodal Large Language Models (MLLMs) have not been explicitly explored in
video compression. Herein, we introduce a unified paradigm for Cross-Modality
Video Coding (CMVC), which is a pioneering approach to explore multimodality
representation and video generative models in video coding. Specifically, on
the encoder side, we disentangle a video into spatial content and motion
components, which are subsequently transformed into distinct modalities to
achieve very compact representation by leveraging MLLMs. During decoding,
previously encoded components and video generation models are leveraged to
create multiple encoding-decoding modes that optimize video reconstruction
quality for specific decoding requirements, including Text-Text-to-Video (TT2V)
mode to ensure high-quality semantic information and Image-Text-to-Video (IT2V)
mode to achieve superb perceptual consistency. In addition, we propose an
efficient frame interpolation model for IT2V mode via Low-Rank Adaption (LoRA)
tuning to guarantee perceptual quality, which allows the generated motion cues
to behave smoothly. Experiments on benchmarks indicate that TT2V achieves
effective semantic reconstruction, while IT2V exhibits competitive perceptual
consistency. These results highlight potential directions for future research
in video coding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deciphering the Definition of Adversarial Robustness for post-hoc OOD
  Detectors <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15104v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15104v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter Lorenz, Mario Fernandez, Jens Müller, Ullrich Köthe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting out-of-distribution (OOD) inputs is critical for safely deploying
deep learning models in real-world scenarios. In recent years, many OOD
detectors have been developed, and even the benchmarking has been standardized,
i.e. OpenOOD. The number of post-hoc detectors is growing fast. They are
showing an option to protect a pre-trained classifier against natural
distribution shifts and claim to be ready for real-world scenarios. However,
its effectiveness in dealing with adversarial examples (AdEx) has been
neglected in most studies. In cases where an OOD detector includes AdEx in its
experiments, the lack of uniform parameters for AdEx makes it difficult to
accurately evaluate the performance of the OOD detector. This paper
investigates the adversarial robustness of 16 post-hoc detectors against
various evasion attacks. It also discusses a roadmap for adversarial defense in
OOD detectors that would help adversarial robustness. We believe that level 1
(AdEx on a unified dataset) should be added to any OOD detector to see the
limitations. The last level in the roadmap (defense against adaptive attacks)
we added for integrity from an adversarial machine learning (AML) point of
view, which we do not believe is the ultimate goal for OOD detectors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at ICML workshop 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond-Labels: Advancing Open-Vocabulary Segmentation With
  Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16769v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16769v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Atta ur Rahman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning can resolve numerous image or linguistic processing
problems when effectively trained. This study investigated simple yet efficient
methods for adaping previously learned foundation models for open-vocabulary
semantic segmentation tasks. Our research proposed "Beyond-Labels," a
lightweight transformer-based fusion module that uses a handful of image
segmentation data to fuse frozen image representations with language concepts.
Furthermore, we efficiently captured positional information in images using
Fourier embeddings, thus improving the generalization across various image
sizes. Extensive ablation tests were performed to investigate the important
components of our proposed method; when tested against the common benchmark
PASCAL-5i, it demonstrated superior performance despite being trained on frozen
image and language characteristics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PhysBench: Benchmarking and Enhancing Vision-Language Models for
  Physical World Understanding <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16411v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16411v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Chow, Jiageng Mao, Boyi Li, Daniel Seita, Vitor Guizilini, Yue Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the physical world is a fundamental challenge in embodied AI,
critical for enabling agents to perform complex tasks and operate safely in
real-world environments. While Vision-Language Models (VLMs) have shown great
promise in reasoning and task planning for embodied agents, their ability to
comprehend physical phenomena remains extremely limited. To close this gap, we
introduce PhysBench, a comprehensive benchmark designed to evaluate VLMs'
physical world understanding capability across a diverse set of tasks.
PhysBench contains 10,002 entries of interleaved video-image-text data,
categorized into four major domains: physical object properties, physical
object relationships, physical scene understanding, and physics-based dynamics,
further divided into 19 subclasses and 8 distinct capability dimensions. Our
extensive experiments, conducted on 75 representative VLMs, reveal that while
these models excel in common-sense reasoning, they struggle with understanding
the physical world -- likely due to the absence of physical knowledge in their
training data and the lack of embedded physical priors. To tackle the
shortfall, we introduce PhysAgent, a novel framework that combines the
generalization strengths of VLMs with the specialized expertise of vision
models, significantly enhancing VLMs' physical understanding across a variety
of tasks, including an 18.4\% improvement on GPT-4o. Furthermore, our results
demonstrate that enhancing VLMs' physical world understanding capabilities can
help embodied agents such as MOKA. We believe that PhysBench and PhysAgent
offer valuable insights and contribute to bridging the gap between VLMs and
physical world understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025. Project page: https://physbench.github.io/ Dataset:
  https://huggingface.co/datasets/USC-GVL/PhysBench</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Robust Unsupervised Attention Prediction in Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15045v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15045v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengshi Qi, Xiaoyang Bi, Pengfei Zhu, Huadong Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robustly predicting attention regions of interest for self-driving systems is
crucial for driving safety but presents significant challenges due to the
labor-intensive nature of obtaining large-scale attention labels and the domain
gap between self-driving scenarios and natural scenes. These challenges are
further exacerbated by complex traffic environments, including camera
corruption under adverse weather, noise interferences, and central bias from
long-tail distributions. To address these issues, we propose a robust
unsupervised attention prediction method. An Uncertainty Mining Branch refines
predictions by analyzing commonalities and differences across multiple
pre-trained models on natural scenes, while a Knowledge Embedding Block bridges
the domain gap by incorporating driving knowledge to adaptively enhance
pseudo-labels. Additionally, we introduce RoboMixup, a novel data augmentation
method that improves robustness against corruption through soft attention and
dynamic augmentation, and mitigates central bias by integrating random cropping
into Mixup as a regularizer. To systematically evaluate robustness in
self-driving attention prediction, we introduce the DriverAttention-C
benchmark, comprising over 100k frames across three subsets: BDD-A-C,
DR(eye)VE-C, and DADA-2000-C. Our method achieves performance equivalent to or
surpassing fully supervised state-of-the-art approaches on three public
datasets and the proposed robustness benchmark, reducing relative corruption
degradation by 58.8% and 52.8%, and improving central bias robustness by 12.4%
and 11.4% in KLD and CC metrics, respectively. Code and data are available at
https://github.com/zaplm/DriverAttention.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EchoFM: Foundation Model for Generalizable Echocardiogram Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23413v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23413v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sekeun Kim, Pengfei Jin, Sifan Song, Cheng Chen, Yiwei Li, Hui Ren, Xiang Li, Tianming Liu, Quanzheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models have recently gained significant attention because of their
generalizability and adaptability across multiple tasks and data distributions.
Although medical foundation models have emerged, solutions for cardiac imaging,
especially echocardiography videos, are still unexplored. In this paper, we
introduce EchoFM, a foundation model specifically designed to represent and
analyze echocardiography videos. In EchoFM, we propose a self-supervised
learning framework that captures both spatial and temporal variability patterns
through a spatio-temporal consistent masking strategy and periodic-driven
contrastive learning. This framework can effectively capture the
spatio-temporal dynamics of echocardiography and learn the representative video
features without any labels. We pre-train our model on an extensive dataset
comprising over 290,000 echocardiography videos covering 26 scan views across
different imaging modes, with up to 20 million frames of images. The
pre-trained EchoFM can then be easily adapted and fine-tuned for a variety of
downstream tasks, serving as a robust backbone model. Our evaluation was
systemically designed for four downstream tasks after the echocardiography
examination routine. Experiment results show that EchoFM surpasses
state-of-the-art methods, including specialized echocardiography methods,
self-supervised pre-training models, and general-purposed pre-trained
foundation models, across all downstream tasks.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">13</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RL-based Query Rewriting with Distilled LLM for online E-Commerce
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18056v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18056v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Duy A. Nguyen, Rishi Kesav Mohan, Van Yang, Pritom Saha Akash, Kevin Chen-Chuan Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Query rewriting (QR) is a critical technique in e-commerce search, addressing
the lexical gap between user queries and product descriptions to enhance search
performance. Existing QR approaches typically fall into two categories:
discriminative models and generative methods leveraging large language models
(LLMs). Discriminative models often struggle with natural language
understanding and offer limited flexibility in rewriting, while generative
LLMs, despite producing high-quality rewrites, face high inference latency and
cost in online settings. These limitations force offline deployment, making
them vulnerable to issues like information staleness and semantic drift. To
overcome these challenges, we propose a novel hybrid pipeline for QR that
balances efficiency and effectiveness. Our approach combines offline knowledge
distillation to create a lightweight but efficient student model with online
reinforcement learning (RL) to refine query rewriting dynamically using
real-time feedback. A key innovation is the use of LLMs as simulated human
feedback, enabling scalable reward signals and cost-effective evaluation
without manual annotations. Experimental results on Amazon ESCI dataset
demonstrate significant improvements in query relevance, diversity, and
adaptability, as well as positive feedback from the LLM simulation. This work
contributes to advancing LLM capabilities for domain-specific applications,
offering a robust solution for dynamic and complex e-commerce search
environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Generative LLMs Create Query Variants for Test Collections? An
  Exploratory Study <span class="chip">SIGIR'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17981v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17981v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marwah Alaofi, Luke Gallagher, Mark Sanderson, Falk Scholer, Paul Thomas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores the utility of a Large Language Model (LLM) to
automatically generate queries and query variants from a description of an
information need. Given a set of information needs described as backstories, we
explore how similar the queries generated by the LLM are to those generated by
humans. We quantify the similarity using different metrics and examine how the
use of each set would contribute to document pooling when building test
collections. Our results show potential in using LLMs to generate query
variants. While they may not fully capture the wide variety of human-generated
variants, they generate similar sets of relevant documents, reaching up to
71.1% overlap at a pool depth of 100.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in the proceedings of SIGIR'23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLMs can be Fooled into Labelling a Document as Relevant (best café
  near me; this paper is perfectly relevant) <span class="chip">SIGIR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17969v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17969v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marwah Alaofi, Paul Thomas, Falk Scholer, Mark Sanderson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLMs are increasingly being used to assess the relevance of information
objects. This work reports on experiments to study the labelling of short texts
(i.e., passages) for relevance, using multiple open-source and proprietary
LLMs. While the overall agreement of some LLMs with human judgements is
comparable to human-to-human agreement measured in previous research, LLMs are
more likely to label passages as relevant compared to human judges, indicating
that LLM labels denoting non-relevance are more reliable than those indicating
relevance.
  This observation prompts us to further examine cases where human judges and
LLMs disagree, particularly when the human judge labels the passage as
non-relevant and the LLM labels it as relevant. Results show a tendency for
many LLMs to label passages that include the original query terms as relevant.
We, therefore, conduct experiments to inject query words into random and
irrelevant passages, not unlike the way we inserted the query "best caf\'e near
me" into this paper. The results show that LLMs are highly influenced by the
presence of query words in the passages under assessment, even if the wider
passage has no relevance to the query. This tendency of LLMs to be fooled by
the mere presence of query words demonstrates a weakness in our current
measures of LLM labelling: relying on overall agreement misses important
patterns of failures. There is a real risk of bias in LLM-generated relevance
labels and, therefore, a risk of bias in rankers trained on those labels.
  We also investigate the effects of deliberately manipulating LLMs by
instructing them to label passages as relevant, similar to the instruction
"this paper is perfectly relevant" inserted above. We find that such
manipulation influences the performance of some LLMs, highlighting the critical
need to consider potential vulnerabilities when deploying LLMs in real-world
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in the proceedings of SIGIR-AP'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aggregation Schemes for Single-Vector WSI Representation Learning in
  Digital Pathology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17822v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17822v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sobhan Hemati, Ghazal Alabtah, Saghir Alfasly, H. R. Tizhoosh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A crucial step to efficiently integrate Whole Slide Images (WSIs) in
computational pathology is assigning a single high-quality feature vector,
i.e., one embedding, to each WSI. With the existence of many pre-trained deep
neural networks and the emergence of foundation models, extracting embeddings
for sub-images (i.e., tiles or patches) is straightforward. However, for WSIs,
given their high resolution and gigapixel nature, inputting them into existing
GPUs as a single image is not feasible. As a result, WSIs are usually split
into many patches. Feeding each patch to a pre-trained model, each WSI can then
be represented by a set of patches, hence, a set of embeddings. Hence, in such
a setup, WSI representation learning reduces to set representation learning
where for each WSI we have access to a set of patch embeddings. To obtain a
single embedding from a set of patch embeddings for each WSI, multiple
set-based learning schemes have been proposed in the literature. In this paper,
we evaluate the WSI search performance of multiple recently developed
aggregation techniques (mainly set representation learning techniques)
including simple average or max pooling operations, Deep Sets, Memory networks,
Focal attention, Gaussian Mixture Model (GMM) Fisher Vector, and deep sparse
and binary Fisher Vector on four different primary sites including bladder,
breast, kidney, and Colon from TCGA. Further, we benchmark the search
performance of these methods against the median of minimum distances of patch
embeddings, a non-aggregating approach used for WSI retrieval.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WARP: An Efficient Engine for Multi-Vector Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17788v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17788v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan Luca Scheerer, Matei Zaharia, Christopher Potts, Gustavo Alonso, Omar Khattab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the efficiency of multi-vector retrieval methods like ColBERT and
its recent variant XTR. We introduce WARP, a retrieval engine that drastically
improves the efficiency of XTR-based ColBERT retrievers through three key
innovations: (1) WARP$_\text{SELECT}$ for dynamic similarity imputation, (2)
implicit decompression to bypass costly vector reconstruction, and (3) a
two-stage reduction process for efficient scoring. Combined with optimized C++
kernels and specialized inference runtimes, WARP reduces end-to-end latency by
41x compared to XTR's reference implementation and thereby achieves a 3x
speedup over PLAID from the the official ColBERT implementation.
  We study the efficiency of multi-vector retrieval methods like ColBERT and
its recent variant XTR. We introduce WARP, a retrieval engine that drastically
improves the efficiency of XTR-based ColBERT retrievers through three key
innovations: (1) WARP$_\text{SELECT}$ for dynamic similarity imputation, (2)
implicit decompression during retrieval, and (3) a two-stage reduction process
for efficient scoring. Thanks also to highly-optimized C++ kernels and to the
adoption of specialized inference runtimes, WARP can reduce end-to-end query
latency relative to XTR's reference implementation by 41x. And it thereby
achieves a 3x speedup over the official ColBERTv2 PLAID engine, while
preserving retrieval quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distinguished Quantized Guidance for Diffusion-based Sequence
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17670v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17670v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenyu Mao, Shuchang Liu, Haoyang Liu, Haozhe Liu, Xiang Li, Lanatao Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models (DMs) have emerged as promising approaches for sequential
recommendation due to their strong ability to model data distributions and
generate high-quality items. Existing work typically adds noise to the next
item and progressively denoises it guided by the user's interaction sequence,
generating items that closely align with user interests. However, we identify
two key issues in this paradigm. First, the sequences are often heterogeneous
in length and content, exhibiting noise due to stochastic user behaviors. Using
such sequences as guidance may hinder DMs from accurately understanding user
interests. Second, DMs are prone to data bias and tend to generate only the
popular items that dominate the training dataset, thus failing to meet the
personalized needs of different users. To address these issues, we propose
Distinguished Quantized Guidance for Diffusion-based Sequence Recommendation
(DiQDiff), which aims to extract robust guidance to understand user interests
and generate distinguished items for personalized user interests within DMs. To
extract robust guidance, DiQDiff introduces Semantic Vector Quantization (SVQ)
to quantize sequences into semantic vectors (e.g., collaborative signals and
category interests) using a codebook, which can enrich the guidance to better
understand user interests. To generate distinguished items, DiQDiff
personalizes the generation through Contrastive Discrepancy Maximization (CDM),
which maximizes the distance between denoising trajectories using contrastive
loss to prevent biased generation for different users. Extensive experiments
are conducted to compare DiQDiff with multiple baseline models across four
widely-used datasets. The superior recommendation performance of DiQDiff
against leading approaches demonstrates its effectiveness in sequential
recommendation tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncertainty Quantification and Decomposition for LLM-based
  Recommendation <span class="chip">WWW 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17630v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17630v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wonbin Kweon, Sanghwan Jang, SeongKu Kang, Hwanjo Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the widespread adoption of large language models (LLMs) for
recommendation, we demonstrate that LLMs often exhibit uncertainty in their
recommendations. To ensure the trustworthy use of LLMs in generating
recommendations, we emphasize the importance of assessing the reliability of
recommendations generated by LLMs. We start by introducing a novel framework
for estimating the predictive uncertainty to quantitatively measure the
reliability of LLM-based recommendations. We further propose to decompose the
predictive uncertainty into recommendation uncertainty and prompt uncertainty,
enabling in-depth analyses of the primary source of uncertainty. Through
extensive experiments, we (1) demonstrate predictive uncertainty effectively
indicates the reliability of LLM-based recommendations, (2) investigate the
origins of uncertainty with decomposed uncertainty measures, and (3) propose
uncertainty-aware prompting for a lower predictive uncertainty and enhanced
recommendation. Our source code and model weights are available at
https://github.com/WonbinKweon/UNC_LLM_REC_WWW2025
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WWW 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cross-Language Approach for Quranic QA 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17449v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17449v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Islam Oshallah, Mohamed Basem, Ali Hamdi, Ammar Mohammed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Question answering systems face critical limitations in languages with
limited resources and scarce data, making the development of robust models
especially challenging. The Quranic QA system holds significant importance as
it facilitates a deeper understanding of the Quran, a Holy text for over a
billion people worldwide. However, these systems face unique challenges,
including the linguistic disparity between questions written in Modern Standard
Arabic and answers found in Quranic verses written in Classical Arabic, and the
small size of existing datasets, which further restricts model performance. To
address these challenges, we adopt a cross-language approach by (1) Dataset
Augmentation: expanding and enriching the dataset through machine translation
to convert Arabic questions into English, paraphrasing questions to create
linguistic diversity, and retrieving answers from an English translation of the
Quran to align with multilingual training requirements; and (2) Language Model
Fine-Tuning: utilizing pre-trained models such as BERT-Medium, RoBERTa-Base,
DeBERTa-v3-Base, ELECTRA-Large, Flan-T5, Bloom, and Falcon to address the
specific requirements of Quranic QA. Experimental results demonstrate that this
cross-language approach significantly improves model performance, with
RoBERTa-Base achieving the highest MAP@10 (0.34) and MRR (0.52), while
DeBERTa-v3-Base excels in Recall@10 (0.50) and Precision@10 (0.24). These
findings underscore the effectiveness of cross-language strategies in
overcoming linguistic barriers and advancing Quranic QA systems
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Value Function Decomposition in Markov Recommendation Process 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17409v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17409v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaobei Wang, Shuchang Liu, Qingpeng Cai, Xiang Li, Lantao Hu, Han li, Guangming Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in recommender systems have shown that user-system
interaction essentially formulates long-term optimization problems, and online
reinforcement learning can be adopted to improve recommendation performance.
The general solution framework incorporates a value function that estimates the
user's expected cumulative rewards in the future and guides the training of the
recommendation policy. To avoid local maxima, the policy may explore potential
high-quality actions during inference to increase the chance of finding better
future rewards. To accommodate the stepwise recommendation process, one widely
adopted approach to learning the value function is learning from the difference
between the values of two consecutive states of a user. However, we argue that
this paradigm involves an incorrect approximation in the stochastic process.
Specifically, between the current state and the next state in each training
sample, there exist two separate random factors from the stochastic policy and
the uncertain user environment. Original temporal difference (TD) learning
under these mixed random factors may result in a suboptimal estimation of the
long-term rewards. As a solution, we show that these two factors can be
separately approximated by decomposing the original temporal difference loss.
The disentangled learning framework can achieve a more accurate estimation with
faster learning and improved robustness against action exploration. As
empirical verification of our proposed method, we conduct offline experiments
with online simulated environments built based on public datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WavePulse: Real-time Content Analytics of Radio Livestreams <span class="chip">WWW</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.17998v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.17998v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Govind Mittal, Sarthak Gupta, Shruti Wagle, Chirag Chopra, Anthony J DeMattee, Nasir Memon, Mustaque Ahamad, Chinmay Hegde
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Radio remains a pervasive medium for mass information dissemination, with
AM/FM stations reaching more Americans than either smartphone-based social
networking or live television. Increasingly, radio broadcasts are also streamed
online and accessed over the Internet. We present WavePulse, a framework that
records, documents, and analyzes radio content in real-time. While our
framework is generally applicable, we showcase the efficacy of WavePulse in a
collaborative project with a team of political scientists focusing on the 2024
Presidential Elections. We use WavePulse to monitor livestreams of 396 news
radio stations over a period of three months, processing close to 500,000 hours
of audio streams. These streams were converted into time-stamped, diarized
transcripts and analyzed to track answer key political science questions at
both the national and state levels. Our analysis revealed how local issues
interacted with national trends, providing insights into information flow. Our
results demonstrate WavePulse's efficacy in capturing and analyzing content
from radio livestreams sourced from the Web. Code and dataset can be accessed
at \url{https://wave-pulse.io}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at The Web Conference (WWW) 2025. 20 Pages, 24 figures.
  Access code and dataset at https://wave-pulse.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Convergence of No-Regret Dynamics in Information Retrieval Games
  with Proportional Ranking Functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.11517v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.11517v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Omer Madmon, Idan Pipano, Itamar Reinman, Moshe Tennenholtz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Publishers who publish their content on the web act strategically, in a
behavior that can be modeled within the online learning framework. Regret, a
central concept in machine learning, serves as a canonical measure for
assessing the performance of learning agents within this framework. We prove
that any proportional content ranking function with a concave activation
function induces games in which no-regret learning dynamics converge. Moreover,
for proportional ranking functions, we prove the equivalence of the concavity
of the activation function, the social concavity of the induced games and the
concavity of the induced games. We also study the empirical trade-offs between
publishers' and users' welfare, under different choices of the activation
function, using a state-of-the-art no-regret dynamics algorithm. Furthermore,
we demonstrate how the choice of the ranking function and changes in the
ecosystem structure affect these welfare measures, as well as the dynamics'
convergence rate.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LayerPlexRank: Exploring Node Centrality and Layer Influence through
  Algebraic Connectivity in Multiplex Networks <span class="chip">CIKM '24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.05576v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.05576v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Ren, Jiaojiao Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the calculation of centrality in complex networks becomes increasingly
vital across technological, biological, and social systems, precise and
scalable ranking methods are essential for understanding these networks. This
paper introduces LayerPlexRank, an algorithm that simultaneously assesses node
centrality and layer influence in multiplex networks using algebraic
connectivity metrics. This method enhances the robustness of the ranking
algorithm by effectively assessing structural changes across layers using
random walk, considering the overall connectivity of the graph. We substantiate
the utility of LayerPlexRank with theoretical analyses and empirical
validations on varied real-world datasets, contrasting it with established
centrality measures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Proceedings of the 33rd ACM International Conference on
  Information and Knowledge Management (CIKM '24)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph-Sequential Alignment and Uniformity: Toward Enhanced
  Recommendation Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.04276v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.04276v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuwei Cao, Liangwei Yang, Zhiwei Liu, Yuqing Liu, Chen Wang, Yueqing Liang, Hao Peng, Philip S. Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph-based and sequential methods are two popular recommendation paradigms,
each excelling in its domain but lacking the ability to leverage signals from
the other. To address this, we propose a novel method that integrates both
approaches for enhanced performance. Our framework uses Graph Neural Network
(GNN)-based and sequential recommenders as separate submodules while sharing a
unified embedding space optimized jointly. To enable positive knowledge
transfer, we design a loss function that enforces alignment and uniformity both
within and across submodules. Experiments on three real-world datasets
demonstrate that the proposed method significantly outperforms using either
approach alone and achieves state-of-the-art results. Our implementations are
publicly available at https://github.com/YuweiCao-UIC/GSAU.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to The Web Conference 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Guided Virtual Reality Therapy for Anxiety: A Systematic <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17375v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17375v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Winona Graham, Russell Drinkwater, Joshua Kelson, Muhammad Ashad Kabir
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Virtual reality (VR) technology can be used to treat anxiety symptoms and
disorders. However, most VR interventions for anxiety have been therapist
guided rather than self-guided. This systematic review aimed to examine the
effectiveness and user experience (i.e., usability, acceptability, safety, and
attrition rates) of self-guided VR therapy interventions in people with any
anxiety condition as well as provide future research directions. Peer-reviewed
journal articles reporting on self-guided VR interventions for anxiety were
sought from the Cochrane Library, IEEE Explore Digital Library, PsycINFO,
PubMED, Scopus, and Web of Science databases. Study data from the eligible
articles were extracted, tabulated, and addressed with a narrative synthesis. A
total of 21 articles met the inclusion criteria. The findings revealed that
self-guided VR interventions for anxiety can provide an effective treatment of
social anxiety disorder, public speaking anxiety, and specific phobias. User
experiences outcomes of safety, usability, and acceptability were generally
positive and the average attrition rate was low. However, there was a lack of
standardised assessments to measure user experiences. Self-guided VR for
anxiety can provide an engaging approach for effectively and safely treating
common anxiety conditions. Nevertheless, more experimental studies are required
to examine their use in underrepresented anxiety populations, their long-term
treatment effects beyond 12 months, and compare their effectiveness against
other self-help interventions for anxiety (e.g., internet interventions and
bibliotherapy).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>40 pages, 1 figure, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When Video Coding Meets Multimodal Large Language Models: A Unified
  Paradigm for Video Coding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.08093v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.08093v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pingping Zhang, Jinlong Li, Kecheng Chen, Meng Wang, Long Xu, Haoliang Li, Nicu Sebe, Sam Kwong, Shiqi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing codecs are designed to eliminate intrinsic redundancies to create a
compact representation for compression. However, strong external priors from
Multimodal Large Language Models (MLLMs) have not been explicitly explored in
video compression. Herein, we introduce a unified paradigm for Cross-Modality
Video Coding (CMVC), which is a pioneering approach to explore multimodality
representation and video generative models in video coding. Specifically, on
the encoder side, we disentangle a video into spatial content and motion
components, which are subsequently transformed into distinct modalities to
achieve very compact representation by leveraging MLLMs. During decoding,
previously encoded components and video generation models are leveraged to
create multiple encoding-decoding modes that optimize video reconstruction
quality for specific decoding requirements, including Text-Text-to-Video (TT2V)
mode to ensure high-quality semantic information and Image-Text-to-Video (IT2V)
mode to achieve superb perceptual consistency. In addition, we propose an
efficient frame interpolation model for IT2V mode via Low-Rank Adaption (LoRA)
tuning to guarantee perceptual quality, which allows the generated motion cues
to behave smoothly. Experiments on benchmarks indicate that TT2V achieves
effective semantic reconstruction, while IT2V exhibits competitive perceptual
consistency. These results highlight potential directions for future research
in video coding.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-28T00:00:00Z">2025-01-28</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">22</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Better Slow than Sorry: Introducing Positive Friction for Reliable
  Dialogue Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17348v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17348v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mert İnan, Anthony Sicilia, Suvodip Dey, Vardhan Dongre, Tejas Srinivasan, Jesse Thomason, Gökhan Tür, Dilek Hakkani-Tür, Malihe Alikhani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While theories of discourse and cognitive science have long recognized the
value of unhurried pacing, recent dialogue research tends to minimize friction
in conversational systems. Yet, frictionless dialogue risks fostering
uncritical reliance on AI outputs, which can obscure implicit assumptions and
lead to unintended consequences. To meet this challenge, we propose integrating
positive friction into conversational AI, which promotes user reflection on
goals, critical thinking on system response, and subsequent re-conditioning of
AI systems. We hypothesize systems can improve goal alignment, modeling of user
mental states, and task success by deliberately slowing down conversations in
strategic moments to ask questions, reveal assumptions, or pause. We present an
ontology of positive friction and collect expert human annotations on
multi-domain and embodied goal-oriented corpora. Experiments on these corpora,
along with simulated interactions using state-of-the-art systems, suggest
incorporating friction not only fosters accountable decision-making, but also
enhances machine understanding of user beliefs and goals, and increases task
success rates.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Inferring from Logits: Exploring Best Practices for Decoding-Free
  Generative Candidate Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17338v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17338v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyu Derek Ma, Yanna Ding, Zijie Huang, Jianxi Gao, Yizhou Sun, Wei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative Language Models rely on autoregressive decoding to produce the
output sequence token by token. Many tasks such as preference optimization,
require the model to produce task-level output consisting of multiple tokens
directly by selecting candidates from a pool as predictions. Determining a
task-level prediction from candidates using the ordinary token-level decoding
mechanism is constrained by time-consuming decoding and interrupted gradients
by discrete token selection. Existing works have been using decoding-free
candidate selection methods to obtain candidate probability from initial output
logits over vocabulary. Though these estimation methods are widely used, they
are not systematically evaluated, especially on end tasks. We introduce an
evaluation of a comprehensive collection of decoding-free candidate selection
approaches on a comprehensive set of tasks, including five multiple-choice QA
tasks with a small candidate pool and four clinical decision tasks with a
massive amount of candidates, some with 10k+ options. We evaluate the
estimation methods paired with a wide spectrum of foundation LMs covering
different architectures, sizes and training paradigms. The results and insights
from our analysis inform the future model design.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attribution analysis of legal language as used by LLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17330v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17330v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Richard K. Belew
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Three publicly-available LLM specifically designed for legal tasks have been
implemented and shown that classification accuracy can benefit from training
over legal corpora, but why and how? Here we use two publicly-available legal
datasets, a simpler binary classification task of ``overruling'' texts, and a
more elaborate multiple choice task identifying ``holding'' judicial decisions.
We report on experiments contrasting the legal LLM and a generic BERT model for
comparison, against both datasets. We use integrated gradient attribution
techniques to impute ``causes'' of variation in the models' perfomance, and
characterize them in terms of the tokenizations each use. We find that while
all models can correctly classify some test examples from the casehold task,
other examples can only be identified by only one, model, and attribution can
be used to highlight the reasons for this. We find that differential behavior
of the models' tokenizers accounts for most of the difference and analyze these
differences in terms of the legal language they process. Frequency analysis of
tokens generated by dataset texts, combined with use of known ``stop word''
lists, allow identification of tokens that are clear signifiers of legal
topics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Memorize and Rank: Elevating Large Language Models for Clinical
  Diagnosis Prediction <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17326v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17326v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyu Derek Ma, Xiaoxuan Wang, Yijia Xiao, Anthony Cuturrufo, Vijay S Nori, Eran Halperin, Wei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Clinical diagnosis prediction models, when provided with a patient's medical
history, aim to detect potential diseases early, facilitating timely
intervention and improving prognostic outcomes. However, the inherent scarcity
of patient data and large disease candidate space often pose challenges in
developing satisfactory models for this intricate task. The exploration of
leveraging Large Language Models (LLMs) for encapsulating clinical decision
processes has been limited. We introduce MERA, a clinical diagnosis prediction
model that bridges pertaining natural language knowledge with medical practice.
We apply hierarchical contrastive learning on a disease candidate ranking list
to alleviate the large decision space issue. With concept memorization through
fine-tuning, we bridge the natural language clinical knowledge with medical
codes. Experimental results on MIMIC-III and IV datasets show that MERA
achieves the state-of-the-art diagnosis prediction performance and dramatically
elevates the diagnosis prediction capabilities of generative LMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ "Ownership, Not Just Happy Talk": Co-Designing a Participatory Large
  Language Model for Journalism 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17299v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17299v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emily Tseng, Meg Young, Marianne Aubin Le Quéré, Aimee Rinehart, Harini Suresh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Journalism has emerged as an essential domain for understanding the uses,
limitations, and impacts of large language models (LLMs) in the workplace. News
organizations face divergent financial incentives: LLMs already permeate
newswork processes within financially constrained organizations, even as
ongoing legal challenges assert that AI companies violate their copyright. At
stake are key questions about what LLMs are created to do, and by whom: How
might a journalist-led LLM work, and what can participatory design illuminate
about the present-day challenges about adapting ``one-size-fits-all''
foundation models to a given context of use? In this paper, we undertake a
co-design exploration to understand how a participatory approach to LLMs might
address opportunities and challenges around AI in journalism. Our 20 interviews
with reporters, data journalists, editors, labor organizers, product leads, and
executives highlight macro, meso, and micro tensions that designing for this
opportunity space must address. From these desiderata, we describe the result
of our co-design work: organizational structures and functionality for a
journalist-controlled LLM. In closing, we discuss the limitations of commercial
foundation models for workplace use, and the methodological implications of
applying participatory methods to LLM co-design.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review for an ACM conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating Hallucinated Translations in Large Language Models with
  Hallucination-focused Preference Optimization <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17295v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17295v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zilu Tang, Rajen Chatterjee, Sarthak Garg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine Translation (MT) is undergoing a paradigm shift, with systems based
on fine-tuned large language models (LLM) becoming increasingly competitive
with traditional encoder-decoder models trained specifically for translation
tasks. However, LLM-based systems are at a higher risk of generating
hallucinations, which can severely undermine user's trust and safety. Most
prior research on hallucination mitigation focuses on traditional MT models,
with solutions that involve post-hoc mitigation - detecting hallucinated
translations and re-translating them. While effective, this approach introduces
additional complexity in deploying extra tools in production and also increases
latency. To address these limitations, we propose a method that intrinsically
learns to mitigate hallucinations during the model training phase.
Specifically, we introduce a data creation framework to generate hallucination
focused preference datasets. Fine-tuning LLMs on these preference datasets
reduces the hallucination rate by an average of 96% across five language pairs,
while preserving overall translation quality. In a zero-shot setting our
approach reduces hallucinations by 89% on an average across three unseen target
languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2025 Main Conference Long paper (9 pages)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fine-Tuning Open-Source Large Language Models to Improve Their
  Performance on Radiation Oncology Tasks: A Feasibility Study to Investigate
  Their Potential Clinical Applications in Radiation Oncology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17286v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17286v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peilong Wang, Zhengliang Liu, Yiwei Li, Jason Holmes, Peng Shu, Lian Zhang, Xiang Li, Quanzheng Li, Brady S. Laughlin, Diego Santos Toesca, Sujay A. Vora, Samir H. Patel, Terence T. Sio, Tianming Liu, Wei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Background: The radiation oncology clinical practice involves many steps
relying on the dynamic interplay of abundant text data. Large language models
have displayed remarkable capabilities in processing complex text information.
But their direct applications in specific fields like radiation oncology remain
underexplored.
  Purpose: This study aims to investigate whether fine-tuning LLMs with domain
knowledge can improve the performance on Task (1) treatment regimen generation,
Task (2) treatment modality selection (photon, proton, electron, or
brachytherapy), and Task (3) ICD-10 code prediction in radiation oncology.
  Methods: Data for 15,724 patient cases were extracted. Cases where patients
had a single diagnostic record, and a clearly identifiable primary treatment
plan were selected for preprocessing and manual annotation to have 7,903 cases
of the patient diagnosis, treatment plan, treatment modality, and ICD-10 code.
Each case was used to construct a pair consisting of patient diagnostics
details and an answer (treatment regimen, treatment modality, or ICD-10 code
respectively) for the supervised fine-tuning of these three tasks. Open source
LLaMA2-7B and Mistral-7B models were utilized for the fine-tuning with the
Low-Rank Approximations method. Accuracy and ROUGE-1 score were reported for
the fine-tuned models and original models. Clinical evaluation was performed on
Task (1) by radiation oncologists, while precision, recall, and F-1 score were
evaluated for Task (2) and (3). One-sided Wilcoxon signed-rank tests were used
to statistically analyze the results.
  Results: Fine-tuned LLMs outperformed original LLMs across all tasks with
p-value <= 0.001. Clinical evaluation demonstrated that over 60% of the
fine-tuned LLMs-generated treatment regimens were clinically acceptable.
Precision, recall, and F1-score showed improved performance of fine-tuned LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tailored Truths: Optimizing LLM Persuasion with Personalization and
  Fabricated Statistics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17273v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17273v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jasper Timm, Chetan Talele, Jacob Haimes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are becoming increasingly persuasive,
demonstrating the ability to personalize arguments in conversation with humans
by leveraging their personal data. This may have serious impacts on the scale
and effectiveness of disinformation campaigns. We studied the persuasiveness of
LLMs in a debate setting by having humans $(n=33)$ engage with LLM-generated
arguments intended to change the human's opinion. We quantified the LLM's
effect by measuring human agreement with the debate's hypothesis pre- and
post-debate and analyzing both the magnitude of opinion change, as well as the
likelihood of an update in the LLM's direction. We compare persuasiveness
across established persuasion strategies, including personalized arguments
informed by user demographics and personality, appeal to fabricated statistics,
and a mixed strategy utilizing both personalized arguments and fabricated
statistics. We found that static arguments generated by humans and GPT-4o-mini
have comparable persuasive power. However, the LLM outperformed static
human-written arguments when leveraging the mixed strategy in an interactive
debate setting. This approach had a $\mathbf{51\%}$ chance of persuading
participants to modify their initial position, compared to $\mathbf{32\%}$ for
the static human-written arguments. Our results highlight the concerning
potential for LLMs to enable inexpensive and persuasive large-scale
disinformation campaigns.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comprehensive Evaluation for a Large Scale Knowledge Graph Question
  Answering Service 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17270v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17270v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saloni Potdar, Daniel Lee, Omar Attia, Varun Embar, De Meng, Ramesh Balaji, Chloe Seivwright, Eric Choi, Mina H. Farid, Yiwen Sun, Yunyao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Question answering systems for knowledge graph (KGQA), answer factoid
questions based on the data in the knowledge graph. KGQA systems are complex
because the system has to understand the relations and entities in the
knowledge-seeking natural language queries and map them to structured queries
against the KG to answer them. In this paper, we introduce Chronos, a
comprehensive evaluation framework for KGQA at industry scale. It is designed
to evaluate such a multi-component system comprehensively, focusing on (1)
end-to-end and component-level metrics, (2) scalable to diverse datasets and
(3) a scalable approach to measure the performance of the system prior to
release. In this paper, we discuss the unique challenges associated with
evaluating KGQA systems at industry scale, review the design of Chronos, and
how it addresses these challenges. We will demonstrate how it provides a base
for data-driven decisions and discuss the challenges of using it to measure and
improve a real-world KGQA system.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Giving the Old a Fresh Spin: Quality Estimation-Assisted Constrained
  Decoding for Automatic Post-Editing <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17265v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17265v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sourabh Deoghare, Diptesh Kanojia, Pushpak Bhattacharyya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic Post-Editing (APE) systems often struggle with over-correction,
where unnecessary modifications are made to a translation, diverging from the
principle of minimal editing. In this paper, we propose a novel technique to
mitigate over-correction by incorporating word-level Quality Estimation (QE)
information during the decoding process. This method is architecture-agnostic,
making it adaptable to any APE system, regardless of the underlying model or
training approach. Our experiments on English-German, English-Hindi, and
English-Marathi language pairs show the proposed approach yields significant
improvements over their corresponding baseline APE systems, with TER gains of
$0.65$, $1.86$, and $1.44$ points, respectively. These results underscore the
complementary relationship between QE and APE tasks and highlight the
effectiveness of integrating QE information to reduce over-correction in APE
systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2025 Main Conference: Short Papers</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FactCG: Enhancing Fact Checkers with Graph-Based Multi-Hop Data <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17144v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17144v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deren Lei, Yaxi Li, Siyao Li, Mengya Hu, Rui Xu, Ken Archer, Mingyu Wang, Emily Ching, Alex Deng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prior research on training grounded factuality classification models to
detect hallucinations in large language models (LLMs) has relied on public
natural language inference (NLI) data and synthetic data. However, conventional
NLI datasets are not well-suited for document-level reasoning, which is
critical for detecting LLM hallucinations. Recent approaches to document-level
synthetic data generation involve iteratively removing sentences from documents
and annotating factuality using LLM-based prompts. While effective, this method
is computationally expensive for long documents and limited by the LLM's
capabilities. In this work, we analyze the differences between existing
synthetic training data used in state-of-the-art models and real LLM output
claims. Based on our findings, we propose a novel approach for synthetic data
generation, CG2C, that leverages multi-hop reasoning on context graphs
extracted from documents. Our fact checker model, FactCG, demonstrates improved
performance with more connected reasoning, using the same backbone models.
Experiments show it even outperforms GPT-4-o on the LLM-Aggrefact benchmark
with much smaller model size.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ASTRAL: Automated Safety Testing of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17132v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17132v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miriam Ugarte, Pablo Valle, José Antonio Parejo, Sergio Segura, Aitor Arrieta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have recently gained attention due to their
ability to understand and generate sophisticated human-like content. However,
ensuring their safety is paramount as they might provide harmful and unsafe
responses. Existing LLM testing frameworks address various safety-related
concerns (e.g., drugs, terrorism, animal abuse) but often face challenges due
to unbalanced and obsolete datasets. In this paper, we present ASTRAL, a tool
that automates the generation and execution of test cases (i.e., prompts) for
testing the safety of LLMs. First, we introduce a novel black-box coverage
criterion to generate balanced and diverse unsafe test inputs across a diverse
set of safety categories as well as linguistic writing characteristics (i.e.,
different style and persuasive writing techniques). Second, we propose an
LLM-based approach that leverages Retrieval Augmented Generation (RAG),
few-shot prompting strategies and web browsing to generate up-to-date test
inputs. Lastly, similar to current LLM test automation techniques, we leverage
LLMs as test oracles to distinguish between safe and unsafe test outputs,
allowing a fully automated testing approach. We conduct an extensive evaluation
on well-known LLMs, revealing the following key findings: i) GPT3.5 outperforms
other LLMs when acting as the test oracle, accurately detecting unsafe
responses, and even surpassing more recent LLMs (e.g., GPT-4), as well as LLMs
that are specifically tailored to detect unsafe LLM outputs (e.g., LlamaGuard);
ii) the results confirm that our approach can uncover nearly twice as many
unsafe LLM behaviors with the same number of test inputs compared to currently
used static datasets; and iii) our black-box coverage criterion combined with
web browsing can effectively guide the LLM on generating up-to-date unsafe test
inputs, significantly increasing the number of unsafe LLM behaviors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Histoires Morales: A French <span class="highlight-title">Dataset</span> for Assessing Moral Alignment <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17117v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17117v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thibaud Leteno, Irina Proskurina, Antoine Gourru, Julien Velcin, Charlotte Laclau, Guillaume Metzler, Christophe Gravier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aligning language models with human values is crucial, especially as they
become more integrated into everyday life. While models are often adapted to
user preferences, it is equally important to ensure they align with moral norms
and behaviours in real-world social situations. Despite significant progress in
languages like English and Chinese, French has seen little attention in this
area, leaving a gap in understanding how LLMs handle moral reasoning in this
language. To address this gap, we introduce Histoires Morales, a French dataset
derived from Moral Stories, created through translation and subsequently
refined with the assistance of native speakers to guarantee grammatical
accuracy and adaptation to the French cultural context. We also rely on
annotations of the moral values within the dataset to ensure their alignment
with French norms. Histoires Morales covers a wide range of social situations,
including differences in tipping practices, expressions of honesty in
relationships, and responsibilities toward animals. To foster future research,
we also conduct preliminary experiments on the alignment of multilingual models
on French and English data and the robustness of the alignment. We find that
while LLMs are generally aligned with human moral norms by default, they can be
easily influenced with user-preference optimization for both moral and immoral
data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing Large Language Model Training Using FP4 Quantization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17116v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17116v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruizhe Wang, Yeyun Gong, Xiao Liu, Guoshuai Zhao, Ziyue Yang, Baining Guo, Zhengjun Zha, Peng Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing computational demands of training large language models (LLMs)
necessitate more efficient methods. Quantized training presents a promising
solution by enabling low-bit arithmetic operations to reduce these costs. While
FP8 precision has demonstrated feasibility, leveraging FP4 remains a challenge
due to significant quantization errors and limited representational capacity.
This work introduces the first FP4 training framework for LLMs, addressing
these challenges with two key innovations: a differentiable quantization
estimator for precise weight updates and an outlier clamping and compensation
strategy to prevent activation collapse. To ensure stability, the framework
integrates a mixed-precision training scheme and vector-wise quantization.
Experimental results demonstrate that our FP4 framework achieves accuracy
comparable to BF16 and FP8, with minimal degradation, scaling effectively to
13B-parameter LLMs trained on up to 100B tokens. With the emergence of
next-generation hardware supporting FP4, our framework sets a foundation for
efficient ultra-low precision training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ COS(M+O)S: Curiosity and RL-Enhanced MCTS for Exploring Story Space via
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17104v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17104v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tobias Materzok
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present COS(M+O)S, a System 2-inspired framework for open-ended plot
development that systematically explores the vast space of possible story
expansions, enabling a 3B-parameter language model to approach the plot quality
of a 70B model on select short-story tasks. The method accomplishes this by
combining Monte Carlo Tree Search (MCTS), guided by a step-level value model
that rewards moderate surprisal (curiosity) while penalizing incoherence, and
Odds Ratio Preference Optimization (ORPO) to fine-tune the policy on high-value
plot expansions. This iterative reinforcement learning loop systematically
explores multiple candidate plot branches, backpropagates quality signals, and
adapts the policy for faster convergence, notably shifting the policy from
puzzle-based Chain-of-Thought to more character-driven storytelling. In
small-scale tests with short-story prompts, 67%-77% of participants favored
COS(M+O)S's highest-rated expansions over lower-rated ones, suggesting that our
learned value function aligns. GPT-4o ratings further show that COS(M+O)S
surpasses naive single-pass decoding from Llama 3.2 3B by 0.59 SD, coming
within 0.06 SD of Llama 3.1 70B (no significant difference, p=0.93). Pairwise
comparisons with o1 place COS(M+O)S 1.5 SD above the 3B baseline and find no
statistically significant gap from 70B. Nevertheless, absolute story quality
remains modest, constrained by the small model's capacity and limited training
data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FlexCap: Describe Anything in Images in Controllable Detail <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12026v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12026v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Debidatta Dwibedi, Vidhi Jain, Jonathan Tompson, Andrew Zisserman, Yusuf Aytar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce FlexCap, a vision-language model that generates region-specific
descriptions of varying lengths. FlexCap is trained to produce
length-conditioned captions for input boxes, enabling control over information
density, with descriptions ranging from concise object labels to detailed
captions. To achieve this, we create large-scale training datasets of image
region descriptions with varying lengths from captioned web images. We
demonstrate FlexCap's effectiveness in several applications: first, it achieves
strong performance in dense captioning tasks on the Visual Genome dataset.
Second, we show how FlexCap's localized descriptions can serve as input to a
large language model to create a visual question answering (VQA) system,
achieving state-of-the-art zero-shot performance on multiple VQA benchmarks.
Our experiments illustrate FlexCap's utility for tasks including image
labeling, object attribute recognition, and visual dialog. Project webpage:
https://flex-cap.github.io .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mitigating Memorization In Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02159v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02159v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mansi Sakarvadia, Aswathy Ajith, Arham Khan, Nathaniel Hudson, Caleb Geniesse, Kyle Chard, Yaoqing Yang, Ian Foster, Michael W. Mahoney
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models (LMs) can "memorize" information, i.e., encode training data
in their weights in such a way that inference-time queries can lead to verbatim
regurgitation of that data. This ability to extract training data can be
problematic, for example, when data are private or sensitive. In this work, we
investigate methods to mitigate memorization: three regularizer-based, three
finetuning-based, and eleven machine unlearning-based methods, with five of the
latter being new methods that we introduce. We also introduce TinyMem, a suite
of small, computationally-efficient LMs for the rapid development and
evaluation of memorization-mitigation methods. We demonstrate that the
mitigation methods that we develop using TinyMem can successfully be applied to
production-grade LMs, and we determine via experiment that: regularizer-based
mitigation methods are slow and ineffective at curbing memorization;
fine-tuning-based methods are effective at curbing memorization, but overly
expensive, especially for retaining higher accuracies; and unlearning-based
methods are faster and more effective, allowing for the precise localization
and removal of memorized information from LM weights prior to inference. We
show, in particular, that our proposed unlearning method BalancedSubnet
outperforms other mitigation methods at removing memorized information while
preserving performance on target tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Simple Averaging: Improving NLP Ensemble Performance with
  Topological-Data-Analysis-Based Weighting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.14184v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.14184v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Polina Proskura, Alexey Zaytsev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In machine learning, ensembles are important tools for improving the model
performance. In natural language processing specifically, ensembles boost the
performance of a method due to multiple large models available in open source.
However, existing approaches mostly rely on simple averaging of predictions by
ensembles with equal weights for each model, ignoring differences in the
quality and conformity of models. We propose to estimate weights for ensembles
of NLP models using not only knowledge of their individual performance but also
their similarity to each other. By adopting distance measures based on
Topological Data Analysis (TDA), we improve our ensemble. The quality improves
for both text classification accuracy and relevant uncertainty estimation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accelerating Large Language Model <span class="highlight-title">Pretrain</span>ing via LFR Pedagogy: Learn,
  Focus, and <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06131v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06131v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neha Prakriya, Jui-Nan Yen, Cho-Jui Hsieh, Jason Cong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional Large Language Model (LLM) pretraining relies on autoregressive
language modeling with randomly sampled data from web-scale datasets. Inspired
by human learning techniques like spaced repetition, we hypothesize that random
sampling leads to high training costs, lower-quality models, and significant
data forgetting. To address these inefficiencies, we propose the
Learn-Focus-Review (LFR) paradigm -- a dynamic training approach that adapts to
the model's learning progress. LFR tracks the model's learning performance
across data blocks (sequences of tokens) and prioritizes revisiting challenging
regions of the dataset that are more prone to being forgotten, enabling better
retention and more efficient learning. Using the LFR paradigm, we pretrained
Llama and GPT models on the SlimPajama and OpenWebText datasets, respectively.
These models were evaluated on downstream tasks across various domains,
including question answering, problem-solving, commonsense reasoning, language
modeling, and translation. Compared to baseline models trained on the full
datasets, LFR consistently achieved lower perplexity and higher accuracy, while
using only 5%--19% of the training tokens. Furthermore, LFR matched the
performance of industry-standard Pythia models with up to 2$\times$ the
parameter count, using just 3.2% of the training tokens, demonstrating its
effectiveness and efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Coupling without Communication and Drafter-Invariant Speculative
  Decoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.07978v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.07978v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Majid Daliri, Christopher Musco, Ananda Theertha Suresh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Suppose Alice has a distribution $P$ and Bob has a distribution $Q$. Alice
wants to draw a sample $a\sim P$ and Bob a sample $b \sim Q$ such that $a = b$
with as high of probability as possible. It is well-known that, by sampling
from an optimal coupling between the distributions, Alice and Bob can achieve
$\Pr[a = b] = 1 - D_{TV}(P,Q)$, where $D_{TV}(P,Q)$ is the total variation
distance between $P$ and $Q$. What if Alice and Bob must solve this same
problem \emph{without communicating at all?} Perhaps surprisingly, with access
to public randomness, they can still achieve $\Pr[a = b] \geq \frac{1 -
D_{TV}(P,Q)}{1 + D_{TV}(P,Q)} \geq 1-2D_{TV}(P,Q)$ using a simple protocol
based on the Weighted MinHash algorithm. This bound was shown to be optimal in
the worst-case by [Bavarian et al., 2020]. In this work, we revisit the
communication-free coupling problem. We provide a simpler proof of the
optimality result from [Bavarian et al., 2020]. We show that, while the
worst-case success probability of Weighted MinHash cannot be improved, an
equally simple protocol based on Gumbel sampling offers a Pareto improvement:
for every pair of distributions $P, Q$, Gumbel sampling achieves an equal or
higher value of $\Pr[a = b]$ than Weighted MinHash. Importantly, this
improvement translates to practice. We demonstrate an application of
communication-free coupling to \emph{speculative decoding}, a recent method for
accelerating autoregressive large language models [Leviathan, Kalman, Matias,
ICML 2023]. We show that communication-free protocols can be used to contruct
\emph{\CSD{}} schemes, which have the desirable property that their output is
fixed given a fixed random seed, regardless of what drafter is used for
speculation. In experiments on a language generation task, Gumbel sampling
outperforms Weighted MinHash. Code is available at
https://github.com/majid-daliri/DISD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-reflecting Large Language Models: A Hegelian Dialectical Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14917v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14917v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sara Abdali, Can Goksen, Saeed Amizadeh, Kazuhito Koishida
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Investigating NLP through a philosophical lens has recently caught
researcher's eyes as it connects computational methods with classical schools
of philosophy. This paper introduces a philosophical approach inspired by the
Hegelian Dialectic for LLMs' self-reflection, utilizing a self-dialectical
approach to emulate internal critiques and then synthesize new ideas by
resolving the contradicting points. Moreover, this paper investigates the
effect of LLMs' temperature for generation by establishing a dynamic annealing
approach, which promotes the creativity in the early stages and gradually
refines it by focusing on the nuances, as well as a fixed temperature strategy
for generation. Our proposed approach is examined to determine its ability to
generate novel ideas from an initial proposition. Additionally, a Multi Agent
Majority Voting (MAMV) strategy is leveraged to assess the validity and novelty
of the generated ideas, which proves beneficial in the absence of domain
experts. Our experiments show promise in generating new ideas and provide a
stepping stone for future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adversarial Vulnerabilities in Large Language Models for Time Series
  Forecasting <span class="chip">AISTATS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08099v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08099v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fuqiang Liu, Sicong Jiang, Luis Miranda-Moreno, Seongjin Choi, Lijun Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have recently demonstrated significant potential
in the field of time series forecasting, offering impressive capabilities in
handling complex temporal data. However, their robustness and reliability in
real-world applications remain under-explored, particularly concerning their
susceptibility to adversarial attacks. In this paper, we introduce a targeted
adversarial attack framework for LLM-based time series forecasting. By
employing both gradient-free and black-box optimization methods, we generate
minimal yet highly effective perturbations that significantly degrade the
forecasting accuracy across multiple datasets and LLM architectures. Our
experiments, which include models like TimeGPT and LLM-Time with GPT-3.5,
GPT-4, LLaMa, and Mistral, show that adversarial attacks lead to much more
severe performance degradation than random noise, and demonstrate the broad
effectiveness of our attacks across different LLMs. The results underscore the
critical vulnerabilities of LLMs in time series forecasting, highlighting the
need for robust defense mechanisms to ensure their reliable deployment in
practical applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AISTATS 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">8</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhanced Retrieval of Long Documents: Leveraging Fine-Grained Block
  Representations with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17039v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17039v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minghan Li, Eric Gaussier, Guodong Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, large language models (LLMs) have demonstrated exceptional
power in various domains, including information retrieval. Most of the previous
practices involve leveraging these models to create a single embedding for each
query, each passage, or each document individually, a strategy exemplified and
used by the Retrieval-Augmented Generation (RAG) framework. While this method
has proven effective, we argue that it falls short in fully capturing the
nuanced intricacies of document-level texts due to its reliance on a relatively
coarse-grained representation. To address this limitation, we introduce a
novel, fine-grained approach aimed at enhancing the accuracy of relevance
scoring for long documents. Our methodology firstly segments a long document
into blocks, each of which is embedded using an LLM, for matching with the
query representation. When calculating the relevance score, we aggregate the
query-block relevance scores through a weighted sum method, yielding a
comprehensive score for the query with the entire document. Despite its
apparent simplicity, our experimental findings reveal that this approach
outperforms standard representation methods and achieves a significant
reduction in embedding generation latency. Moreover, by carefully optimizing
pairwise loss functions, superior performances have been achieved.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Document Screenshot Retrievers are Vulnerable to Pixel Poisoning Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16902v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16902v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengyao Zhuang, Ekaterina Khramtsova, Xueguang Ma, Bevan Koopman, Jimmy Lin, Guido Zuccon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in dense retrieval have introduced vision-language model
(VLM)-based retrievers, such as DSE and ColPali, which leverage document
screenshots embedded as vectors to enable effective search and offer a
simplified pipeline over traditional text-only methods. In this study, we
propose three pixel poisoning attack methods designed to compromise VLM-based
retrievers and evaluate their effectiveness under various attack settings and
parameter configurations. Our empirical results demonstrate that injecting even
a single adversarial screenshot into the retrieval corpus can significantly
disrupt search results, poisoning the top-10 retrieved documents for 41.9% of
queries in the case of DSE and 26.4% for ColPali. These vulnerability rates
notably exceed those observed with equivalent attacks on text-only retrievers.
Moreover, when targeting a small set of known queries, the attack success rate
raises, achieving complete success in certain cases. By exposing the
vulnerabilities inherent in vision-language models, this work highlights the
potential risks associated with their deployment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Secure Federated Graph-Filtering for Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16888v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16888v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julien Nicolas, César Sabater, Mohamed Maouche, Sonia Ben Mokhtar, Mark Coates
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems often rely on graph-based filters, such as normalized
item-item adjacency matrices and low-pass filters. While effective, the
centralized computation of these components raises concerns about privacy,
security, and the ethical use of user data. This work proposes two
decentralized frameworks for securely computing these critical graph components
without centralizing sensitive information. The first approach leverages
lightweight Multi-Party Computation and distributed singular vector
computations to privately compute key graph filters. The second extends this
framework by incorporating low-rank approximations, enabling a trade-off
between communication efficiency and predictive performance. Empirical
evaluations on benchmark datasets demonstrate that the proposed methods achieve
comparable accuracy to centralized state-of-the-art systems while ensuring data
confidentiality and maintaining low communication costs. Our results highlight
the potential for privacy-preserving decentralized architectures to bridge the
gap between utility and user data protection in modern recommender systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hypergraph Diffusion for High-Order Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16722v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16722v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Darnbi Sakong, Thanh Trung Huynh, Jun Jo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems rely on Collaborative Filtering (CF) to predict user
preferences by leveraging patterns in historical user-item interactions. While
traditional CF methods primarily focus on learning compact vector embeddings
for users and items, graph neural network (GNN)-based approaches have emerged
as a powerful alternative, utilizing the structure of user-item interaction
graphs to enhance recommendation accuracy. However, existing GNN-based models,
such as LightGCN and UltraGCN, often struggle with two major limitations: an
inability to fully account for heterophilic interactions, where users engage
with diverse item categories, and the over-smoothing problem in multi-layer
GNNs, which hinders their ability to model complex, high-order relationships.
To address these gaps, we introduce WaveHDNN, an innovative wavelet-enhanced
hypergraph diffusion framework. WaveHDNN integrates a Heterophily-aware
Collaborative Encoder, designed to capture user-item interactions across
diverse categories, with a Multi-scale Group-wise Structure Encoder, which
leverages wavelet transforms to effectively model localized graph structures.
Additionally, cross-view contrastive learning is employed to maintain robust
and consistent representations. Experiments on benchmark datasets validate the
efficacy of WaveHDNN, demonstrating its superior ability to capture both
heterophilic and localized structural information, leading to improved
recommendation performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VeriFact: Verifying Facts in LLM-Generated Clinical Text with Electronic
  Health Records 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16672v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16672v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philip Chung, Akshay Swaminathan, Alex J. Goodell, Yeasul Kim, S. Momsen Reincke, Lichy Han, Ben Deverett, Mohammad Amin Sadeghi, Abdel-Badih Ariss, Marc Ghanem, David Seong, Andrew A. Lee, Caitlin E. Coombes, Brad Bradshaw, Mahir A. Sufian, Hyo Jung Hong, Teresa P. Nguyen, Mohammad R. Rasouli, Komal Kamra, Mark A. Burbridge, James C. McAvoy, Roya Saffary, Stephen P. Ma, Dev Dash, James Xie, Ellen Y. Wang, Clifford A. Schmiesing, Nigam Shah, Nima Aghaeepour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Methods to ensure factual accuracy of text generated by large language models
(LLM) in clinical medicine are lacking. VeriFact is an artificial intelligence
system that combines retrieval-augmented generation and LLM-as-a-Judge to
verify whether LLM-generated text is factually supported by a patient's medical
history based on their electronic health record (EHR). To evaluate this system,
we introduce VeriFact-BHC, a new dataset that decomposes Brief Hospital Course
narratives from discharge summaries into a set of simple statements with
clinician annotations for whether each statement is supported by the patient's
EHR clinical notes. Whereas highest agreement between clinicians was 88.5%,
VeriFact achieves up to 92.7% agreement when compared to a denoised and
adjudicated average human clinican ground truth, suggesting that VeriFact
exceeds the average clinician's ability to fact-check text against a patient's
medical record. VeriFact may accelerate the development of LLM-based EHR
applications by removing current evaluation bottlenecks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>62 pages, 5 figures, 1 table, pre-print manuscript</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Foundational Large Language Models for Materials Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09560v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09560v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vaibhav Mishra, Somaditya Singh, Dhruv Ahlawat, Mohd Zaki, Vaibhav Bihani, Hargun Singh Grover, Biswajit Mishra, Santiago Miret,  Mausam, N. M. Anoop Krishnan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Materials discovery and development are critical for addressing global
challenges. Yet, the exponential growth in materials science literature
comprising vast amounts of textual data has created significant bottlenecks in
knowledge extraction, synthesis, and scientific reasoning. Large Language
Models (LLMs) offer unprecedented opportunities to accelerate materials
research through automated analysis and prediction. Still, their effective
deployment requires domain-specific adaptation for understanding and solving
domain-relevant tasks. Here, we present LLaMat, a family of foundational models
for materials science developed through continued pretraining of LLaMA models
on an extensive corpus of materials literature and crystallographic data.
Through systematic evaluation, we demonstrate that LLaMat excels in
materials-specific NLP and structured information extraction while maintaining
general linguistic capabilities. The specialized LLaMat-CIF variant
demonstrates unprecedented capabilities in crystal structure generation,
predicting stable crystals with high coverage across the periodic table.
Intriguingly, despite LLaMA-3's superior performance in comparison to LLaMA-2,
we observe that LLaMat-2 demonstrates unexpectedly enhanced domain-specific
performance across diverse materials science tasks, including structured
information extraction from text and tables, more particularly in crystal
structure generation, a potential adaptation rigidity in overtrained LLMs.
Altogether, the present work demonstrates the effectiveness of domain
adaptation towards developing practically deployable LLM copilots for materials
research. Beyond materials science, our findings reveal important
considerations for domain adaptation of LLMs, such as model selection, training
methodology, and domain-specific performance, which may influence the
development of specialized scientific AI systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging Passage Embeddings for Efficient Listwise Reranking with
  Large Language Models <span class="chip">WWW 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14848v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14848v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Liu, Bo Wang, Nan Wang, Jiaxin Mao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have demonstrated the effectiveness of using large language
language models (LLMs) in passage ranking. The listwise approaches, such as
RankGPT, have become new state-of-the-art in this task. However, the efficiency
of RankGPT models is limited by the maximum context length and relatively high
latency of LLM inference. To address these issues, in this paper, we propose
PE-Rank, leveraging the single passage embedding as a good context compression
for efficient listwise passage reranking. By treating each passage as a special
token, we can directly input passage embeddings into LLMs, thereby reducing
input length. Additionally, we introduce an inference method that dynamically
constrains the decoding space to these special tokens, accelerating the
decoding process. For adapting the model to reranking, we employ listwise
learning to rank loss for training. Evaluation results on multiple benchmarks
demonstrate that PE-Rank significantly improves efficiency in both prefilling
and decoding, while maintaining competitive ranking effectiveness. The Code is
available at https://github.com/liuqi6777/pe_rank.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by WWW 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generating Negative Samples for Multi-Modal Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15183v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15183v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanbiao Ji, Yue Ding, Dan Luo, Chang Liu, Jing Tong, Shaokai Wu, Hongtao Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal recommender systems (MMRS) have gained significant attention due
to their ability to leverage information from various modalities to enhance
recommendation quality. However, existing negative sampling techniques often
struggle to effectively utilize the multi-modal data, leading to suboptimal
performance. In this paper, we identify two key challenges in negative sampling
for MMRS: (1) producing cohesive negative samples contrasting with positive
samples and (2) maintaining a balanced influence across different modalities.
To address these challenges, we propose NegGen, a novel framework that utilizes
multi-modal large language models (MLLMs) to generate balanced and contrastive
negative samples. We design three different prompt templates to enable NegGen
to analyze and manipulate item attributes across multiple modalities, and then
generate negative samples that introduce better supervision signals and ensure
modality balance. Furthermore, NegGen employs a causal learning module to
disentangle the effect of intervened key features and irrelevant item
attributes, enabling fine-grained learning of user preferences. Extensive
experiments on real-world datasets demonstrate the superior performance of
NegGen compared to state-of-the-art methods in both negative sampling and
multi-modal recommendation.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MIDI-<span class="highlight-title">GPT</span>: A Controllable Generative Model for Computer-Assisted
  Multitrack Music Composition <span class="chip">AAAI 25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17011v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17011v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philippe Pasquier, Jeff Ens, Nathan Fradet, Paul Triana, Davide Rizzotti, Jean-Baptiste Rolland, Maryam Safi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present and release MIDI-GPT, a generative system based on the Transformer
architecture that is designed for computer-assisted music composition
workflows. MIDI-GPT supports the infilling of musical material at the track and
bar level, and can condition generation on attributes including: instrument
type, musical style, note density, polyphony level, and note duration. In order
to integrate these features, we employ an alternative representation for
musical material, creating a time-ordered sequence of musical events for each
track and concatenating several tracks into a single sequence, rather than
using a single time-ordered sequence where the musical events corresponding to
different tracks are interleaved. We also propose a variation of our
representation allowing for expressiveness. We present experimental results
that demonstrate that MIDI-GPT is able to consistently avoid duplicating the
musical material it was trained on, generate music that is stylistically
similar to the training dataset, and that attribute controls allow enforcing
various constraints on the generated material. We also outline several
real-world applications of MIDI-GPT, including collaborations with industry
partners that explore the integration and evaluation of MIDI-GPT into
commercial products, as well as several artistic works produced using it.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AVE Speech <span class="highlight-title">Dataset</span>: A Comprehensive Benchmark for Multi-Modal Speech
  Recognition Integrating Audio, Visual, and Electromyographic Signals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16780v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16780v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongliang Zhou, Yakun Zhang, Jinghan Wu, Xingyu Zhang, Liang Xie, Erwei Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The global aging population faces considerable challenges, particularly in
communication, due to the prevalence of hearing and speech impairments. To
address these, we introduce the AVE speech dataset, a comprehensive multi-modal
benchmark for speech recognition tasks. The dataset includes a 100-sentence
Mandarin Chinese corpus with audio signals, lip-region video recordings, and
six-channel electromyography (EMG) data, collected from 100 participants. Each
subject read the entire corpus ten times, with each sentence averaging
approximately two seconds in duration, resulting in over 55 hours of
multi-modal speech data per modality. Experiments demonstrate that combining
these modalities significantly improves recognition performance, particularly
in cross-subject and high-noise environments. To our knowledge, this is the
first publicly available sentence-level dataset integrating these three
modalities for large-scale Mandarin speech recognition. We expect this dataset
to drive advancements in both acoustic and non-acoustic speech recognition
research, enhancing cross-modal learning and human-machine interaction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Audio-Visual Deepfake Detection With Local Temporal Inconsistencies <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08137v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08137v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marcella Astrid, Enjie Ghorbel, Djamila Aouada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes an audio-visual deepfake detection approach that aims to
capture fine-grained temporal inconsistencies between audio and visual
modalities. To achieve this, both architectural and data synthesis strategies
are introduced. From an architectural perspective, a temporal distance map,
coupled with an attention mechanism, is designed to capture these
inconsistencies while minimizing the impact of irrelevant temporal
subsequences. Moreover, we explore novel pseudo-fake generation techniques to
synthesize local inconsistencies. Our approach is evaluated against
state-of-the-art methods using the DFDC and FakeAVCeleb datasets, demonstrating
its effectiveness in detecting audio-visual deepfakes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-27T00:00:00Z">2025-01-27</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">18</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 360Brew: A Decoder-only Foundation Model for Personalized Ranking and
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16450v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16450v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hamed Firooz, Maziar Sanjabi, Adrian Englhardt, Aman Gupta, Ben Levine, Dre Olgiati, Gungor Polatkan, Iuliia Melnychuk, Karthik Ramgopal, Kirill Talanine, Kutta Srinivasan, Luke Simon, Natesh Sivasubramoniapillai, Necip Fazil Ayan, Qingquan Song, Samira Sriram, Souvik Ghosh, Tao Song, Vignesh Kothapalli, Xiaoling Zhai, Ya Xu, Yu Wang, Yun Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ranking and recommendation systems are the foundation for numerous online
experiences, ranging from search results to personalized content delivery.
These systems have evolved into complex, multilayered architectures that
leverage vast datasets and often incorporate thousands of predictive models.
The maintenance and enhancement of these models is a labor intensive process
that requires extensive feature engineering. This approach not only exacerbates
technical debt but also hampers innovation in extending these systems to
emerging problem domains. In this report, we present our research to address
these challenges by utilizing a large foundation model with a textual interface
for ranking and recommendation tasks. We illustrate several key advantages of
our approach: (1) a single model can manage multiple predictive tasks involved
in ranking and recommendation, (2) decoder models with textual interface due to
their comprehension of reasoning capabilities, can generalize to new
recommendation surfaces and out-of-domain problems, and (3) by employing
natural language interfaces for task definitions and verbalizing member
behaviors and their social connections, we eliminate the need for feature
engineering and the maintenance of complex directed acyclic graphs of model
dependencies. We introduce our research pre-production model, 360Brew V1.0, a
150B parameter, decoder-only model that has been trained and fine-tuned on
LinkedIn's data and tasks. This model is capable of solving over 30 predictive
tasks across various segments of the LinkedIn platform, achieving performance
levels comparable to or exceeding those of current production systems based on
offline metrics, without task-specific fine-tuning. Notably, each of these
tasks is conventionally addressed by dedicated models that have been developed
and maintained over multiple years by teams of a similar or larger size than
our own.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RAPID: Retrieval-Augmented Parallel Inference Drafting for Text-Based
  Video Event Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16303v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16303v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Long Nguyen, Huy Nguyen, Bao Khuu, Huy Luu, Huy Le, Tuan Nguyen, Tho Quan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieving events from videos using text queries has become increasingly
challenging due to the rapid growth of multimedia content. Existing methods for
text-based video event retrieval often focus heavily on object-level
descriptions, overlooking the crucial role of contextual information. This
limitation is especially apparent when queries lack sufficient context, such as
missing location details or ambiguous background elements. To address these
challenges, we propose a novel system called RAPID (Retrieval-Augmented
Parallel Inference Drafting), which leverages advancements in Large Language
Models (LLMs) and prompt-based learning to semantically correct and enrich user
queries with relevant contextual information. These enriched queries are then
processed through parallel retrieval, followed by an evaluation step to select
the most relevant results based on their alignment with the original query.
Through extensive experiments on our custom-developed dataset, we demonstrate
that RAPID significantly outperforms traditional retrieval methods,
particularly for contextually incomplete queries. Our system was validated for
both speed and accuracy through participation in the Ho Chi Minh City AI
Challenge 2024, where it successfully retrieved events from over 300 hours of
video. Further evaluation comparing RAPID with the baseline proposed by the
competition organizers demonstrated its superior effectiveness, highlighting
the strength and robustness of our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review at SoICT'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ URAG: Implementing a Unified Hybrid RAG for Precise Answers in
  University Admission Chatbots -- A Case Study at HCMUT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16276v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16276v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Long Nguyen, Tho Quan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid advancement of Artificial Intelligence, particularly in
Natural Language Processing, Large Language Models (LLMs) have become pivotal
in educational question-answering systems, especially university admission
chatbots. Concepts such as Retrieval-Augmented Generation (RAG) and other
advanced techniques have been developed to enhance these systems by integrating
specific university data, enabling LLMs to provide informed responses on
admissions and academic counseling. However, these enhanced RAG techniques
often involve high operational costs and require the training of complex,
specialized modules, which poses challenges for practical deployment.
Additionally, in the educational context, it is crucial to provide accurate
answers to prevent misinformation, a task that LLM-based systems find
challenging without appropriate strategies and methods. In this paper, we
introduce the Unified RAG (URAG) Framework, a hybrid approach that
significantly improves the accuracy of responses, particularly for critical
queries. Experimental results demonstrate that URAG enhances our in-house,
lightweight model to perform comparably to state-of-the-art commercial models.
Moreover, to validate its practical applicability, we conducted a case study at
our educational institution, which received positive feedback and acclaim. This
study not only proves the effectiveness of URAG but also highlights its
feasibility for real-world implementation in educational settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review at SoICT'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Provence: efficient and robust context pruning for retrieval-augmented
  generation <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16214v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16214v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nadezhda Chirkova, Thibault Formal, Vassilina Nikoulina, Stéphane Clinchant
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation improves various aspects of large language
models (LLMs) generation, but suffers from computational overhead caused by
long contexts as well as the propagation of irrelevant retrieved information
into generated responses. Context pruning deals with both aspects, by removing
irrelevant parts of retrieved contexts before LLM generation. Existing context
pruning approaches are however limited, and do not provide a universal model
that would be both efficient and robust in a wide range of scenarios, e.g.,
when contexts contain a variable amount of relevant information or vary in
length, or when evaluated on various domains. In this work, we close this gap
and introduce Provence (Pruning and Reranking Of retrieVEd relevaNt ContExts),
an efficient and robust context pruner for Question Answering, which
dynamically detects the needed amount of pruning for a given context and can be
used out-of-the-box for various domains. The three key ingredients of Provence
are formulating the context pruning task as sequence labeling, unifying context
pruning capabilities with context reranking, and training on diverse data. Our
experimental results show that Provence enables context pruning with negligible
to no drop in performance, in various domains and settings, at almost no cost
in a standard RAG pipeline. We also conduct a deeper analysis alongside various
ablations to provide insights into training context pruners for future work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Separate This, and All of these Things Around It: Music Source
  Separation via Hyperellipsoidal Queries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16171v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16171v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karn N. Watcharasupat, Alexander Lerch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Music source separation is an audio-to-audio retrieval task of extracting one
or more constituent components, or composites thereof, from a musical audio
mixture. Each of these constituent components is often referred to as a "stem"
in literature. Historically, music source separation has been dominated by a
stem-based paradigm, leading to most state-of-the-art systems being either a
collection of single-stem extraction models, or a tightly coupled system with a
fixed, difficult-to-modify, set of supported stems. Combined with the limited
data availability, advances in music source separation have thus been mostly
limited to the "VDBO" set of stems: \textit{vocals}, \textit{drum},
\textit{bass}, and the catch-all \textit{others}. Recent work in music source
separation has begun to challenge the fixed-stem paradigm, moving towards
models able to extract any musical sound as long as this target type of sound
could be specified to the model as an additional query input. We generalize
this idea to a \textit{query-by-region} source separation system, specifying
the target based on the query regardless of how many sound sources or which
sound classes are contained within it. To do so, we propose the use of
hyperellipsoidal regions as queries to allow for an intuitive yet easily
parametrizable approach to specifying both the target (location) as well as its
spread. Evaluation of the proposed system on the MoisesDB dataset demonstrated
state-of-the-art performance of the proposed system both in terms of
signal-to-noise ratios and retrieval metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to the 2025 International Joint Conference on Artificial
  Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SampleLLM: Optimizing Tabular Data Synthesis in Recommendations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16125v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16125v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingtong Gao, Zhaocheng Du, Xiaopeng Li, Xiangyu Zhao, Yichao Wang, Xiangyang Li, Huifeng Guo, Ruiming Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tabular data synthesis is crucial in machine learning, yet existing general
methods-primarily based on statistical or deep learning models-are highly
data-dependent and often fall short in recommender systems. This limitation
arises from their difficulty in capturing complex distributions and
understanding feature relationships from sparse and limited data, along with
their inability to grasp semantic feature relations. Recently, Large Language
Models (LLMs) have shown potential in generating synthetic data samples through
few-shot learning and semantic understanding. However, they often suffer from
inconsistent distribution and lack of diversity due to their inherent
distribution disparity with the target dataset. To address these challenges and
enhance tabular data synthesis for recommendation tasks, we propose a novel
two-stage framework named SampleLLM to improve the quality of LLM-based tabular
data synthesis for recommendations by ensuring better distribution alignment.
In the first stage, SampleLLM employs LLMs with Chain-of-Thought prompts and
diverse exemplars to generate data that closely aligns with the target dataset
distribution, even when input samples are limited. The second stage uses an
advanced feature attribution-based importance sampling method to refine feature
relationships within the synthesized data, reducing any distribution biases
introduced by the LLM. Experimental results on three recommendation datasets,
two general datasets, and online deployment illustrate that SampleLLM
significantly surpasses existing methods for recommendation tasks and holds
promise for a broader range of tabular data scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Survey</span>: Understand the challenges of MachineLearning Experts using Named
  EntityRecognition Tools 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16112v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16112v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian Freund, Philippe Tamla, Matthias Hemmje
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a survey based on Kasunic's survey research methodology
to identify the criteria used by Machine Learning (ML) experts to evaluate
Named Entity Recognition (NER) tools and frameworks. Comparison and selection
of NER tools and frameworks is a critical step in leveraging NER for
Information Retrieval to support the development of Clinical Practice
Guidelines. In addition, this study examines the main challenges faced by ML
experts when choosing suitable NER tools and frameworks. Using Nunamaker's
methodology, the article begins with an introduction to the topic,
contextualizes the research, reviews the state-of-the-art in science and
technology, and identifies challenges for an expert survey on NER tools and
frameworks. This is followed by a description of the survey's design and
implementation. The paper concludes with an evaluation of the survey results
and the insights gained, ending with a summary and conclusions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 Pages, 13 Figures, 6th International Conference on Natural
  Language Processing, Information Retrieval and AI (NIAI 2025) January 25 ~
  26, 2025, Copenhagen, Denmark</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Options-Aware Dense Retrieval for Multiple-Choice query Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16111v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16111v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manish Singh, Manish Shrivastava
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long-context multiple-choice question answering tasks require robust
reasoning over extensive text sources. Since most of the pre-trained
transformer models are restricted to processing only a few hundred words at a
time, successful completion of such tasks often relies on the identification of
evidence spans, such as sentences, that provide supporting evidence for
selecting the correct answer. Prior research in this domain has predominantly
utilized pre-trained dense retrieval models, given the absence of supervision
to fine-tune the retrieval process. This paper proposes a novel method called
Options Aware Dense Retrieval (OADR) to address these challenges. ORDA uses an
innovative approach to fine-tuning retrieval by leveraging query-options
embeddings, which aim to mimic the embeddings of the oracle query (i.e., the
query paired with the correct answer) for enhanced identification of supporting
evidence. Through experiments conducted on the QuALITY benchmark dataset, we
demonstrate that our proposed model surpasses existing baselines in terms of
performance and accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PISCO: Pretty Simple Compression for Retrieval-Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16075v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16075v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maxime Louis, Hervé Déjean, Stéphane Clinchant
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) pipelines enhance Large Language Models
(LLMs) by retrieving relevant documents, but they face scalability issues due
to high inference costs and limited context size. Document compression is a
practical solution, but current soft compression methods suffer from accuracy
losses and require extensive pretraining. In this paper, we introduce PISCO, a
novel method that achieves a 16x compression rate with minimal accuracy loss
(0-3%) across diverse RAG-based question-answering (QA) tasks. Unlike existing
approaches, PISCO requires no pretraining or annotated data, relying solely on
sequence-level knowledge distillation from document-based questions. With the
ability to fine-tune a 7-10B LLM in 48 hours on a single A100 GPU, PISCO offers
a highly efficient and scalable solution. We present comprehensive experiments
showing that PISCO outperforms existing compression models by 8% in accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding Long Videos via LLM-Powered Entity Relation Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15953v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15953v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meng Chu, Yicong Li, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The analysis of extended video content poses unique challenges in artificial
intelligence, particularly when dealing with the complexity of tracking and
understanding visual elements across time. Current methodologies that process
video frames sequentially struggle to maintain coherent tracking of objects,
especially when these objects temporarily vanish and later reappear in the
footage. A critical limitation of these approaches is their inability to
effectively identify crucial moments in the video, largely due to their limited
grasp of temporal relationships. To overcome these obstacles, we present
GraphVideoAgent, a cutting-edge system that leverages the power of graph-based
object tracking in conjunction with large language model capabilities. At its
core, our framework employs a dynamic graph structure that maps and monitors
the evolving relationships between visual entities throughout the video
sequence. This innovative approach enables more nuanced understanding of how
objects interact and transform over time, facilitating improved frame selection
through comprehensive contextual awareness. Our approach demonstrates
remarkable effectiveness when tested against industry benchmarks. In
evaluations on the EgoSchema dataset, GraphVideoAgent achieved a 2.2
improvement over existing methods while requiring analysis of only 8.2 frames
on average. Similarly, testing on the NExT-QA benchmark yielded a 2.0
performance increase with an average frame requirement of 8.1. These results
underscore the efficiency of our graph-guided methodology in enhancing both
accuracy and computational performance in long-form video understanding tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parametric Retrieval Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15915v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15915v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weihang Su, Yichen Tang, Qingyao Ai, Junxi Yan, Changyue Wang, Hongning Wang, Ziyi Ye, Yujia Zhou, Yiqun Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) techniques have emerged as a promising
solution to enhance the reliability of large language models (LLMs) by
addressing issues like hallucinations, outdated knowledge, and domain
adaptation. In particular, existing RAG methods append relevant documents
retrieved from external corpus or databases to the input of LLMs to guide their
generation process, which we refer to as the in-context knowledge injection
method. While this approach is simple and often effective, it has inherent
limitations. Firstly, increasing the context length and number of relevant
documents can lead to higher computational overhead and degraded performance,
especially in complex reasoning tasks. More importantly, in-context knowledge
injection operates primarily at the input level, but LLMs store their internal
knowledge in their parameters. This gap fundamentally limits the capacity of
in-context methods. To this end, we introduce Parametric retrieval-augmented
generation (Parametric RAG), a new RAG paradigm that integrates external
knowledge directly into the parameters of feed-forward networks (FFN) of an LLM
through document parameterization. This approach not only saves online
computational costs by eliminating the need to inject multiple documents into
the LLMs' input context, but also deepens the integration of external knowledge
into the parametric knowledge space of the LLM. Experimental results
demonstrate that Parametric RAG substantially enhances both the effectiveness
and efficiency of knowledge augmentation in LLMs. Also, it can be combined with
in-context RAG methods to achieve even better performance.
  We have open-sourced all the code, data, and models in the following
anonymized GitHub link: https://github.com/oneal2000/PRAG
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aspect-Aware Decomposition for Opinion Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17191v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17191v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miao Li, Jey Han Lau, Eduard Hovy, Mirella Lapata
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Opinion summarization plays a key role in deriving meaningful insights from
large-scale online reviews. To make this process more explainable and grounded,
we propose a modular approach guided by review aspects which separates the
tasks of aspect identification, opinion consolidation, and meta-review
synthesis, enabling greater transparency and ease of inspection. We conduct
extensive experiments across datasets representing scientific research,
business, and product domains. Results show that our method generates more
grounded summaries compared to strong baseline models, as verified through
automated and human evaluations. Additionally, our modular approach, which
incorporates reasoning based on review aspects, produces more informative
intermediate outputs than knowledge-agnostic decomposed prompting. These
intermediate outputs can also effectively support humans in summarizing
opinions from large volumes of reviews.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Long-Term Interest Clock: Fine-Grained Time Perception in Streaming
  Recommendation System <span class="chip">WWW2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15817v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15817v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongchun Zhu, Guanyu Jiang, Jingwu Chen, Feng Zhang, Xiao Yang, Zuotao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  User interests manifest a dynamic pattern within the course of a day, e.g., a
user usually favors soft music at 8 a.m. but may turn to ambient music at 10
p.m. To model dynamic interests in a day, hour embedding is widely used in
traditional daily-trained industrial recommendation systems. However, its
discreteness can cause periodical online patterns and instability in recent
streaming recommendation systems. Recently, Interest Clock has achieved
remarkable performance in streaming recommendation systems. Nevertheless, it
models users' dynamic interests in a coarse-grained manner, merely encoding
users' discrete interests of 24 hours from short-term behaviors. In this paper,
we propose a fine-grained method for perceiving time information for streaming
recommendation systems, named Long-term Interest Clock (LIC). The key idea of
LIC is adaptively calculating current user interests by taking into
consideration the relevance of long-term behaviors around current time (e.g., 8
a.m.) given a candidate item. LIC consists of two modules: (1) Clock-GSU
retrieves a sub-sequence by searching through long-term behaviors, using query
information from a candidate item and current time, (2) Clock-ESU employs a
time-gap-aware attention mechanism to aggregate sub-sequence with the candidate
item. With Clock-GSU and Clock-ESU, LIC is capable of capturing users' dynamic
fine-grained interests from long-term behaviors. We conduct online A/B tests,
obtaining +0.122% improvements on user active days. Besides, the extended
offline experiments show improvements as well. Long-term Interest Clock has
been integrated into Douyin Music App's recommendation system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by WWW2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AdaF^2M^2: Comprehensive Learning and Responsive Leveraging Features in
  Recommendation System <span class="chip">DASFAA2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15816v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15816v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongchun Zhu, Jingwu Chen, Ling Chen, Yitan Li, Feng Zhang, Xiao Yang, Zuotao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Feature modeling, which involves feature representation learning and
leveraging, plays an essential role in industrial recommendation systems.
However, the data distribution in real-world applications usually follows a
highly skewed long-tail pattern due to the popularity bias, which easily leads
to over-reliance on ID-based features, such as user/item IDs and ID sequences
of interactions. Such over-reliance makes it hard for models to learn features
comprehensively, especially for those non-ID meta features, e.g., user/item
characteristics. Further, it limits the feature leveraging ability in models,
getting less generalized and more susceptible to data noise. Previous studies
on feature modeling focus on feature extraction and interaction, hardly
noticing the problems brought about by the long-tail data distribution. To
achieve better feature representation learning and leveraging on real-world
data, we propose a model-agnostic framework AdaF^2M^2, short for Adaptive
Feature Modeling with Feature Mask. The feature-mask mechanism helps
comprehensive feature learning via multi-forward training with augmented
samples, while the adapter applies adaptive weights on features responsive to
different user/item states. By arming base models with AdaF^2M^2, we conduct
online A/B tests on multiple recommendation scenarios, obtaining +1.37% and
+1.89% cumulative improvements on user active days and app duration
respectively. Besides, the extended offline experiments on different models
show improvements as well. AdaF$^2$M$^2$ has been widely deployed on both
retrieval and ranking tasks in multiple applications of Douyin Group,
indicating its superior effectiveness and universality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by DASFAA2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on Knowledge Organization Systems of Research Fields: Resources
  and Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.04432v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.04432v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Angelo Salatino, Tanay Aggarwal, Andrea Mannocci, Francesco Osborne, Enrico Motta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge Organization Systems (KOSs), such as term lists, thesauri,
taxonomies, and ontologies, play a fundamental role in categorising, managing,
and retrieving information. In the academic domain, KOSs are often adopted for
representing research areas and their relationships, primarily aiming to
classify research articles, academic courses, patents, books, scientific
venues, domain experts, grants, software, experiment materials, and several
other relevant products and agents. These structured representations of
research areas, widely embraced by many academic fields, have proven effective
in empowering AI-based systems to i) enhance retrievability of relevant
documents, ii) enable advanced analytic solutions to quantify the impact of
academic research, and iii) analyse and forecast research dynamics. This paper
aims to present a comprehensive survey of the current KOS for academic
disciplines. We analysed and compared 45 KOSs according to five main
dimensions: scope, structure, curation, usage, and links to other KOSs. Our
results reveal a very heterogeneous scenario in terms of scope, scale, quality,
and usage, highlighting the need for more integrated solutions for representing
research knowledge across academic fields. We conclude by discussing the main
challenges and the most promising future directions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SLMRec: Distilling Large Language Models into Small for Sequential
  Recommendation <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17890v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17890v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wujiang Xu, Qitian Wu, Zujie Liang, Jiaojiao Han, Xuying Ning, Yunxiao Shi, Wenfang Lin, Yongfeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential Recommendation (SR) task involves predicting the next item a user
is likely to interact with, given their past interactions. The SR models
examine the sequence of a user's actions to discern more complex behavioral
patterns and temporal dynamics. Recent research demonstrates the great impact
of LLMs on sequential recommendation systems, either viewing sequential
recommendation as language modeling or serving as the backbone for user
representation. Although these methods deliver outstanding performance, there
is scant evidence of the necessity of a large language model and how large the
language model is needed, especially in the sequential recommendation scene.
Meanwhile, due to the huge size of LLMs, it is inefficient and impractical to
apply a LLM-based model in real-world platforms that often need to process
billions of traffic logs daily. In this paper, we explore the influence of
LLMs' depth by conducting extensive experiments on large-scale industry
datasets. Surprisingly, our motivational experiments reveal that most
intermediate layers of LLMs are redundant, indicating that pruning the
remaining layers can still maintain strong performance. Motivated by this
insight, we empower small language models for SR, namely SLMRec, which adopt a
simple yet effective knowledge distillation method. Moreover, SLMRec is
orthogonal to other post-training efficiency techniques, such as quantization
and pruning, so that they can be leveraged in combination. Comprehensive
experimental results illustrate that the proposed SLMRec model attains the best
performance using only 13% of the parameters found in LLM-based recommendation
models while simultaneously achieving up to 6.6x and 8.0x speedups in training
and inference time costs, respectively. Besides, we provide a theoretical
justification for why small language models can perform comparably to large
language models in SR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>International Conference on Learning Representations (ICLR 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Recommenadation aided Caching using Combinatorial Multi-armed Bandits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.00080v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.00080v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pavamana K J, Chandramani Kishore Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study content caching with recommendations in a wireless network where the
users are connected through a base station equipped with a finite-capacity
cache. We assume a fixed set of contents with unknown user preferences and
content popularities. The base station can cache a subset of the contents and
can also recommend subsets of the contents to different users in order to
encourage them to request the recommended contents. Recommendations, depending
on their acceptability, can thus be used to increase cache hits. We first
assume that the users' recommendation acceptabilities are known and formulate
the cache hit optimization problem as a combinatorial multi-armed bandit
(CMAB). We propose a UCB-based algorithm to decide which contents to cache and
recommend and provide an upper bound on the regret of this algorithm.
Subsequently, we consider a more general scenario where the users'
recommendation acceptabilities are also unknown and propose another UCB-based
algorithm that learns these as well. We numerically demonstrate the performance
of our algorithms and compare these to state-of-the-art algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Model Distilling Medication Recommendation Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.02803v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.02803v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qidong Liu, Xian Wu, Xiangyu Zhao, Yuanshao Zhu, Zijian Zhang, Feng Tian, Yefeng Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recommendation of medication is a vital aspect of intelligent healthcare
systems, as it involves prescribing the most suitable drugs based on a
patient's specific health needs. Unfortunately, many sophisticated models
currently in use tend to overlook the nuanced semantics of medical data, while
only relying heavily on identities. Furthermore, these models face significant
challenges in handling cases involving patients who are visiting the hospital
for the first time, as they lack prior prescription histories to draw upon. To
tackle these issues, we harness the powerful semantic comprehension and
input-agnostic characteristics of Large Language Models (LLMs). Our research
aims to transform existing medication recommendation methodologies using LLMs.
In this paper, we introduce a novel approach called Large Language Model
Distilling Medication Recommendation (LEADER). We begin by creating appropriate
prompt templates that enable LLMs to suggest medications effectively. However,
the straightforward integration of LLMs into recommender systems leads to an
out-of-corpus issue specific to drugs. We handle it by adapting the LLMs with a
novel output layer and a refined tuning loss function. Although LLM-based
models exhibit remarkable capabilities, they are plagued by high computational
costs during inference, which is impractical for the healthcare sector. To
mitigate this, we have developed a feature-level knowledge distillation
technique, which transfers the LLM's proficiency to a more compact model.
Extensive experiments conducted on two real-world datasets, MIMIC-III and
MIMIC-IV, demonstrate that our proposed model not only delivers effective
results but also is efficient. To ease the reproducibility of our experiments,
we release the implementation code online.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Movement- and Traffic-based User Identification in Commercial Virtual
  Reality Applications: Threats and Opportunities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16326v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16326v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sara Baldoni, Salim Benhamadi, Federico Chiariotti, Michele Zorzi, Federica Battisti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the unprecedented diffusion of virtual reality, the number of
application scenarios is continuously growing. As commercial and gaming
applications become pervasive, the need for the secure and convenient
identification of users, often overlooked by the research in immersive media,
is becoming more and more pressing. Networked scenarios such as Cloud gaming or
cooperative virtual training and teleoperation require both a user-friendly and
streamlined experience and user privacy and security. In this work, we
investigate the possibility of identifying users from their movement patterns
and data traffic traces while playing four commercial games, using a publicly
available dataset. If, on the one hand, this paves the way for easy
identification and automatic customization of the virtual reality content, it
also represents a serious threat to users' privacy due to network
analysis-based fingerprinting. Based on this, we analyze the threats and
opportunities for virtual reality users' security and privacy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at IEEE VR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MetaDecorator: Generating Immersive Virtual Tours through Multimodality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16164v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16164v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuang Xie, Yang Liu, Jeannie S. A. Lee, Haiwei Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  MetaDecorator, is a framework that empowers users to personalize virtual
spaces. By leveraging text-driven prompts and image synthesis techniques,
MetaDecorator adorns static panoramas captured by 360{\deg} imaging devices,
transforming them into uniquely styled and visually appealing environments.
This significantly enhances the realism and engagement of virtual tours
compared to traditional offerings. Beyond the core framework, we also discuss
the integration of Large Language Models (LLMs) and haptics in the VR
application to provide a more immersive experience.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Parallelism in Music and Language: A Perspective from Symbol
  Emergence Systems based on Probabilistic Generative Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15721v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15721v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tadahiro Taniguchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Music and language are structurally similar. Such structural similarity is
often explained by generative processes. This paper describes the recent
development of probabilistic generative models (PGMs) for language learning and
symbol emergence in robotics. Symbol emergence in robotics aims to develop a
robot that can adapt to real-world environments and human linguistic
communications and acquire language from sensorimotor information alone (i.e.,
in an unsupervised manner). This is regarded as a constructive approach to
symbol emergence systems. To this end, a series of PGMs have been developed,
including those for simultaneous phoneme and word discovery, lexical
acquisition, object and spatial concept formation, and the emergence of a
symbol system. By extending the models, a symbol emergence system comprising a
multi-agent system in which a symbol system emerges is revealed to be modeled
using PGMs. In this model, symbol emergence can be regarded as collective
predictive coding. This paper expands on this idea by combining the theory that
''emotion is based on the predictive coding of interoceptive signals'' and
''symbol emergence systems,'' and describes the possible hypothesis of the
emergence of meaning in music.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Implicit Location-Caption Alignment via Complementary Masking for
  Weakly-Supervised Dense Video Captioning <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.12791v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.12791v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiping Ge, Qiang Chen, Zhiwei Jiang, Yafeng Yin, Liu Qin, Ziyao Chen, Qing Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Weakly-Supervised Dense Video Captioning (WSDVC) aims to localize and
describe all events of interest in a video without requiring annotations of
event boundaries. This setting poses a great challenge in accurately locating
the temporal location of event, as the relevant supervision is unavailable.
Existing methods rely on explicit alignment constraints between event locations
and captions, which involve complex event proposal procedures during both
training and inference. To tackle this problem, we propose a novel implicit
location-caption alignment paradigm by complementary masking, which simplifies
the complex event proposal and localization process while maintaining
effectiveness. Specifically, our model comprises two components: a dual-mode
video captioning module and a mask generation module. The dual-mode video
captioning module captures global event information and generates descriptive
captions, while the mask generation module generates differentiable positive
and negative masks for localizing the events. These masks enable the implicit
alignment of event locations and captions by ensuring that captions generated
from positively and negatively masked videos are complementary, thereby forming
a complete video description. In this way, even under weak supervision, the
event location and event caption can be aligned implicitly. Extensive
experiments on the public datasets demonstrate that our method outperforms
existing weakly-supervised methods and achieves competitive results compared to
fully-supervised methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-26T00:00:00Z">2025-01-26</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">8</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SCP-116K: A High-Quality Problem-Solution <span class="highlight-title">Dataset</span> and a Generalized
  Pipeline for Automated Extraction in the Higher Education Science Domain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15587v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15587v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dakuan Lu, Xiaoyu Tan, Rui Xu, Tianchu Yao, Chao Qu, Wei Chu, Yinghui Xu, Yuan Qi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent breakthroughs in large language models (LLMs) exemplified by the
impressive mathematical and scientific reasoning capabilities of the o1 model
have spotlighted the critical importance of high-quality training data in
advancing LLM performance across STEM disciplines. While the mathematics
community has benefited from a growing body of curated datasets, the scientific
domain at the higher education level has long suffered from a scarcity of
comparable resources. To address this gap, we present SCP-116K, a new
large-scale dataset of 116,756 high-quality problem-solution pairs,
automatically extracted from heterogeneous sources using a streamlined and
highly generalizable pipeline. Our approach involves stringent filtering to
ensure the scientific rigor and educational level of the extracted materials,
while maintaining adaptability for future expansions or domain transfers. By
openly releasing both the dataset and the extraction pipeline, we seek to
foster research on scientific reasoning, enable comprehensive performance
evaluations of new LLMs, and lower the barrier to replicating the successes of
advanced models like o1 in the broader science community. We believe SCP-116K
will serve as a critical resource, catalyzing progress in high-level scientific
reasoning tasks and promoting further innovations in LLM development. The
dataset and code are publicly available at
https://github.com/AQA6666/SCP-116K-open.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 1 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unveiling the Potential of Multimodal Retrieval Augmented Generation
  with Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15470v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15470v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaohan Yu, Zhihan Yang, Chong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Retrieval Augmented Generation (MRAG) systems, while promising for
enhancing Multimodal Large Language Models (MLLMs), often rely on rigid,
single-step retrieval methods. This limitation hinders their ability to
effectively address real-world scenarios that demand adaptive information
acquisition and query refinement. To overcome this, we introduce the novel task
of Multimodal Retrieval Augmented Generation Planning (MRAG Planning), focusing
on optimizing MLLM performance while minimizing computational overhead. We
present CogPlanner, a versatile framework inspired by human cognitive
processes. CogPlanner iteratively refines queries and selects retrieval
strategies, enabling both parallel and sequential modeling approaches. To
rigorously evaluate MRAG Planning, we introduce CogBench, a new benchmark
specifically designed for this task. CogBench facilitates the integration of
lightweight CogPlanner with resource-efficient MLLMs. Our experimental findings
demonstrate that CogPlanner surpasses existing MRAG baselines, achieving
significant improvements in both accuracy and efficiency with minimal
computational overhead.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Aspect Performance-aware Hypergraph Neural Network for <span class="highlight-title">Review</span>-based
  Recommendation <span class="chip">WSDM'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15429v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15429v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junrui Liu, Tong Li, Di Wu, Zifang Tang, Yuan Fang, Zhen Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online reviews allow consumers to provide detailed feedback on various
aspects of items. Existing methods utilize these aspects to model users'
fine-grained preferences for specific item features through graph neural
networks. We argue that the performance of items on different aspects is
important for making precise recommendations, which has not been taken into
account by existing approaches, due to lack of data. In this paper, we propose
an aspect performance-aware hypergraph neural network (APH) for the
review-based recommendation, which learns the performance of items from the
conflicting sentiment polarity of user reviews. Specifically, APH
comprehensively models the relationships among users, items, aspects, and
sentiment polarity by systematically constructing an aspect hypergraph based on
user reviews. In addition, APH aggregates aspects representing users and items
by employing an aspect performance-aware hypergraph aggregation method. It
aggregates the sentiment polarities from multiple users by jointly considering
user preferences and the semantics of their sentiments, determining the weights
of sentiment polarities to infer the performance of items on various aspects.
Such performances are then used as weights to aggregate neighboring aspects.
Experiments on six real-world datasets demonstrate that APH improves MSE,
Precision@5, and Recall@5 by an average of 2.30%, 4.89%, and 1.60% over the
best baseline. The source code and data are available at
https://github.com/dianziliu/APH.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, accepted by WSDM'25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Empirically-parametrized Spatio-Temporal Extended-SIR Model for
  Combined Dilution and Vaccination Mitigation for Rabies Outbreaks in Wild
  Jackals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Teddy Lazebnik, Yehuda Samuel, Jonathan Tichon, Roi Lapid, Roni King, Tomer Nissimian, Orr Spiegel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The transmission of zoonotic diseases between animals and humans poses an
increasing threat. Rabies is a prominent example with various instances
globally, facilitated by a surplus of meso-predators (commonly, facultative
synanthropic species e.g., golden jackals [Canis aureus, hereafter jackals])
thanks to the abundance of anthropogenic resources leading to dense populations
close to human establishments. To mitigate rabies outbreaks and prevent human
infections, authorities target the jackal which is the main rabies vector in
many regions, through the dissemination of oral vaccines in known jackals'
activity centers, as well as opportunistic culling to reduce population
density. Because dilution (i.e., culling) is not selective towards sick or
un-vaccinated individuals, these two complementary epizootic intervention
policies (EIPs) can interfere with each other. Nonetheless, there is only
limited examination of the interactive effectiveness of these EIPs and their
potential influence on rabies epizootic spread dynamics, highlighting the need
to understand these measures and the spread of rabies in wild jackals. In this
study, we introduce a novel spatio-temporal extended-SIR
(susceptible-infected-recovered) model with a graph-based spatial framework for
evaluating mitigation efficiency. We implement the model in a case study using
a jackal population in northern Israel, and using spatial and movement data
collected by Advanced Tracking and Localization of Animals in real-life Systems
(ATLAS) telemetry. An agent-based simulation approach allows us to explore
various biologically-realistic scenarios, and assess the impact of different
EIPs configurations. Our model suggests that under biologically-realistic
underlying assumptions and scenarios, the effectiveness of both EIPs is not
influenced much by the jackal population size but is sensitive to their
dispersal between activity centers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zero-Shot Interactive Text-to-Image Retrieval via Diffusion-Augmented
  Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15379v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15379v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijun Long, Kangheng Liang, Gerardo Aragon-Camarasa, Richard Mccreadie, Paul Henderson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interactive Text-to-Image Retrieval (I-TIR) has emerged as a transformative
user-interactive tool for applications in domains such as e-commerce and
education. Yet, current methodologies predominantly depend on finetuned
Multimodal Large Language Models (MLLMs), which face two critical limitations:
(1) Finetuning imposes prohibitive computational overhead and long-term
maintenance costs. (2) Finetuning narrows the pretrained knowledge distribution
of MLLMs, reducing their adaptability to novel scenarios. These issues are
exacerbated by the inherently dynamic nature of real-world I-TIR systems, where
queries and image databases evolve in complexity and diversity, often deviating
from static training distributions. To overcome these constraints, we propose
Diffusion Augmented Retrieval (DAR), a paradigm-shifting framework that
bypasses MLLM finetuning entirely. DAR synergizes Large Language Model
(LLM)-guided query refinement with Diffusion Model (DM)-based visual synthesis
to create contextually enriched intermediate representations. This
dual-modality approach deciphers nuanced user intent more holistically,
enabling precise alignment between textual queries and visually relevant
images. Rigorous evaluations across four benchmarks reveal DAR's dual
strengths: (1) Matches state-of-the-art finetuned I-TIR models on
straightforward queries without task-specific training. (2) Scalable
Generalization: Surpasses finetuned baselines by 7.61% in Hits@10 (top-10
accuracy) under multi-turn conversational complexity, demonstrating robustness
to intricate, distributionally shifted interactions. By eliminating finetuning
dependencies and leveraging generative-augmented representations, DAR
establishes a new trajectory for efficient, adaptive, and scalable cross-modal
retrieval systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How to Mitigate Information Loss in Knowledge Graphs for GraphRAG:
  Leveraging Triple Context Restoration and Query-Driven Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15378v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15378v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manzong Huang, Chenyang Bu, Yi He, Xindong Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge Graph (KG)-augmented Large Language Models (LLMs) have recently
propelled significant advances in complex reasoning tasks, thanks to their
broad domain knowledge and contextual awareness. Unfortunately, current methods
often assume KGs to be complete, which is impractical given the inherent
limitations of KG construction and the potential loss of contextual cues when
converting unstructured text into entity-relation triples. In response, this
paper proposes the Triple Context Restoration and Query-driven Feedback
(TCR-QF) framework, which reconstructs the textual context underlying each
triple to mitigate information loss, while dynamically refining the KG
structure by iteratively incorporating query-relevant missing knowledge.
Experiments on five benchmark question-answering datasets substantiate the
effectiveness of TCR-QF in KG and LLM integration, where itachieves a 29.1%
improvement in Exact Match and a 15.5% improvement in F1 over its
state-of-the-art GraphRAG competitors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph Representation Learning via Causal Diffusion for
  Out-of-Distribution Recommendation <span class="chip">WWW2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.00490v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.00490v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chu Zhao, Enneng Yang, Yuliang Liang, Pengxiang Lan, Yuting Liu, Jianzhe Zhao, Guibing Guo, Xingwei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs)-based recommendation algorithms typically assume
that training and testing data are drawn from independent and identically
distributed (IID) spaces. However, this assumption often fails in the presence
of out-of-distribution (OOD) data, resulting in significant performance
degradation. In this study, we construct a Structural Causal Model (SCM) to
analyze interaction data, revealing that environmental confounders (e.g., the
COVID-19 pandemic) lead to unstable correlations in GNN-based models, thus
impairing their generalization to OOD data. To address this issue, we propose a
novel approach, graph representation learning via causal diffusion
(CausalDiffRec) for OOD recommendation. This method enhances the model's
generalization on OOD data by eliminating environmental confounding factors and
learning invariant graph representations. Specifically, we use backdoor
adjustment and variational inference to infer the real environmental
distribution, thereby eliminating the impact of environmental confounders. This
inferred distribution is then used as prior knowledge to guide the
representation learning in the reverse phase of the diffusion process to learn
the invariant representation. In addition, we provide a theoretical derivation
that proves optimizing the objective function of CausalDiffRec can encourage
the model to learn environment-invariant graph representations, thereby
achieving excellent generalization performance in recommendations under
distribution shifts. Our extensive experiments validate the effectiveness of
CausalDiffRec in improving the generalization of OOD data, and the average
improvement is up to 10.69% on Food, 18.83% on KuaiRec, 22.41% on Yelp2018, and
11.65% on Douban datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, accepted by WWW2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Agent-OM: Leveraging LLM Agents for Ontology Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.00326v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.00326v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhangcheng Qiang, Weiqing Wang, Kerry Taylor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ontology matching (OM) enables semantic interoperability between different
ontologies and resolves their conceptual heterogeneity by aligning related
entities. OM systems currently have two prevailing design paradigms:
conventional knowledge-based expert systems and newer machine learning-based
predictive systems. While large language models (LLMs) and LLM agents have
revolutionised data engineering and have been applied creatively in many
domains, their potential for OM remains underexplored. This study introduces a
novel agent-powered LLM-based design paradigm for OM systems. With
consideration of several specific challenges in leveraging LLM agents for OM,
we propose a generic framework, namely Agent-OM (Agent for Ontology Matching),
consisting of two Siamese agents for retrieval and matching, with a set of OM
tools. Our framework is implemented in a proof-of-concept system. Evaluations
of three Ontology Alignment Evaluation Initiative (OAEI) tracks over
state-of-the-art OM systems show that our system can achieve results very close
to the long-standing best performance on simple OM tasks and can significantly
improve the performance on complex and few-shot OM tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 12 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Complex Heterogeneous Multimodal Fake News via Social Latent
  Network Inference <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15508v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15508v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingxin Li, Yuchen Zhang, Haowei Xu, Xianghua Li, Chao Gao, Zhen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the diversification of online social platforms, news dissemination has
become increasingly complex, heterogeneous, and multimodal, making the fake
news detection task more challenging and crucial. Previous works mainly focus
on obtaining social relationships of news via retweets, limiting the accurate
detection when real cascades are inaccessible. Given the proven assessment of
the spreading influence of events, this paper proposes a method called HML
(Complex Heterogeneous Multimodal Fake News Detection method via Latent Network
Inference). Specifically, an improved social latent network inference strategy
is designed to estimate the maximum likelihood of news influences under the
same event. Meanwhile, a novel heterogeneous graph is built based on social
attributes for multimodal news under different events. Further, to better
aggregate the relationships among heterogeneous multimodal features, this paper
proposes a self-supervised-based multimodal content learning strategy, to
enhance, align, fuse and compare heterogeneous modal contents. Based above, a
personalized heterogeneous graph representation learning is designed to
classify fake news. Extensive experiments demonstrate that the proposed method
outperforms the SOTA in real social media news datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cross-Modal Transfer from Memes to Videos: Addressing Data Scarcity in
  Hateful Video Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15438v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15438v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Wang, Rui Yang Tan, Roy Ka-Wei Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting hate speech in online content is essential to ensuring safer
digital spaces. While significant progress has been made in text and meme
modalities, video-based hate speech detection remains under-explored, hindered
by a lack of annotated datasets and the high cost of video annotation. This gap
is particularly problematic given the growing reliance on large models, which
demand substantial amounts of training data. To address this challenge, we
leverage meme datasets as both a substitution and an augmentation strategy for
training hateful video detection models. Our approach introduces a
human-assisted reannotation pipeline to align meme dataset labels with video
datasets, ensuring consistency with minimal labeling effort. Using two
state-of-the-art vision-language models, we demonstrate that meme data can
substitute for video data in resource-scarce scenarios and augment video
datasets to achieve further performance gains. Our results consistently
outperform state-of-the-art benchmarks, showcasing the potential of cross-modal
transfer learning for advancing hateful video detection. Dataset and code are
available at https://github.com/Social-AI-Studio/CrossModalTransferLearning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures, THE WEB CONFERENCE 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-25T00:00:00Z">2025-01-25</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">14</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Retrieval-Augmented Generation through Multi-Agent
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15228v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15228v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiqun Chen, Lingyong Yan, Weiwei Sun, Xinyu Ma, Yi Zhang, Shuaiqiang Wang, Dawei Yin, Yiming Yang, Jiaxin Mao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) is extensively utilized to incorporate
external, current knowledge into large language models, thereby minimizing
hallucinations. A standard RAG pipeline may comprise several components, such
as query rewriting, document retrieval, document filtering, and answer
generation. However, these components are typically optimized separately
through supervised fine-tuning, which can lead to misalignments between the
objectives of individual modules and the overarching aim of generating accurate
answers in question-answering (QA) tasks. Although recent efforts have explored
reinforcement learning (RL) to optimize specific RAG components, these
approaches often focus on overly simplistic pipelines with only two components
or do not adequately address the complex interdependencies and collaborative
interactions among the modules. To overcome these challenges, we propose
treating the RAG pipeline as a multi-agent cooperative task, with each
component regarded as an RL agent. Specifically, we present MMOA-RAG, a
Multi-Module joint Optimization Algorithm for RAG, which employs multi-agent
reinforcement learning to harmonize all agents' goals towards a unified reward,
such as the F1 score of the final answer. Experiments conducted on various QA
datasets demonstrate that MMOA-RAG improves the overall pipeline performance
and outperforms existing baselines. Furthermore, comprehensive ablation studies
validate the contributions of individual components and the adaptability of
MMOA-RAG across different RAG components and datasets. The code of MMOA-RAG is
on https://github.com/chenyiqun/MMOA-RAG.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Technology Mapping with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15120v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15120v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minh Hieu Nguyen, Hien Thu Pham, Hiep Minh Ha, Ngoc Quang Hung Le, Jun Jo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In today's fast-evolving business landscape, having insight into the
technology stacks that organizations use is crucial for forging partnerships,
uncovering market openings, and informing strategic choices. However,
conventional technology mapping, which typically hinges on keyword searches,
struggles with the sheer scale and variety of data available, often failing to
capture nascent technologies. To overcome these hurdles, we present STARS
(Semantic Technology and Retrieval System), a novel framework that harnesses
Large Language Models (LLMs) and Sentence-BERT to pinpoint relevant
technologies within unstructured content, build comprehensive company profiles,
and rank each firm's technologies according to their operational importance. By
integrating entity extraction with Chain-of-Thought prompting and employing
semantic ranking, STARS provides a precise method for mapping corporate
technology portfolios. Experimental results show that STARS markedly boosts
retrieval accuracy, offering a versatile and high-performance solution for
cross-industry technology mapping.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ABXI: Invariant Interest Adaptation for Task-Guided Cross-Domain
  Sequential Recommendation <span class="chip">WWW '25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15118v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15118v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingtian Bian, Marcus Vinícius de Carvalho, Tieying Li, Jiaxing Xu, Hui Fang, Yiping Ke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-Domain Sequential Recommendation (CDSR) has recently gained attention
for countering data sparsity by transferring knowledge across domains. A common
approach merges domain-specific sequences into cross-domain sequences, serving
as bridges to connect domains. One key challenge is to correctly extract the
shared knowledge among these sequences and appropriately transfer it. Most
existing works directly transfer unfiltered cross-domain knowledge rather than
extracting domain-invariant components and adaptively integrating them into
domain-specific modelings. Another challenge lies in aligning the
domain-specific and cross-domain sequences. Existing methods align these
sequences based on timestamps, but this approach can cause prediction
mismatches when the current tokens and their targets belong to different
domains. In such cases, the domain-specific knowledge carried by the current
tokens may degrade performance. To address these challenges, we propose the
A-B-Cross-to-Invariant Learning Recommender (ABXI). Specifically, leveraging
LoRA's effectiveness for efficient adaptation, ABXI incorporates two types of
LoRAs to facilitate knowledge adaptation. First, all sequences are processed
through a shared encoder that employs a domain LoRA for each sequence, thereby
preserving unique domain characteristics. Next, we introduce an invariant
projector that extracts domain-invariant interests from cross-domain
representations, utilizing an invariant LoRA to adapt these interests into
modeling each specific domain. Besides, to avoid prediction mismatches, all
domain-specific sequences are aligned to match the domains of the cross-domain
ground truths. Experimental results on three datasets demonstrate that our
approach outperforms other CDSR counterparts by a large margin. The codes are
available in \url{https://github.com/DiMarzioBian/ABXI}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by WebConf '25 (WWW '25)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PatchRec: Multi-Grained Patching for Efficient LLM-based Sequential
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15087v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15087v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayi Liao, Ruobing Xie, Sihang Li, Xiang Wang, Xingwu Sun, Zhanhui Kang, Xiangnan He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models for sequential recommendation (LLM4SR), which transform
user-item interactions into language modeling, have shown promising results.
However, due to the limitations of context window size and the computational
costs associated with Large Language Models (LLMs), current approaches
primarily truncate user history by only considering the textual information of
items from the most recent interactions in the input prompt. This truncation
fails to fully capture the long-term behavioral patterns of users. To address
this, we propose a multi-grained patching framework -- PatchRec. It compresses
the textual tokens of an item title into a compact item patch, and further
compresses multiple item patches into a denser session patch, with earlier
interactions being compressed to a greater degree. The framework consists of
two stages: (1) Patch Pre-training, which familiarizes LLMs with item-level
compression patterns, and (2) Patch Fine-tuning, which teaches LLMs to model
sequences at multiple granularities. Through this simple yet effective
approach, empirical results demonstrate that PatchRec outperforms existing
methods, achieving significant performance gains with fewer tokens fed to the
LLM. Specifically, PatchRec shows up to a 32% improvement in HR@20 on the
Goodreads dataset over uncompressed baseline, while using only 7% of the
tokens. This multi-grained sequence modeling paradigm, with an adjustable
compression ratio, enables LLMs to be efficiently deployed in real-world
recommendation systems that handle extremely long user behavior sequences.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CG-RAG: Research Question Answering by Citation Graph
  Retrieval-Augmented LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15067v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15067v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuntong Hu, Zhihan Lei, Zhongjie Dai, Allen Zhang, Abhinav Angirekula, Zheng Zhang, Liang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research question answering requires accurate retrieval and contextual
understanding of scientific literature. However, current Retrieval-Augmented
Generation (RAG) methods often struggle to balance complex document
relationships with precise information retrieval. In this paper, we introduce
Contextualized Graph Retrieval-Augmented Generation (CG-RAG), a novel framework
that integrates sparse and dense retrieval signals within graph structures to
enhance retrieval efficiency and subsequently improve generation quality for
research question answering. First, we propose a contextual graph
representation for citation graphs, effectively capturing both explicit and
implicit connections within and across documents. Next, we introduce
Lexical-Semantic Graph Retrieval (LeSeGR), which seamlessly integrates sparse
and dense retrieval signals with graph encoding. It bridges the gap between
lexical precision and semantic understanding in citation graph retrieval,
demonstrating generalizability to existing graph retrieval and hybrid retrieval
methods. Finally, we present a context-aware generation strategy that utilizes
the retrieved graph-structured information to generate precise and contextually
enriched responses using large language models (LLMs). Extensive experiments on
research question answering benchmarks across multiple domains demonstrate that
our CG-RAG framework significantly outperforms RAG methods combined with
various state-of-the-art retrieval approaches, delivering superior retrieval
accuracy and generation quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An AI-Driven Live Systematic <span class="highlight-title">Review</span>s in the Brain-Heart Interconnectome:
  Minimizing Research Waste and Advancing Evidence Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17181v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17181v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arya Rahgozar, Pouria Mortezaagha, Jodi Edwards, Douglas Manuel, Jessie McGowen, Merrick Zwarenstein, Dean Fergusson, Andrea Tricco, Kelly Cobey, Margaret Sampson, Malcolm King, Dawn Richards, Alexandra Bodnaruc, David Moher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Brain-Heart Interconnectome (BHI) combines neurology and cardiology but
is hindered by inefficiencies in evidence synthesis, poor adherence to quality
standards, and research waste. To address these challenges, we developed an
AI-driven system to enhance systematic reviews in the BHI domain. The system
integrates automated detection of Population, Intervention, Comparator,
Outcome, and Study design (PICOS), semantic search using vector embeddings,
graph-based querying, and topic modeling to identify redundancies and
underexplored areas. Core components include a Bi-LSTM model achieving 87%
accuracy for PICOS compliance, a study design classifier with 95.7% accuracy,
and Retrieval-Augmented Generation (RAG) with GPT-3.5, which outperformed GPT-4
for graph-based and topic-driven queries. The system provides real-time
updates, reducing research waste through a living database and offering an
interactive interface with dashboards and conversational AI. While initially
developed for BHI, the system's adaptable architecture enables its application
across various biomedical fields, supporting rigorous evidence synthesis,
efficient resource allocation, and informed clinical decision-making.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MDEval: Evaluating and Enhancing Markdown Awareness in Large Language
  Models <span class="chip">WWW 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15000v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15000v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongpu Chen, Yinfeng Liu, Long Shi, Zhi-Jie Wang, Xingyan Chen, Yu Zhao, Fuji Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are expected to offer structured Markdown
responses for the sake of readability in web chatbots (e.g., ChatGPT). Although
there are a myriad of metrics to evaluate LLMs, they fail to evaluate the
readability from the view of output content structure. To this end, we focus on
an overlooked yet important metric -- Markdown Awareness, which directly
impacts the readability and structure of the content generated by these
language models. In this paper, we introduce MDEval, a comprehensive benchmark
to assess Markdown Awareness for LLMs, by constructing a dataset with 20K
instances covering 10 subjects in English and Chinese. Unlike traditional
model-based evaluations, MDEval provides excellent interpretability by
combining model-based generation tasks and statistical methods. Our results
demonstrate that MDEval achieves a Spearman correlation of 0.791 and an
accuracy of 84.1% with human, outperforming existing methods by a large margin.
Extensive experimental results also show that through fine-tuning over our
proposed dataset, less performant open-source models are able to achieve
comparable performance to GPT-4o in terms of Markdown Awareness. To ensure
reproducibility and transparency, MDEval is open sourced at
https://github.com/SWUFE-DB-Group/MDEval-Benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WWW 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SynDL: A Large-Scale Synthetic Test Collection for Passage Retrieval <span class="chip">WWW 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.16312v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.16312v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hossein A. Rahmani, Xi Wang, Emine Yilmaz, Nick Craswell, Bhaskar Mitra, Paul Thomas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale test collections play a crucial role in Information Retrieval
(IR) research. However, according to the Cranfield paradigm and the research
into publicly available datasets, the existing information retrieval research
studies are commonly developed on small-scale datasets that rely on human
assessors for relevance judgments - a time-intensive and expensive process.
Recent studies have shown the strong capability of Large Language Models (LLMs)
in producing reliable relevance judgments with human accuracy but at a greatly
reduced cost. In this paper, to address the missing large-scale ad-hoc document
retrieval dataset, we extend the TREC Deep Learning Track (DL) test collection
via additional language model synthetic labels to enable researchers to test
and evaluate their search systems at a large scale. Specifically, such a test
collection includes more than 1,900 test queries from the previous years of
tracks. We compare system evaluation with past human labels from past years and
find that our synthetically created large-scale test collection can lead to
highly correlated system rankings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, resource paper, WWW 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RDSA: A Robust Deep Graph Clustering Framework via Dual Soft Assignment <span class="chip">DASFAA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21745v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21745v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Xiang, Li Fan, Tulika Saha, Xiaoying Pang, Yushan Pan, Haiyang Zhang, Chengtao Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph clustering is an essential aspect of network analysis that involves
grouping nodes into separate clusters. Recent developments in deep learning
have resulted in graph clustering, which has proven effective in many
applications. Nonetheless, these methods often encounter difficulties when
dealing with real-world graphs, particularly in the presence of noisy edges.
Additionally, many denoising graph clustering methods tend to suffer from lower
performance, training instability, and challenges in scaling to large datasets
compared to non-denoised models. To tackle these issues, we introduce a new
framework called the Robust Deep Graph Clustering Framework via Dual Soft
Assignment (RDSA). RDSA consists of three key components: (i) a node embedding
module that effectively integrates the graph's topological features and node
attributes; (ii) a structure-based soft assignment module that improves graph
modularity by utilizing an affinity matrix for node assignments; and (iii) a
node-based soft assignment module that identifies community landmarks and
refines node assignments to enhance the model's robustness. We assess RDSA on
various real-world datasets, demonstrating its superior performance relative to
existing state-of-the-art methods. Our findings indicate that RDSA provides
robust clustering across different graph types, excelling in clustering
effectiveness and robustness, including adaptability to noise, stability, and
scalability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by DASFAA 2025; Complete version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Grained Query-Guided Set Prediction Network for Grounded
  Multimodal Named Entity Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.21033v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.21033v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jielong Tang, Zhenxing Wang, Ziyang Gong, Jianxing Yu, Xiangwei Zhu, Jian Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Grounded Multimodal Named Entity Recognition (GMNER) is an emerging
information extraction (IE) task, aiming to simultaneously extract entity
spans, types, and corresponding visual regions of entities from given
sentence-image pairs data. Recent unified methods employing machine reading
comprehension or sequence generation-based frameworks show limitations in this
difficult task. The former, utilizing human-designed type queries, struggles to
differentiate ambiguous entities, such as Jordan (Person) and off-White x
Jordan (Shoes). The latter, following the one-by-one decoding order, suffers
from exposure bias issues. We maintain that these works misunderstand the
relationships of multimodal entities. To tackle these, we propose a novel
unified framework named Multi-grained Query-guided Set Prediction Network
(MQSPN) to learn appropriate relationships at intra-entity and inter-entity
levels. Specifically, MQSPN explicitly aligns textual entities with visual
regions by employing a set of learnable queries to strengthen intra-entity
connections. Based on distinct intra-entity modeling, MQSPN reformulates GMNER
as a set prediction, guiding models to establish appropriate inter-entity
relationships from a optimal global matching perspective. Additionally, we
incorporate a query-guided Fusion Net (QFNet) as a glue network to boost better
alignment of two-level relationships. Extensive experiments demonstrate that
our approach achieves state-of-the-art performances in widely used benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Retrieval with Learned Similarities <span class="chip">WWW 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.15462v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.15462v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bailu Ding, Jiaqi Zhai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval plays a fundamental role in recommendation systems, search, and
natural language processing (NLP) by efficiently finding relevant items from a
large corpus given a query. Dot products have been widely used as the
similarity function in such tasks, enabled by Maximum Inner Product Search
(MIPS) algorithms for efficient retrieval. However, state-of-the-art retrieval
algorithms have migrated to learned similarities. These advanced approaches
encompass multiple query embeddings, complex neural networks, direct item ID
decoding via beam search, and hybrid solutions. Unfortunately, we lack
efficient solutions for retrieval in these state-of-the-art setups. Our work
addresses this gap by investigating efficient retrieval techniques with
expressive learned similarity functions. We establish Mixture-of-Logits (MoL)
as a universal approximator of similarity functions, demonstrate that MoL's
expressiveness can be realized empirically to achieve superior performance on
diverse retrieval scenarios, and propose techniques to retrieve the approximate
top-k results using MoL with tight error bounds. Through extensive
experimentation, we show that MoL, enhanced by our proposed mutual
information-based load balancing loss, sets new state-of-the-art results across
heterogeneous scenarios, including sequential retrieval models in
recommendation systems and finetuning language models for question answering;
and our approximate top-$k$ algorithms outperform baselines by up to 66x in
latency while achieving >.99 recall rate compared to exact algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in WWW 2025. Our code and model checkpoints are available
  at https://github.com/bailuding/rails</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Information Discovery in e-Commerce 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05763v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05763v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaochun Ren, Xiangnan He, Dawei Yin, Maarten de Rijke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electronic commerce, or e-commerce, is the buying and selling of goods and
services, or the transmitting of funds or data online. E-commerce platforms
come in many kinds, with global players such as Amazon, Airbnb, Alibaba, eBay
and platforms targeting specific geographic regions. Information retrieval has
a natural role to play in e-commerce, especially in connecting people to goods
and services. Information discovery in e-commerce concerns different types of
search (e.g., exploratory search vs. lookup tasks), recommender systems, and
natural language processing in e-commerce portals. The rise in popularity of
e-commerce sites has made research on information discovery in e-commerce an
increasingly active research area. This is witnessed by an increase in
publications and dedicated workshops in this space. Methods for information
discovery in e-commerce largely focus on improving the effectiveness of
e-commerce search and recommender systems, on enriching and using knowledge
graphs to support e-commerce, and on developing innovative question answering
and bot-based solutions that help to connect people to goods and services. In
this survey, an overview is given of the fundamental infrastructure,
algorithms, and technical solutions for information discovery in e-commerce.
The topics covered include user behavior and profiling, search, recommendation,
and language technology in e-commerce.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CTR-KAN: KAN for Adaptive High-Order Feature Interaction Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.08713v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.08713v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunxiao Shi, Wujiang Xu, Haimin Zhang, Qiang Wu, Yongfeng Zhang, Min Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modeling high-order feature interactions is critical for click-through rate
(CTR) prediction, yet traditional approaches often face challenges in balancing
predictive accuracy and computational efficiency. These methods typically rely
on pre-defined interaction orders, which limit flexibility and require
extensive prior knowledge. Moreover, explicitly modeling high-order
interactions can lead to significant computational overhead. To tackle these
challenges, we propose CTR-KAN, an adaptive framework for efficient high-order
feature interaction modeling. CTR-KAN builds upon the Kolmogorov-Arnold Network
(KAN) paradigm, addressing its limitations in CTR prediction tasks.
Specifically, we introduce key enhancements, including a lightweight
architecture that reduces the computational complexity of KAN and supports
embedding-based feature representations. Additionally, CTR-KAN integrates
guided symbolic regression to effectively capture multiplicative relationships,
a known challenge in standard KAN implementations. Extensive experiments
demonstrate that CTR-KAN achieves state-of-the-art predictive accuracy with
significantly lower computational costs. Its sparse network structure also
facilitates feature pruning and enhances global interpretability, making
CTR-KAN a powerful tool for efficient inference in real-world CTR prediction
scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>draft paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GraphEx: A Graph-based Extraction Method for Advertiser Keyphrase
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.03140v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.03140v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashirbad Mishra, Soumik Dey, Marshall Wu, Jinyu Zhao, He Yu, Kaichen Ni, Binbin Li, Kamesh Madduri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online sellers and advertisers are recommended keyphrases for their listed
products, which they bid on to enhance their sales. One popular paradigm that
generates such recommendations is Extreme Multi-Label Classification (XMC),
which involves tagging/mapping keyphrases to items. We outline the limitations
of using traditional item-query based tagging or mapping techniques for
keyphrase recommendations on E-Commerce platforms. We introduce GraphEx, an
innovative graph-based approach that recommends keyphrases to sellers using
extraction of token permutations from item titles. Additionally, we demonstrate
that relying on traditional metrics such as precision/recall can be misleading
in practical applications, thereby necessitating a combination of metrics to
evaluate performance in real-world scenarios. These metrics are designed to
assess the relevance of keyphrases to items and the potential for buyer
outreach. GraphEx outperforms production models at eBay, achieving the
objectives mentioned above. It supports near real-time inferencing in
resource-constrained production environments and scales effectively for
billions of items.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Audio-Language Models for Audio-Centric Tasks: A <span class="highlight-title">survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15177v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15177v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Su, Jisheng Bai, Qisheng Xu, Kele Xu, Yong Dou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-Language Models (ALMs), which are trained on audio-text data, focus on
the processing, understanding, and reasoning of sounds. Unlike traditional
supervised learning approaches learning from predefined labels, ALMs utilize
natural language as a supervision signal, which is more suitable for describing
complex real-world audio recordings. ALMs demonstrate strong zero-shot
capabilities and can be flexibly adapted to diverse downstream tasks. These
strengths not only enhance the accuracy and generalization of audio processing
tasks but also promote the development of models that more closely resemble
human auditory perception and comprehension. Recent advances in ALMs have
positioned them at the forefront of computer audition research, inspiring a
surge of efforts to advance ALM technologies. Despite rapid progress in the
field of ALMs, there is still a notable lack of systematic surveys that
comprehensively organize and analyze developments. In this paper, we present a
comprehensive review of ALMs with a focus on general audio tasks, aiming to
fill this gap by providing a structured and holistic overview of ALMs.
Specifically, we cover: (1) the background of computer audition and
audio-language models; (2) the foundational aspects of ALMs, including
prevalent network architectures, training objectives, and evaluation methods;
(3) foundational pre-training and audio-language pre-training approaches; (4)
task-specific fine-tuning, multi-task tuning and agent systems for downstream
applications; (5) datasets and benchmarks; and (6) current challenges and
future directions. Our review provides a clear technical roadmap for
researchers to understand the development and future trends of existing
technologies, offering valuable references for implementation in real-world
scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph-Based Cross-Domain Knowledge Distillation for Cross-<span class="highlight-title">Dataset</span>
  Text-to-Image Person Retrieval <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15052v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15052v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bingjun Luo, Jinpeng Wang, Wang Zewen, Junjie Zhu, Xibin Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video surveillance systems are crucial components for ensuring public safety
and management in smart city. As a fundamental task in video surveillance,
text-to-image person retrieval aims to retrieve the target person from an image
gallery that best matches the given text description. Most existing
text-to-image person retrieval methods are trained in a supervised manner that
requires sufficient labeled data in the target domain. However, it is common in
practice that only unlabeled data is available in the target domain due to the
difficulty and cost of data annotation, which limits the generalization of
existing methods in practical application scenarios. To address this issue, we
propose a novel unsupervised domain adaptation method, termed Graph-Based
Cross-Domain Knowledge Distillation (GCKD), to learn the cross-modal feature
representation for text-to-image person retrieval in a cross-dataset scenario.
The proposed GCKD method consists of two main components. Firstly, a
graph-based multi-modal propagation module is designed to bridge the
cross-domain correlation among the visual and textual samples. Secondly, a
contrastive momentum knowledge distillation module is proposed to learn the
cross-modal feature representation using the online knowledge distillation
strategy. By jointly optimizing the two modules, the proposed method is able to
achieve efficient performance for cross-dataset text-to-image person retrieval.
acExtensive experiments on three publicly available text-to-image person
retrieval datasets demonstrate the effectiveness of the proposed GCKD method,
which consistently outperforms the state-of-the-art baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PolySmart @ TRECVid 2024 Video Captioning (VTT) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15509v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15509v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxin Wu, Wengyu Zhang, Xiao-Yong Wei, Qing Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present our methods and results for the Video-To-Text (VTT)
task at TRECVid 2024, exploring the capabilities of Vision-Language Models
(VLMs) like LLaVA and LLaVA-NeXT-Video in generating natural language
descriptions for video content. We investigate the impact of fine-tuning VLMs
on VTT datasets to enhance description accuracy, contextual relevance, and
linguistic consistency. Our analysis reveals that fine-tuning substantially
improves the model's ability to produce more detailed and domain-aligned text,
bridging the gap between generic VLM tasks and the specialized needs of VTT.
Experimental results demonstrate that our fine-tuned model outperforms baseline
VLMs across various evaluation metrics, underscoring the importance of
domain-specific tuning for complex VTT tasks.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-24T00:00:00Z">2025-01-24</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">26</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ExPerT: Effective and Explainable Evaluation of Personalized Long-Form
  Text Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14956v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14956v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alireza Salemi, Julian Killingback, Hamed Zamani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating personalized text generated by large language models (LLMs) is
challenging, as only the LLM user, i.e., prompt author, can reliably assess the
output, but re-engaging the same individuals across studies is infeasible. This
paper addresses the challenge of evaluating personalized text generation by
introducing ExPerT, an explainable reference-based evaluation framework. ExPerT
leverages an LLM to extract atomic aspects and their evidence from the
generated and reference texts, match the aspects, and evaluate their alignment
based on content and writing style -- two key attributes in personalized text
generation. Additionally, ExPerT generates detailed, fine-grained explanations
for every step of the evaluation process, enhancing transparency and
interpretability. Our experiments demonstrate that ExPerT achieves a 7.2%
relative improvement in alignment with human judgments compared to the
state-of-the-art text generation evaluation methods. Furthermore, human
evaluators rated the usability of ExPerT's explanations at 4.7 out of 5,
highlighting its effectiveness in making evaluation decisions more
interpretable.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MISCON: A Mission-Driven Conversational Consultant for Pre-Venture
  Entrepreneurs in Food Deserts <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14954v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14954v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Subhasis Dasgupta, Hans Taparia, Laura Schmidt, Amarnath Gupta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work-in-progress report describes MISCON, a conversational consultant
being developed for a public mission project called NOURISH. With MISCON,
aspiring small business owners in a food-insecure region and their advisors in
Community-based organizations would be able to get information, recommendation
and analysis regarding setting up food businesses. MISCON conversations are
modeled as state machine that uses a heterogeneous knowledge graph as well as
several analytical tools and services including a variety of LLMs. In this
short report, we present the functional architecture and some design
considerations behind MISCON.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages. Acccepted for AAAI 2025 Workshop on AI for Public Missions,
  March 3rd, 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Search results diversification in competitive search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14922v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14922v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tommy Mordo, Itamar Reinman, Moshe Tennenholtz, Oren Kurland
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In Web retrieval, there are many cases of competition between authors of Web
documents: their incentive is to have their documents highly ranked for queries
of interest. As such, the Web is a prominent example of a competitive search
setting. Past work on competitive search focused on ranking functions based
solely on relevance estimation. We study ranking functions that integrate a
results-diversification aspect. We show that the competitive search setting
with diversity-based ranking has an equilibrium. Furthermore, we theoretically
and empirically show that the phenomenon of authors mimicking content in
documents highly ranked in the past, which was demonstrated in previous work,
is mitigated when search results diversification is applied.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do LLMs Provide Consistent Answers to Health-Related Questions across
  Languages? <span class="chip">ECIR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14719v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14719v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ipek Baris Schlicht, Zhixue Zhao, Burcu Sayin, Lucie Flek, Paolo Rosso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Equitable access to reliable health information is vital for public health,
but the quality of online health resources varies by language, raising concerns
about inconsistencies in Large Language Models (LLMs) for healthcare. In this
study, we examine the consistency of responses provided by LLMs to
health-related questions across English, German, Turkish, and Chinese. We
largely expand the HealthFC dataset by categorizing health-related questions by
disease type and broadening its multilingual scope with Turkish and Chinese
translations. We reveal significant inconsistencies in responses that could
spread healthcare misinformation. Our main contributions are 1) a multilingual
health-related inquiry dataset with meta-information on disease categories, and
2) a novel prompt-based evaluation workflow that enables sub-dimensional
comparisons between two languages through parsing. Our findings highlight key
challenges in deploying LLM-based tools in multilingual contexts and emphasize
the need for improved cross-lingual alignment to ensure accurate and equitable
healthcare information.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages. Short paper appeared at 47th European Conference on
  Information Retrieval (ECIR 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledge Graphs Construction from Criminal Court Appeals: Insights from
  the French Cassation Court 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14579v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14579v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander V. Belikov, Sacha Raoult
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite growing interest, accurately and reliably representing unstructured
data, such as court decisions, in a structured form, remains a challenge.
Recent advancements in generative AI applied to language modeling enabled the
transformation of text into knowledge graphs, unlocking new opportunities for
analysis and modeling. This paper presents a framework for constructing
knowledge graphs from appeals to the French Cassation Court. The framework
includes a domain-specific ontology and a derived dataset, offering a
foundation for structured legal data representation and analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Correlating Factors for Domain Adaptation Performance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14466v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14466v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Goksenin Yuksel, Jaap Kamps
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dense retrievers have demonstrated significant potential for neural
information retrieval; however, they lack robustness to domain shifts, limiting
their efficacy in zero-shot settings across diverse domains. In this paper, we
set out to analyze the possible factors that lead to successful domain
adaptation of dense retrievers. We include domain similarity proxies between
generated queries to test and source domains. Furthermore, we conduct a case
study comparing two powerful domain adaptation techniques. We find that
generated query type distribution is an important factor, and generating
queries that share a similar domain to the test documents improves the
performance of domain adaptation methods. This study further emphasizes the
importance of domain-tailored generated queries.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpretability Analysis of Domain Adapted Dense Retrievers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14459v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14459v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Goksenin Yuksel, Jaap Kamps
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dense retrievers have demonstrated significant potential for neural
information retrieval; however, they exhibit a lack of robustness to domain
shifts, thereby limiting their efficacy in zero-shot settings across diverse
domains. Previous research has investigated unsupervised domain adaptation
techniques to adapt dense retrievers to target domains. However, these studies
have not focused on explainability analysis to understand how such adaptations
alter the model's behavior. In this paper, we propose utilizing the integrated
gradients framework to develop an interpretability method that provides both
instance-based and ranking-based explanations for dense retrievers. To generate
these explanations, we introduce a novel baseline that reveals both query and
document attributions. This method is used to analyze the effects of domain
adaptation on input attributions for query and document tokens across two
datasets: the financial question answering dataset (FIQA) and the biomedical
information retrieval dataset (TREC-COVID). Our visualizations reveal that
domain-adapted models focus more on in-domain terminology compared to
non-adapted models, exemplified by terms such as "hedge," "gold," "corona," and
"disease." This research addresses how unsupervised domain adaptation
techniques influence the behavior of dense retrievers when adapted to new
domains. Additionally, we demonstrate that integrated gradients are a viable
choice for explaining and analyzing the internal mechanisms of these opaque
neural models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Remining Hard Negatives for Generative Pseudo Labeled Domain Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14434v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14434v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Goksenin Yuksel, David Rau, Jaap Kamps
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dense retrievers have demonstrated significant potential for neural
information retrieval; however, they exhibit a lack of robustness to domain
shifts, thereby limiting their efficacy in zero-shot settings across diverse
domains. A state-of-the-art domain adaptation technique is Generative Pseudo
Labeling (GPL). GPL uses synthetic query generation and initially mined hard
negatives to distill knowledge from cross-encoder to dense retrievers in the
target domain. In this paper, we analyze the documents retrieved by the
domain-adapted model and discover that these are more relevant to the target
queries than those of the non-domain-adapted model. We then propose refreshing
the hard-negative index during the knowledge distillation phase to mine better
hard negatives. Our remining R-GPL approach boosts ranking performance in 13/14
BEIR datasets and 9/12 LoTTe datasets. Our contributions are (i) analyzing hard
negatives returned by domain-adapted and non-domain-adapted models and (ii)
applying the GPL training with and without hard-negative re-mining in LoTTE and
BEIR datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CAMEO: Autocorrelation-Preserving Line Simplification for Lossy Time
  Series Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14432v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14432v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carlos Enrique Muñiz-Cuza, Matthias Boehm, Torben Bach Pedersen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time series data from a variety of sensors and IoT devices need effective
compression to reduce storage and I/O bandwidth requirements. While most time
series databases and systems rely on lossless compression, lossy techniques
offer even greater space-saving with a small loss in precision. However, the
unknown impact on downstream analytics applications requires a semi-manual
trial-and-error exploration. We initiate work on lossy compression that
provides guarantees on complex statistical features (which are strongly
correlated with the accuracy of the downstream analytics). Specifically, we
propose a new lossy compression method that provides guarantees on the
autocorrelation and partial-autocorrelation functions (ACF/PACF) of a time
series. Our method leverages line simplification techniques as well as
incremental maintenance of aggregates, blocking, and parallelization strategies
for effective and efficient compression. The results show that our method
improves compression ratios by 2x on average and up to 54x on selected
datasets, compared to previous lossy and lossless compression methods.
Moreover, we maintain -- and sometimes even improve -- the forecasting accuracy
by preserving the autocorrelation properties of the time series. Our framework
is extensible to multivariate time series and other statistical features of the
time series.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Handling Heterophily in Recommender Systems with Wavelet Hypergraph
  Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14399v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14399v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Darnbi Sakong, Thanh Tam Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems are pivotal in delivering personalised user experiences
across various domains. However, capturing the heterophily patterns and the
multi-dimensional nature of user-item interactions poses significant
challenges. To address this, we introduce FWHDNN (Fusion-based Wavelet
Hypergraph Diffusion Neural Networks), an innovative framework aimed at
advancing representation learning in hypergraph-based recommendation tasks. The
model incorporates three key components: (1) a cross-difference relation
encoder leveraging heterophily-aware hypergraph diffusion to adapt
message-passing for diverse class labels, (2) a multi-level cluster-wise
encoder employing wavelet transform-based hypergraph neural network layers to
capture multi-scale topological relationships, and (3) an integrated
multi-modal fusion mechanism that combines structural and textual information
through intermediate and late-fusion strategies. Extensive experiments on
real-world datasets demonstrate that FWHDNN surpasses state-of-the-art methods
in accuracy, robustness, and scalability in capturing high-order
interconnections between users and items.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Chain-of-Retrieval Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14342v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14342v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liang Wang, Haonan Chen, Nan Yang, Xiaolong Huang, Zhicheng Dou, Furu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces an approach for training o1-like RAG models that
retrieve and reason over relevant information step by step before generating
the final answer. Conventional RAG methods usually perform a single retrieval
step before the generation process, which limits their effectiveness in
addressing complex queries due to imperfect retrieval results. In contrast, our
proposed method, CoRAG (Chain-of-Retrieval Augmented Generation), allows the
model to dynamically reformulate the query based on the evolving state. To
train CoRAG effectively, we utilize rejection sampling to automatically
generate intermediate retrieval chains, thereby augmenting existing RAG
datasets that only provide the correct final answer. At test time, we propose
various decoding strategies to scale the model's test-time compute by
controlling the length and number of sampled retrieval chains. Experimental
results across multiple benchmarks validate the efficacy of CoRAG, particularly
in multi-hop question answering tasks, where we observe more than 10 points
improvement in EM score compared to strong baselines. On the KILT benchmark,
CoRAG establishes a new state-of-the-art performance across a diverse range of
knowledge-intensive tasks. Furthermore, we offer comprehensive analyses to
understand the scaling behavior of CoRAG, laying the groundwork for future
research aimed at developing factual and grounded foundation models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-stage Large Language Model Pipelines Can Outperform <span class="highlight-title">GPT</span>-4o in
  Relevance Assessment <span class="chip">WWW'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14296v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14296v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julian A. Schnabel, Johanne R. Trippas, Falk Scholer, Danula Hettiachchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The effectiveness of search systems is evaluated using relevance labels that
indicate the usefulness of documents for specific queries and users. While
obtaining these relevance labels from real users is ideal, scaling such data
collection is challenging. Consequently, third-party annotators are employed,
but their inconsistent accuracy demands costly auditing, training, and
monitoring. We propose an LLM-based modular classification pipeline that
divides the relevance assessment task into multiple stages, each utilising
different prompts and models of varying sizes and capabilities. Applied to TREC
Deep Learning (TREC-DL), one of our approaches showed an 18.4% Krippendorff's
$\alpha$ accuracy increase over OpenAI's GPT-4o mini while maintaining a cost
of about 0.2 USD per million input tokens, offering a more efficient and
scalable solution for relevance assessment. This approach beats the baseline
performance of GPT-4o (5 USD). With a pipeline approach, even the accuracy of
the GPT-4o flagship model, measured in $\alpha$, could be improved by 9.7%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WebConf'25, WWW'25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Pre-train</span> and Fine-tune: Recommenders as Large Models <span class="chip">WWW2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14268v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14268v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenhao Jiang, Chenghao Chen, Hao Feng, Yu Yang, Jin Liu, Jie Zhang, Jia Jia, Ning Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In reality, users have different interests in different periods, regions,
scenes, etc. Such changes in interest are so drastic that they are difficult to
be captured by recommenders. Existing multi-domain learning can alleviate this
problem. However, the structure of the industrial recommendation system is
complex, the amount of data is huge, and the training cost is extremely high,
so it is difficult to modify the structure of the industrial recommender and
re-train it. To fill this gap, we consider recommenders as large pre-trained
models and fine-tune them. We first propose the theory of the information
bottleneck for fine-tuning and present an explanation for the fine-tuning
technique in recommenders. To tailor for recommendation, we design an
information-aware adaptive kernel (IAK) technique to fine-tune the pre-trained
recommender. Specifically, we define fine-tuning as two phases: knowledge
compression and knowledge matching and let the training stage of IAK explicitly
approximate these two phases. Our proposed approach designed from the essence
of fine-tuning is well interpretable. Extensive online and offline experiments
show the superiority of our proposed method. Besides, we also share unique and
important lessons we learned when deploying the method in a large-scale online
platform. We also present the potential issues of fine-tuning techniques in
recommendation systems and the corresponding solutions. The recommender with
IAK technique has been deployed on the homepage of a billion-scale online food
platform for several months and has yielded considerable profits in our
business.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by WWW2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting Applicable and Comprehensive Knowledge Tracing in Large-Scale
  Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14256v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14256v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiyun Zhou, Wenkang Han, Jingyuan Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge Tracing (KT) is a fundamental component of Intelligent Tutoring
Systems (ITS), enabling the modeling of students' knowledge states to predict
future performance. The introduction of Deep Knowledge Tracing (DKT), the first
deep learning-based KT (DLKT) model, has brought significant advantages in
terms of applicability and comprehensiveness. However, recent DLKT models, such
as Attentive Knowledge Tracing (AKT), have often prioritized predictive
performance at the expense of these benefits. While deep sequential models like
DKT have shown potential, they face challenges related to parallel computing,
storage decision modification, and limited storage capacity. To address these
limitations, we propose DKT2, a novel KT model that leverages the recently
developed xLSTM architecture. DKT2 enhances input representation using the
Rasch model and incorporates Item Response Theory (IRT) for interpretability,
allowing for the decomposition of learned knowledge into familiar and
unfamiliar knowledge. By integrating this knowledge with predicted questions,
DKT2 generates comprehensive knowledge states. Extensive experiments conducted
across three large-scale datasets demonstrate that DKT2 consistently
outperforms 17 baseline models in various prediction tasks, underscoring its
potential for real-world educational applications. This work bridges the gap
between theoretical advancements and practical implementation in KT.Our code
and datasets will be available at https://github.com/codebase-2025/DKT2.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SimRAG: Self-Improving Retrieval-Augmented Generation for Adapting Large
  Language Models to Specialized Domains <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17952v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17952v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ran Xu, Hui Liu, Sreyashi Nag, Zhenwei Dai, Yaochen Xie, Xianfeng Tang, Chen Luo, Yang Li, Joyce C. Ho, Carl Yang, Qi He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) enhances the question-answering (QA)
abilities of large language models (LLMs) by integrating external knowledge.
However, adapting general-purpose RAG systems to specialized fields such as
science and medicine poses unique challenges due to distribution shifts and
limited access to domain-specific data. To tackle this, we propose SimRAG, a
self-training approach that equips the LLM with joint capabilities of question
answering and question generation for domain adaptation. Our method first
fine-tunes the LLM on instruction-following, question-answering, and
search-related data. Then, it prompts the same LLM to generate diverse
domain-relevant questions from unlabeled corpora, with an additional filtering
strategy to retain high-quality synthetic examples. By leveraging these
self-generated synthetic examples, the LLM can improve their performance on
domain-specific RAG tasks. Experiments on 11 datasets, spanning two backbone
sizes and three domains, demonstrate that SimRAG outperforms baselines by
1.2\%--8.6\%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2025 main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Knowledge Graph Enhanced Language Agents for Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.19627v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.19627v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taicheng Guo, Chaochun Liu, Hai Wang, Varun Mannam, Fang Wang, Xin Chen, Xiangliang Zhang, Chandan K. Reddy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language agents have recently been used to simulate human behavior and
user-item interactions for recommendation systems. However, current language
agent simulations do not understand the relationships between users and items,
leading to inaccurate user profiles and ineffective recommendations. In this
work, we explore the utility of Knowledge Graphs (KGs), which contain extensive
and reliable relationships between users and items, for recommendation. Our key
insight is that the paths in a KG can capture complex relationships between
users and items, eliciting the underlying reasons for user preferences and
enriching user profiles. Leveraging this insight, we propose Knowledge Graph
Enhanced Language Agents(KGLA), a framework that unifies language agents and KG
for recommendation systems. In the simulated recommendation scenario, we
position the user and item within the KG and integrate KG paths as natural
language descriptions into the simulation. This allows language agents to
interact with each other and discover sufficient rationale behind their
interactions, making the simulation more accurate and aligned with real-world
cases, thus improving recommendation performance. Our experimental results show
that KGLA significantly improves recommendation performance (with a 33%-95%
boost in NDCG@1 among three widely used benchmarks) compared to the previous
best baseline method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Peer Recommendation Interventions for Health-related Social Support: a
  Feasibility Assessment <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.04973v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.04973v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zachary Levonian, Matthew Zent, Ngan Nguyen, Matthew McNamara, Loren Terveen, Svetlana Yarosh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online health communities (OHCs) offer the promise of connecting with
supportive peers. Forming these connections first requires finding relevant
peers - a process that can be time-consuming. Peer recommendation systems are a
computational approach to make finding peers easier during a health journey. By
encouraging OHC users to alter their online social networks, peer
recommendations could increase available support. But these benefits are
hypothetical and based on mixed, observational evidence. To experimentally
evaluate the effect of peer recommendations, we conceptualize these systems as
health interventions designed to increase specific beneficial connection
behaviors. In this paper, we designed a peer recommendation intervention to
increase two behaviors: reading about peer experiences and interacting with
peers. We conducted an initial feasibility assessment of this intervention by
conducting a 12-week field study in which 79 users of CaringBridge received
weekly peer recommendations via email. Our results support the usefulness and
demand for peer recommendation and suggest benefits to evaluating larger peer
recommendation interventions. Our contributions include practical guidance on
the development and evaluation of peer recommendation interventions for OHCs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CSCW 2025, 24 article pages, 34 pages of references & appendices, 17
  figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comparative Evaluation of Quantification Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2103.03223v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2103.03223v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tobias Schumacher, Markus Strohmaier, Florian Lemmerich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantification represents the problem of estimating the distribution of class
labels on unseen data. It also represents a growing research field in
supervised machine learning, for which a large variety of different algorithms
has been proposed in recent years. However, a comprehensive empirical
comparison of quantification methods that supports algorithm selection is not
available yet. In this work, we close this research gap by conducting a
thorough empirical performance comparison of 24 different quantification
methods on overall more than 40 data sets, considering binary as well as
multiclass quantification settings. We observe that no single algorithm
generally outperforms all competitors, but identify a group of methods
including the threshold selection-based Median Sweep and TSMax methods, the DyS
framework including the HDy method, Forman's mixture model, and Friedman's
method that performs best in the binary setting. For the multiclass setting, we
observe that a different, broad group of algorithms yields good performance,
including the HDx method, the Generalized Probabilistic Adjusted Count, the
readme method, the energy distance minimization method, the EM algorithm for
quantification, and Friedman's method. We also find that tuning the underlying
classifiers has in most cases only a limited impact on the quantification
performance. More generally, we find that the performance on multiclass
quantification is inferior to the results obtained in the binary setting. Our
results can guide practitioners who intend to apply quantification algorithms
and help researchers to identify opportunities for future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>40 pages, 18 figures, 9 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Investigation of <span class="highlight-title">Prompt</span> Variations for Zero-shot LLM-based Rankers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14117v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14117v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuoqi Sun, Shengyao Zhuang, Shuai Wang, Guido Zuccon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We provide a systematic understanding of the impact of specific components
and wordings used in prompts on the effectiveness of rankers based on zero-shot
Large Language Models (LLMs). Several zero-shot ranking methods based on LLMs
have recently been proposed. Among many aspects, methods differ across (1) the
ranking algorithm they implement, e.g., pointwise vs. listwise, (2) the
backbone LLMs used, e.g., GPT3.5 vs. FLAN-T5, (3) the components and wording
used in prompts, e.g., the use or not of role-definition (role-playing) and the
actual words used to express this. It is currently unclear whether performance
differences are due to the underlying ranking algorithm, or because of spurious
factors such as better choice of words used in prompts. This confusion risks to
undermine future research. Through our large-scale experimentation and
analysis, we find that ranking algorithms do contribute to differences between
methods for zero-shot LLM ranking. However, so do the LLM backbones -- but even
more importantly, the choice of prompt components and wordings affect the
ranking. In fact, in our experiments, we find that, at times, these latter
elements have more impact on the ranker's effectiveness than the actual ranking
algorithms, and that differences among ranking methods become more blurred when
prompt variations are considered.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DEEPER: Dense Electroencephalography Passage Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.06695v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.06695v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niall McGuire, Yashar Moshfeghi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A fundamental challenge in Information Retrieval (IR) is the cognitive burden
of translating internal information needs into explicit textual queries. This
translation barrier particularly affects users with undefined information needs
or those who face physical constraints in traditional text input methods. While
Brain-Machine Interfaces (BMIs) have emerged as a potential solution for direct
neural query interpretation, existing approaches that attempt to convert brain
signals into text queries have demonstrated limited success in capturing the
complexity of neural semantic patterns. This paper introduces DEEPER Dense EEG
Passage Retrieval, a novel framework that bypasses the need for explicit query
translation by directly mapping electroencephalography (EEG) signals to
relevant text passages. Our approach employs dense retrieval architectures to
create a unified semantic space where both EEG signals and text passages can be
effectively compared. Experimental evaluation on the ZuCo dataset shows that
DEEPER substantially outperforms current EEG-to-text baselines, achieving
nearly 5x improvement in retrieval precision while demonstrating robust
performance across a diverse set of 30 participants. Through detailed ablation
analysis, we identify key architectural components, including specialized
neural encoders and strategic negative sampling techniques, that enable
effective cross-modal semantic alignment. Our findings demonstrate the
feasibility of direct EEG passage retrieval and suggest new possibilities for
developing IR systems that can more naturally interface with users' cognitive
processes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Supervised Learning-enhanced Multi-Group Actor Critic for Live Stream
  Allocation in Feed 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.10381v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.10381v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingxin Liu, Xiang Gao, Yisha Li, Xin Li, Haiyang Lu, Ben Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning (RL) has been widely applied in recommendation systems
to capture long-term user engagement, thus improving dwelling time and
improving user retention. In the context of a short video & live stream mixed
recommendation scenario, the live stream recommendation system (RS) decides
whether to inject at most one live stream into the video feed for each user
request. To maximize long-term user engagement, it is crucial to determine an
optimal live stream injection policy for accurate live stream allocation.
However, traditional RL algorithms often face divergence and instability
problems, and these issues may cause too many live stream allocation, which
interrupts user's short video interest and leads to a decrease in the user's
app usage duration. To address these challenges, we propose a novel Supervised
Learning-enhanced Multi-Group Actor Critic algorithm (SL-MGAC). Specifically,
we introduce a supervised learning-enhanced actor-critic framework that
incorporates variance reduction techniques, where multi-task reward learning
helps restrict bootstrapping error accumulation during critic learning.
Additionally, we design a multi-group state decomposition module for both actor
and critic networks to reduce prediction variance and improve model stability.
We also propose a novel reward function to prevent overly greedy live stream
allocation. Empirically, we evaluate the SL-MGAC algorithm using offline policy
evaluation (OPE) and online A/B testing. Experimental results demonstrate that
the proposed method not only outperforms baseline methods under the
platform-level constraints but also exhibits enhanced stability in online
recommendation scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sequence Generation Modeling for Continuous Value Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20211v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20211v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongxu Ma, Kai Tian, Tao Zhang, Xuefeng Zhang, Chunjie Chen, Han Li, Jihong Guan, Shuigeng Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continuous value prediction (CVP) plays a crucial role in short video
recommendation, capturing user preferences through precise numerical
estimations. However, traditional regression-based methods often struggle with
challenges like wide value ranges and imbalanced data, leading to prediction
bias. While ordinal classification approaches have been introduced to address
these issues, their reliance on discretization reduces accuracy and overlooks
inherent relationships between intervals. To overcome these limitations, we
introduce a novel Generative Regression (GR) framework for CVP, inspired by
sequence generation techniques in language modeling. Our method transforms
numerical values into token sequences through structural discretization,
preserving original data fidelity while improving prediction precision.
Leveraging a carefully crafted vocabulary and label encoding, GR employs
curriculum learning with an embedding mixup strategy to bridge
training-inference gaps. Experimental evaluations on four public datasets and
one large-scale industrial dataset validate the superiority of GR over existing
methods. Real-world A/B tests on Kuaishou, a leading video platform, further
demonstrate its practical effectiveness. Additionally, GR proves adaptable to
other regression tasks, such as Lifetime Value (LTV) prediction, showcasing its
potential as a robust solution for diverse CVP challenges.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures, conference or other essential info</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph Regularized Encoder Training for Extreme Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.18434v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.18434v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anshul Mittal, Shikhar Mohan, Deepak Saini, Siddarth Asokan, Suchith C. Prabhu, Lakshya Kumar, Pankaj Malhotra, Jain jiao, Amit Singh, Sumeet Agarwal, Soumen Chakrabarti, Purushottam Kar, Manik Varma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep extreme classification (XC) aims to train an encoder architecture and an
accompanying classifier architecture to tag a data point with the most relevant
subset of labels from a very large universe of labels. XC applications in
ranking, recommendation and tagging routinely encounter tail labels for which
the amount of training data is exceedingly small. Graph convolutional networks
(GCN) present a convenient but computationally expensive way to leverage task
metadata and enhance model accuracies in these settings. This paper formally
establishes that in several use cases, the steep computational cost of GCNs is
entirely avoidable by replacing GCNs with non-GCN architectures. The paper
notices that in these settings, it is much more effective to use graph data to
regularize encoder training than to implement a GCN. Based on these insights,
an alternative paradigm RAMEN is presented to utilize graph metadata in XC
settings that offers significant performance boosts with zero increase in
inference computational costs. RAMEN scales to datasets with up to 1M labels
and offers prediction accuracy up to 15% higher on benchmark datasets than
state of the art methods, including those that use graph metadata to train
GCNs. RAMEN also offers 10% higher accuracy over the best baseline on a
proprietary recommendation dataset sourced from click logs of a popular search
engine. Code for RAMEN will be released publicly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at TheWebConf</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GraFPrint: A GNN-Based Approach for Audio Identification <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10994v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10994v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aditya Bhattacharjee, Shubhr Singh, Emmanouil Benetos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces GraFPrint, an audio identification framework that
leverages the structural learning capabilities of Graph Neural Networks (GNNs)
to create robust audio fingerprints. Our method constructs a k-nearest neighbor
(k-NN) graph from time-frequency representations and applies max-relative graph
convolutions to encode local and global information. The network is trained
using a self-supervised contrastive approach, which enhances resilience to
ambient distortions by optimizing feature representation. GraFPrint
demonstrates superior performance on large-scale datasets at various levels of
granularity, proving to be both lightweight and scalable, making it suitable
for real-world applications with extensive reference databases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE International Conference on Acoustics, Speech, and
  Signal Processing (ICASSP 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Auto-Cypher: Improving LLMs on Cypher generation via LLM-supervised
  generation-verification framework <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.12612v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.12612v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aman Tiwari, Shiva Krishna Reddy Malay, Vikas Yadav, Masoud Hashemi, Sathwik Tejaswi Madhusudhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph databases like Neo4j are gaining popularity for handling complex,
interconnected data, over traditional relational databases in modeling and
querying relationships. While translating natural language into SQL queries is
well-researched, generating Cypher queries for Neo4j remains relatively
underexplored. In this work, we present an automated, LLM-Supervised, pipeline
to generate high-quality synthetic data for Text2Cypher. Our Cypher data
generation pipeline introduces LLM-As-Database-Filler, a novel strategy for
ensuring Cypher query correctness, thus resulting in high quality generations.
Using our pipeline, we generate high quality Text2Cypher data - SynthCypher
containing 29.8k instances across various domains and queries with varying
complexities. Training open-source LLMs like LLaMa-3.1-8B, Mistral-7B, and
QWEN-7B on SynthCypher results in performance gains of up to 40% on the
Text2Cypher test split and 30% on the SPIDER benchmark, adapted for graph
databases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NAACL 2025 main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimizing Sequential Recommendation Models with Scaling Laws and
  Approximate Entropy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.00430v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.00430v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tingjia Shen, Hao Wang, Chuhan Wu, Jin Yao Chin, Wei Guo, Yong Liu, Huifeng Guo, Defu Lian, Ruiming Tang, Enhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling Laws have emerged as a powerful framework for understanding how model
performance evolves as they increase in size, providing valuable insights for
optimizing computational resources. In the realm of Sequential Recommendation
(SR), which is pivotal for predicting users' sequential preferences, these laws
offer a lens through which to address the challenges posed by the scalability
of SR models. However, the presence of structural and collaborative issues in
recommender systems prevents the direct application of the Scaling Law (SL) in
these systems. In response, we introduce the Performance Law for SR models,
which aims to theoretically investigate and model the relationship between
model performance and data quality. Specifically, we first fit the HR and NDCG
metrics to transformer-based SR models. Subsequently, we propose Approximate
Entropy (ApEn) to assess data quality, presenting a more nuanced approach
compared to traditional data quantity metrics. Our method enables accurate
predictions across various dataset scales and model sizes, demonstrating a
strong correlation in large SR models and offering insights into achieving
optimal performance for any given model configuration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating GenAI-powered Evidence Pollution for Out-of-Context
  Multimodal Misinformation Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14728v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14728v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zehong Yan, Peng Qi, Wynne Hsu, Mong Li Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large generative artificial intelligence (GenAI) models have achieved
significant success, they also raise growing concerns about online information
security due to their potential misuse for generating deceptive content.
Out-of-context (OOC) multimodal misinformation detection, which often retrieves
Web evidence to identify the repurposing of images in false contexts, faces the
issue of reasoning over GenAI-polluted evidence to derive accurate predictions.
Existing works simulate GenAI-powered pollution at the claim level with
stylistic rewriting to conceal linguistic cues, and ignore evidence-level
pollution for such information-seeking applications. In this work, we
investigate how polluted evidence affects the performance of existing OOC
detectors, revealing a performance degradation of more than 9 percentage
points. We propose two strategies, cross-modal evidence reranking and
cross-modal claim-evidence reasoning, to address the challenges posed by
polluted evidence. Extensive experiments on two benchmark datasets show that
these strategies can effectively enhance the robustness of existing
out-of-context detectors amidst polluted evidence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2025-02-01T05:26:30.832742910Z">
            2025-02-01 05:26:30 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
